{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Welcome to Doctra","text":""},{"location":"index.html#overview","title":"Overview","text":"<p>Doctra is a powerful Python library for parsing, extracting, and analyzing document content from PDFs. It combines state-of-the-art layout detection, OCR, image restoration, and Vision Language Models (VLM) to provide comprehensive document processing capabilities.</p>"},{"location":"index.html#key-features","title":"Key Features","text":""},{"location":"index.html#comprehensive-pdf-parsing","title":"Comprehensive PDF Parsing","text":"<ul> <li>Layout Detection: Advanced document layout analysis using PaddleOCR</li> <li>OCR Processing: High-quality text extraction with Tesseract</li> <li>Visual Elements: Automatic extraction of figures, charts, and tables</li> <li>Multiple Parsers: Choose the right parser for your use case</li> </ul>"},{"location":"index.html#image-restoration","title":"Image Restoration","text":"<ul> <li>6 Restoration Tasks: Dewarping, deshadowing, appearance enhancement, deblurring, binarization, and end-to-end restoration</li> <li>DocRes Integration: State-of-the-art document image restoration</li> <li>GPU Acceleration: Automatic CUDA detection for faster processing</li> <li>Enhanced Quality: Improves document quality for better OCR results</li> </ul>"},{"location":"index.html#vlm-integration","title":"VLM Integration","text":"<ul> <li>Structured Data Extraction: Convert charts and tables to structured formats</li> <li>Multiple Providers: OpenAI, Gemini, Anthropic, and OpenRouter support</li> <li>Automatic Conversion: Transform visual elements into usable data</li> <li>Flexible Configuration: Easy API key management and model selection</li> </ul>"},{"location":"index.html#rich-output-formats","title":"Rich Output Formats","text":"<ul> <li>Markdown: Human-readable documents with embedded images</li> <li>Excel: Structured data in spreadsheet format</li> <li>JSON: Programmatically accessible data</li> <li>HTML: Interactive web-ready documents</li> <li>Images: High-quality cropped visual elements</li> </ul>"},{"location":"index.html#user-friendly-interfaces","title":"User-Friendly Interfaces","text":"<ul> <li>Web UI: Gradio-based interface with drag &amp; drop</li> <li>Command Line: Powerful CLI for automation</li> <li>Python API: Full programmatic access</li> <li>Real-time Progress: Track processing status</li> </ul>"},{"location":"index.html#quick-start","title":"Quick Start","text":""},{"location":"index.html#installation","title":"Installation","text":"<pre><code>pip install doctra\n</code></pre>"},{"location":"index.html#basic-usage","title":"Basic Usage","text":"<pre><code>from doctra import StructuredPDFParser\n\n# Initialize parser\nparser = StructuredPDFParser()\n\n# Parse a document\nparser.parse(\"document.pdf\")\n</code></pre> <p>System Dependencies</p> <p>Doctra requires Poppler for PDF processing. See the Installation Guide for detailed setup instructions.</p>"},{"location":"index.html#core-components","title":"Core Components","text":""},{"location":"index.html#parsers","title":"Parsers","text":"Parser Description Best For StructuredPDFParser Complete document processing General purpose parsing EnhancedPDFParser Parsing with image restoration Scanned or low-quality documents ChartTablePDFParser Focused extraction Only charts and tables needed"},{"location":"index.html#engines","title":"Engines","text":"Engine Description Use Case DocResEngine Image restoration Standalone image enhancement Layout Detection Document analysis Identify document structure OCR Engine Text extraction Extract text from images VLM Service AI processing Convert visuals to structured data"},{"location":"index.html#use-cases","title":"Use Cases","text":"<ul> <li> Financial Reports: Extract tables, charts, and text from financial documents</li> <li> Research Papers: Parse academic papers with figures and tables</li> <li> Document Archival: Convert scanned documents to searchable formats</li> <li> Data Extraction: Extract structured data from visual elements</li> <li> Document Enhancement: Restore and improve low-quality documents</li> </ul>"},{"location":"index.html#getting-help","title":"Getting Help","text":"<ul> <li> Documentation: You're reading it! Explore the sidebar for detailed guides</li> <li> GitHub Issues: Report bugs or request features</li> <li> PyPI: View package details</li> </ul>"},{"location":"index.html#whats-next","title":"What's Next?","text":"<ul> <li> <p> Quick Start</p> <p>Get up and running with Doctra in minutes</p> <p> Quick Start Guide</p> </li> <li> <p> User Guide</p> <p>Learn about parsers, engines, and advanced features</p> <p> Read the Guide</p> </li> <li> <p> API Reference</p> <p>Detailed API documentation for all components</p> <p> API Docs</p> </li> <li> <p> Examples</p> <p>Real-world examples and integration patterns</p> <p> View Examples</p> </li> </ul>"},{"location":"index.html#acknowledgments","title":"Acknowledgments","text":"<p>Doctra builds upon several excellent open-source projects:</p> <ul> <li>PaddleOCR - Advanced document layout detection and OCR capabilities</li> <li>DocRes - State-of-the-art document image restoration model</li> <li>Outlines - Structured output generation for LLMs</li> </ul> <p>We thank the developers and contributors of these projects for their valuable work.</p>"},{"location":"index.html#license","title":"License","text":"<p>Doctra is released under the MIT License. See the LICENSE file for details.</p>"},{"location":"changelog.html","title":"Changelog","text":"<p>All notable changes to Doctra will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog.html#043-2024-xx-xx","title":"[0.4.3] - 2024-XX-XX","text":""},{"location":"changelog.html#current-release","title":"Current Release","text":"<p>This is the current stable release of Doctra.</p>"},{"location":"changelog.html#features","title":"Features","text":"<ul> <li> <p>Multiple PDF Parsers</p> <ul> <li><code>StructuredPDFParser</code>: Complete document processing</li> <li><code>EnhancedPDFParser</code>: Parsing with image restoration</li> <li><code>ChartTablePDFParser</code>: Specialized chart/table extraction</li> </ul> </li> <li> <p>Image Restoration</p> <ul> <li>DocRes integration for document enhancement</li> <li>6 restoration tasks: appearance, dewarping, deshadowing, deblurring, binarization, end2end</li> <li>GPU acceleration support</li> </ul> </li> <li> <p>VLM Integration</p> <ul> <li>Support for OpenAI, Gemini, Anthropic, and OpenRouter</li> <li>Structured data extraction from charts and tables</li> <li>Automatic conversion to Excel/HTML/JSON</li> </ul> </li> <li> <p>Output Formats</p> <ul> <li>Markdown with embedded images</li> <li>HTML for web viewing</li> <li>Excel for data analysis</li> <li>JSON for programmatic access</li> <li>High-quality image extraction</li> </ul> </li> <li> <p>User Interfaces</p> <ul> <li>Gradio-based web UI</li> <li>Comprehensive CLI</li> <li>Full Python API</li> </ul> </li> <li> <p>Visualization</p> <ul> <li>Layout detection visualization</li> <li>Bounding box overlays</li> <li>Confidence scores</li> <li>Multi-page grid display</li> </ul> </li> </ul>"},{"location":"changelog.html#dependencies","title":"Dependencies","text":"<ul> <li>Python 3.8+</li> <li>PaddlePaddle &gt;= 2.4.0</li> <li>PaddleOCR &gt;= 2.6.0</li> <li>Pillow &gt;= 8.0.0</li> <li>OpenCV &gt;= 4.5.0</li> <li>Pandas &gt;= 1.3.0</li> <li>Tesseract &gt;= 0.1.3</li> <li>PyTesseract &gt;= 0.3.10</li> <li>pdf2image &gt;= 1.16.0</li> <li>Anthropic &gt;= 0.40.0</li> <li>Outlines &gt;= 0.0.34</li> </ul>"},{"location":"changelog.html#unreleased","title":"[Unreleased]","text":""},{"location":"changelog.html#planned-features","title":"Planned Features","text":"<ul> <li> Support for additional document formats (DOCX, PPTX)</li> <li> Improved table structure recognition</li> <li> Batch processing API</li> <li> Docker container</li> <li> Cloud deployment guides</li> <li> Additional VLM providers</li> <li> Performance optimizations</li> <li> Multilingual documentation</li> </ul>"},{"location":"changelog.html#version-history","title":"Version History","text":""},{"location":"changelog.html#043-current","title":"[0.4.3] - Current","text":"<p>Current stable release with full feature set.</p>"},{"location":"changelog.html#040-previous","title":"[0.4.0] - Previous","text":"<p>Initial public release with core features.</p>"},{"location":"changelog.html#upgrade-guide","title":"Upgrade Guide","text":""},{"location":"changelog.html#from-040-to-043","title":"From 0.4.0 to 0.4.3","text":"<p>No breaking changes. Simply upgrade:</p> <pre><code>pip install --upgrade doctra\n</code></pre>"},{"location":"changelog.html#contributing","title":"Contributing","text":"<p>See our Contributing Guide for information on:</p> <ul> <li>Reporting bugs</li> <li>Requesting features</li> <li>Submitting pull requests</li> <li>Development setup</li> </ul>"},{"location":"changelog.html#support","title":"Support","text":"<ul> <li>Documentation: https://ademboukhris457.github.io/Doctra/</li> <li>GitHub Issues: https://github.com/AdemBoukhris457/Doctra/issues</li> <li>PyPI: https://pypi.org/project/doctra/</li> </ul>"},{"location":"api/engines.html","title":"Engines API Reference","text":"<p>Complete API documentation for Doctra engines.</p>"},{"location":"api/engines.html#docresengine","title":"DocResEngine","text":"<p>Image restoration engine for document enhancement.</p>"},{"location":"api/engines.html#doctra.engines.image_restoration.DocResEngine","title":"<code>doctra.engines.image_restoration.DocResEngine</code>","text":"<p>DocRes Image Restoration Engine</p> <p>A wrapper around DocRes inference functionality for easy integration with Doctra's document processing pipeline.</p> Source code in <code>doctra/engines/image_restoration/docres_engine.py</code> <pre><code>class DocResEngine:\n    \"\"\"\n    DocRes Image Restoration Engine\n\n    A wrapper around DocRes inference functionality for easy integration\n    with Doctra's document processing pipeline.\n    \"\"\"\n\n    SUPPORTED_TASKS = [\n        'dewarping', 'deshadowing', 'appearance', \n        'deblurring', 'binarization', 'end2end'\n    ]\n\n    def __init__(\n        self, \n        device: Optional[str] = None,\n        use_half_precision: bool = True,\n        model_path: Optional[str] = None,\n        mbd_path: Optional[str] = None\n    ):\n        \"\"\"\n        Initialize DocRes Engine\n\n        Args:\n            device: Device to run on ('cuda', 'cpu', or None for auto-detect)\n            use_half_precision: Whether to use half precision for inference\n            model_path: Path to DocRes model checkpoint (optional, defaults to Hugging Face Hub)\n            mbd_path: Path to MBD model checkpoint (optional, defaults to Hugging Face Hub)\n        \"\"\"\n        if not DOCRES_AVAILABLE:\n            raise ImportError(\n                \"DocRes is not available. Please install the missing dependencies:\\n\"\n                \"pip install scikit-image&gt;=0.19.3\\n\\n\"\n                \"The DocRes module is already included in this library, but requires \"\n                \"scikit-image for image processing operations.\"\n            )\n\n        # Set device\n        if device is None:\n            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        else:\n            requested_device = torch.device(device)\n            # Check if the requested device is available\n            if requested_device.type == 'cuda' and not torch.cuda.is_available():\n                print(f\"Warning: CUDA requested but not available. Falling back to CPU.\")\n                self.device = torch.device('cpu')\n            else:\n                self.device = requested_device\n\n        self.use_half_precision = use_half_precision\n\n        # Get model paths (always from Hugging Face Hub)\n        try:\n            self.mbd_path, self.model_path = get_model_paths(\n                use_huggingface=True,\n                model_path=model_path,\n                mbd_path=mbd_path\n            )\n        except Exception as e:\n            raise RuntimeError(f\"Failed to get model paths: {e}\")\n\n        # Verify model files exist\n        if not os.path.exists(self.model_path):\n            raise FileNotFoundError(\n                f\"DocRes model not found at {self.model_path}. \"\n                f\"This may indicate a Hugging Face download failure. \"\n                f\"Please check your internet connection and try again.\"\n            )\n\n        if not os.path.exists(self.mbd_path):\n            raise FileNotFoundError(\n                f\"MBD model not found at {self.mbd_path}. \"\n                f\"This may indicate a Hugging Face download failure. \"\n                f\"Please check your internet connection and try again.\"\n            )\n\n        # Initialize model\n        self._model = None\n        self._initialize_model()\n\n    def _initialize_model(self):\n        \"\"\"Initialize the DocRes model\"\"\"\n        try:\n            # Create model architecture\n            self._model = restormer_arch.Restormer( \n                inp_channels=6, \n                out_channels=3, \n                dim=48,\n                num_blocks=[2,3,3,4], \n                num_refinement_blocks=4,\n                heads=[1,2,4,8],\n                ffn_expansion_factor=2.66,\n                bias=False,\n                LayerNorm_type='WithBias',\n                dual_pixel_task=True        \n            )\n\n            # Load model weights - always load to CPU first, then move to target device\n            state = convert_state_dict(torch.load(self.model_path, map_location='cpu')['model_state'])\n\n            self._model.load_state_dict(state)\n            self._model.eval()\n            self._model = self._model.to(self.device)\n\n        except Exception as e:\n            raise RuntimeError(f\"Failed to initialize DocRes model: {e}\")\n\n    def restore_image(\n        self, \n        image: Union[str, np.ndarray], \n        task: str = \"appearance\",\n        save_prompts: bool = False\n    ) -&gt; Tuple[np.ndarray, Dict[str, Any]]:\n        \"\"\"\n        Restore a single image using DocRes\n\n        Args:\n            image: Path to image file or numpy array\n            task: Restoration task to perform\n            save_prompts: Whether to save intermediate prompts\n\n        Returns:\n            Tuple of (restored_image, metadata)\n        \"\"\"\n        if task not in self.SUPPORTED_TASKS:\n            raise ValueError(f\"Unsupported task: {task}. Supported tasks: {self.SUPPORTED_TASKS}\")\n\n        # Load image if path provided\n        if isinstance(image, str):\n            if not os.path.exists(image):\n                raise FileNotFoundError(f\"Image not found: {image}\")\n            img_array = cv2.imread(image)\n            if img_array is None:\n                raise ValueError(f\"Could not load image: {image}\")\n        else:\n            img_array = image.copy()\n\n        original_shape = img_array.shape\n\n        try:\n            # Handle end2end pipeline\n            if task == \"end2end\":\n                return self._run_end2end_pipeline(img_array, save_prompts)\n\n            # Run single task\n            restored_img, metadata = self._run_single_task(img_array, task, save_prompts)\n\n            metadata.update({\n                'original_shape': original_shape,\n                'restored_shape': restored_img.shape,\n                'task': task,\n                'device': str(self.device)\n            })\n\n            return restored_img, metadata\n\n        except Exception as e:\n            raise RuntimeError(f\"Image restoration failed: {e}\")\n\n    def _run_single_task(self, img_array: np.ndarray, task: str, save_prompts: bool) -&gt; Tuple[np.ndarray, Dict]:\n        \"\"\"Run a single restoration task\"\"\"\n\n        # Create temporary file for inference\n        with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as tmp_file:\n            tmp_path = tmp_file.name\n            cv2.imwrite(tmp_path, img_array)\n\n        try:\n            # Change to DocRes directory for inference to work properly\n            original_cwd = os.getcwd()\n            os.chdir(str(docres_dir))\n\n            # Set global DEVICE variable that DocRes inference expects\n            import inference  # Import the inference module to set its global DEVICE\n            inference.DEVICE = self.device\n\n            try:\n                # Run inference\n                prompt1, prompt2, prompt3, restored = inference_one_im(self._model, tmp_path, task)\n            finally:\n                # Always restore original working directory\n                os.chdir(original_cwd)\n\n            metadata = {\n                'task': task,\n                'device': str(self.device)\n            }\n\n            if save_prompts:\n                metadata['prompts'] = {\n                    'prompt1': prompt1,\n                    'prompt2': prompt2, \n                    'prompt3': prompt3\n                }\n\n            return restored, metadata\n\n        finally:\n            # Clean up temporary file with retry for Windows\n            try:\n                # Wait a bit for file handles to be released\n                time.sleep(0.1)\n                os.unlink(tmp_path)\n            except PermissionError:\n                # If still locked, try again after a longer wait\n                time.sleep(1)\n                try:\n                    os.unlink(tmp_path)\n                except PermissionError:\n                    # If still failing, just leave it - it will be cleaned up by the OS\n                    pass\n\n    def _run_end2end_pipeline(self, img_array: np.ndarray, save_prompts: bool) -&gt; Tuple[np.ndarray, Dict]:\n        \"\"\"Run the end2end pipeline: dewarping \u2192 deshadowing \u2192 appearance\"\"\"\n\n        intermediate_steps = {}\n\n        # Change to DocRes directory for inference to work properly\n        original_cwd = os.getcwd()\n        os.chdir(str(docres_dir))\n\n        # Set global DEVICE variable that DocRes inference expects\n        import inference  # Import the inference module to set its global DEVICE\n        inference.DEVICE = self.device\n\n        try:\n            with tempfile.TemporaryDirectory() as tmp_dir:\n                # Step 1: Dewarping\n                step1_path = os.path.join(tmp_dir, \"step1.jpg\")\n                cv2.imwrite(step1_path, img_array)\n\n                prompt1, prompt2, prompt3, dewarped = inference_one_im(self._model, step1_path, \"dewarping\")\n                intermediate_steps['dewarped'] = dewarped\n\n                # Step 2: Deshadowing\n                step2_path = os.path.join(tmp_dir, \"step2.jpg\")\n                cv2.imwrite(step2_path, dewarped)\n\n                prompt1, prompt2, prompt3, deshadowed = inference_one_im(self._model, step2_path, \"deshadowing\")\n                intermediate_steps['deshadowed'] = deshadowed\n\n                # Step 3: Appearance\n                step3_path = os.path.join(tmp_dir, \"step3.jpg\")\n                cv2.imwrite(step3_path, deshadowed)\n\n                prompt1, prompt2, prompt3, final = inference_one_im(self._model, step3_path, \"appearance\")\n\n                metadata = {\n                    'task': 'end2end',\n                    'device': str(self.device),\n                    'intermediate_steps': intermediate_steps\n                }\n\n                if save_prompts:\n                    metadata['prompts'] = {\n                        'prompt1': prompt1,\n                        'prompt2': prompt2,\n                        'prompt3': prompt3\n                    }\n\n                return final, metadata\n        finally:\n            # Always restore original working directory\n            os.chdir(original_cwd)\n\n    def batch_restore(\n        self, \n        images: List[Union[str, np.ndarray]], \n        task: str = \"appearance\",\n        save_prompts: bool = False\n    ) -&gt; List[Tuple[Optional[np.ndarray], Dict[str, Any]]]:\n        \"\"\"\n        Restore multiple images in batch\n\n        Args:\n            images: List of image paths or numpy arrays\n            task: Restoration task to perform\n            save_prompts: Whether to save intermediate prompts\n\n        Returns:\n            List of (restored_image, metadata) tuples\n        \"\"\"\n        results = []\n\n        for i, image in enumerate(images):\n            try:\n                restored_img, metadata = self.restore_image(image, task, save_prompts)\n                results.append((restored_img, metadata))\n            except Exception as e:\n                # Return None for failed images with error metadata\n                error_metadata = {\n                    'error': str(e),\n                    'task': task,\n                    'device': str(self.device),\n                    'image_index': i\n                }\n                results.append((None, error_metadata))\n\n        return results\n\n    def get_supported_tasks(self) -&gt; List[str]:\n        \"\"\"Get list of supported restoration tasks\"\"\"\n        return self.SUPPORTED_TASKS.copy()\n\n    def is_available(self) -&gt; bool:\n        \"\"\"Check if DocRes is available and properly configured\"\"\"\n        return DOCRES_AVAILABLE and self._model is not None\n\n    def restore_pdf(\n        self, \n        pdf_path: str, \n        output_path: str | None = None,\n        task: str = \"appearance\",\n        dpi: int = 200\n    ) -&gt; str | None:\n        \"\"\"\n        Restore an entire PDF document using DocRes\n\n        Args:\n            pdf_path: Path to the input PDF file\n            output_path: Path for the enhanced PDF (if None, auto-generates)\n            task: DocRes restoration task (default: \"appearance\")\n            dpi: DPI for PDF rendering (default: 200)\n\n        Returns:\n            Path to the enhanced PDF or None if failed\n        \"\"\"\n        try:\n            from PIL import Image\n            from doctra.utils.pdf_io import render_pdf_to_images\n\n            # Generate output path if not provided\n            if output_path is None:\n                pdf_dir = os.path.dirname(pdf_path)\n                pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n                output_path = os.path.join(pdf_dir, f\"{pdf_name}_enhanced.pdf\")\n\n            print(f\"\ud83d\udd04 Processing PDF with DocRes: {os.path.basename(pdf_path)}\")\n\n            # Render all pages to images\n            pil_pages = [im for (im, _, _) in render_pdf_to_images(pdf_path, dpi=dpi)]\n\n            if not pil_pages:\n                print(\"\u274c No pages found in PDF\")\n                return None\n\n            # Process each page with DocRes\n            enhanced_pages = []\n\n            # Detect environment for progress bar\n            is_notebook = \"ipykernel\" in sys.modules or \"jupyter\" in sys.modules\n\n            # Create progress bar for page processing\n            if is_notebook:\n                progress_bar = create_notebook_friendly_bar(\n                    total=len(pil_pages), \n                    desc=\"Processing pages\"\n                )\n            else:\n                progress_bar = create_beautiful_progress_bar(\n                    total=len(pil_pages), \n                    desc=\"Processing pages\",\n                    leave=True\n                )\n\n            with progress_bar:\n                for i, page_img in enumerate(pil_pages):\n                    try:\n                        # Convert PIL to numpy array\n                        img_array = np.array(page_img)\n\n                        # Apply DocRes restoration\n                        restored_img, _ = self.restore_image(img_array, task)\n\n                        # Convert back to PIL Image\n                        enhanced_page = Image.fromarray(restored_img)\n                        enhanced_pages.append(enhanced_page)\n\n                        progress_bar.set_description(f\"\u2705 Page {i+1}/{len(pil_pages)} processed\")\n                        progress_bar.update(1)\n\n                    except Exception as e:\n                        print(f\"  \u26a0\ufe0f Page {i+1} processing failed: {e}, using original\")\n                        enhanced_pages.append(page_img)\n                        progress_bar.set_description(f\"\u26a0\ufe0f Page {i+1} failed, using original\")\n                        progress_bar.update(1)\n\n            # Create enhanced PDF\n            if enhanced_pages:\n                enhanced_pages[0].save(\n                    output_path,\n                    \"PDF\",\n                    resolution=100.0,\n                    save_all=True,\n                    append_images=enhanced_pages[1:] if len(enhanced_pages) &gt; 1 else []\n                )\n\n                print(f\"\u2705 Enhanced PDF saved: {output_path}\")\n                return output_path\n            else:\n                print(\"\u274c No pages to save\")\n                return None\n\n        except ImportError as e:\n            print(f\"\u274c Required dependencies not available: {e}\")\n            print(\"Install with: pip install PyMuPDF\")\n            return None\n        except Exception as e:\n            print(f\"\u274c Error processing PDF with DocRes: {e}\")\n            return None\n</code></pre>"},{"location":"api/engines.html#doctra.engines.image_restoration.DocResEngine.__init__","title":"<code>__init__(device=None, use_half_precision=True, model_path=None, mbd_path=None)</code>","text":"<p>Initialize DocRes Engine</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Optional[str]</code> <p>Device to run on ('cuda', 'cpu', or None for auto-detect)</p> <code>None</code> <code>use_half_precision</code> <code>bool</code> <p>Whether to use half precision for inference</p> <code>True</code> <code>model_path</code> <code>Optional[str]</code> <p>Path to DocRes model checkpoint (optional, defaults to Hugging Face Hub)</p> <code>None</code> <code>mbd_path</code> <code>Optional[str]</code> <p>Path to MBD model checkpoint (optional, defaults to Hugging Face Hub)</p> <code>None</code> Source code in <code>doctra/engines/image_restoration/docres_engine.py</code> <pre><code>def __init__(\n    self, \n    device: Optional[str] = None,\n    use_half_precision: bool = True,\n    model_path: Optional[str] = None,\n    mbd_path: Optional[str] = None\n):\n    \"\"\"\n    Initialize DocRes Engine\n\n    Args:\n        device: Device to run on ('cuda', 'cpu', or None for auto-detect)\n        use_half_precision: Whether to use half precision for inference\n        model_path: Path to DocRes model checkpoint (optional, defaults to Hugging Face Hub)\n        mbd_path: Path to MBD model checkpoint (optional, defaults to Hugging Face Hub)\n    \"\"\"\n    if not DOCRES_AVAILABLE:\n        raise ImportError(\n            \"DocRes is not available. Please install the missing dependencies:\\n\"\n            \"pip install scikit-image&gt;=0.19.3\\n\\n\"\n            \"The DocRes module is already included in this library, but requires \"\n            \"scikit-image for image processing operations.\"\n        )\n\n    # Set device\n    if device is None:\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    else:\n        requested_device = torch.device(device)\n        # Check if the requested device is available\n        if requested_device.type == 'cuda' and not torch.cuda.is_available():\n            print(f\"Warning: CUDA requested but not available. Falling back to CPU.\")\n            self.device = torch.device('cpu')\n        else:\n            self.device = requested_device\n\n    self.use_half_precision = use_half_precision\n\n    # Get model paths (always from Hugging Face Hub)\n    try:\n        self.mbd_path, self.model_path = get_model_paths(\n            use_huggingface=True,\n            model_path=model_path,\n            mbd_path=mbd_path\n        )\n    except Exception as e:\n        raise RuntimeError(f\"Failed to get model paths: {e}\")\n\n    # Verify model files exist\n    if not os.path.exists(self.model_path):\n        raise FileNotFoundError(\n            f\"DocRes model not found at {self.model_path}. \"\n            f\"This may indicate a Hugging Face download failure. \"\n            f\"Please check your internet connection and try again.\"\n        )\n\n    if not os.path.exists(self.mbd_path):\n        raise FileNotFoundError(\n            f\"MBD model not found at {self.mbd_path}. \"\n            f\"This may indicate a Hugging Face download failure. \"\n            f\"Please check your internet connection and try again.\"\n        )\n\n    # Initialize model\n    self._model = None\n    self._initialize_model()\n</code></pre>"},{"location":"api/engines.html#doctra.engines.image_restoration.DocResEngine.batch_restore","title":"<code>batch_restore(images, task='appearance', save_prompts=False)</code>","text":"<p>Restore multiple images in batch</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>List[Union[str, ndarray]]</code> <p>List of image paths or numpy arrays</p> required <code>task</code> <code>str</code> <p>Restoration task to perform</p> <code>'appearance'</code> <code>save_prompts</code> <code>bool</code> <p>Whether to save intermediate prompts</p> <code>False</code> <p>Returns:</p> Type Description <code>List[Tuple[Optional[ndarray], Dict[str, Any]]]</code> <p>List of (restored_image, metadata) tuples</p> Source code in <code>doctra/engines/image_restoration/docres_engine.py</code> <pre><code>def batch_restore(\n    self, \n    images: List[Union[str, np.ndarray]], \n    task: str = \"appearance\",\n    save_prompts: bool = False\n) -&gt; List[Tuple[Optional[np.ndarray], Dict[str, Any]]]:\n    \"\"\"\n    Restore multiple images in batch\n\n    Args:\n        images: List of image paths or numpy arrays\n        task: Restoration task to perform\n        save_prompts: Whether to save intermediate prompts\n\n    Returns:\n        List of (restored_image, metadata) tuples\n    \"\"\"\n    results = []\n\n    for i, image in enumerate(images):\n        try:\n            restored_img, metadata = self.restore_image(image, task, save_prompts)\n            results.append((restored_img, metadata))\n        except Exception as e:\n            # Return None for failed images with error metadata\n            error_metadata = {\n                'error': str(e),\n                'task': task,\n                'device': str(self.device),\n                'image_index': i\n            }\n            results.append((None, error_metadata))\n\n    return results\n</code></pre>"},{"location":"api/engines.html#doctra.engines.image_restoration.DocResEngine.get_supported_tasks","title":"<code>get_supported_tasks()</code>","text":"<p>Get list of supported restoration tasks</p> Source code in <code>doctra/engines/image_restoration/docres_engine.py</code> <pre><code>def get_supported_tasks(self) -&gt; List[str]:\n    \"\"\"Get list of supported restoration tasks\"\"\"\n    return self.SUPPORTED_TASKS.copy()\n</code></pre>"},{"location":"api/engines.html#doctra.engines.image_restoration.DocResEngine.is_available","title":"<code>is_available()</code>","text":"<p>Check if DocRes is available and properly configured</p> Source code in <code>doctra/engines/image_restoration/docres_engine.py</code> <pre><code>def is_available(self) -&gt; bool:\n    \"\"\"Check if DocRes is available and properly configured\"\"\"\n    return DOCRES_AVAILABLE and self._model is not None\n</code></pre>"},{"location":"api/engines.html#doctra.engines.image_restoration.DocResEngine.restore_image","title":"<code>restore_image(image, task='appearance', save_prompts=False)</code>","text":"<p>Restore a single image using DocRes</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, ndarray]</code> <p>Path to image file or numpy array</p> required <code>task</code> <code>str</code> <p>Restoration task to perform</p> <code>'appearance'</code> <code>save_prompts</code> <code>bool</code> <p>Whether to save intermediate prompts</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, Dict[str, Any]]</code> <p>Tuple of (restored_image, metadata)</p> Source code in <code>doctra/engines/image_restoration/docres_engine.py</code> <pre><code>def restore_image(\n    self, \n    image: Union[str, np.ndarray], \n    task: str = \"appearance\",\n    save_prompts: bool = False\n) -&gt; Tuple[np.ndarray, Dict[str, Any]]:\n    \"\"\"\n    Restore a single image using DocRes\n\n    Args:\n        image: Path to image file or numpy array\n        task: Restoration task to perform\n        save_prompts: Whether to save intermediate prompts\n\n    Returns:\n        Tuple of (restored_image, metadata)\n    \"\"\"\n    if task not in self.SUPPORTED_TASKS:\n        raise ValueError(f\"Unsupported task: {task}. Supported tasks: {self.SUPPORTED_TASKS}\")\n\n    # Load image if path provided\n    if isinstance(image, str):\n        if not os.path.exists(image):\n            raise FileNotFoundError(f\"Image not found: {image}\")\n        img_array = cv2.imread(image)\n        if img_array is None:\n            raise ValueError(f\"Could not load image: {image}\")\n    else:\n        img_array = image.copy()\n\n    original_shape = img_array.shape\n\n    try:\n        # Handle end2end pipeline\n        if task == \"end2end\":\n            return self._run_end2end_pipeline(img_array, save_prompts)\n\n        # Run single task\n        restored_img, metadata = self._run_single_task(img_array, task, save_prompts)\n\n        metadata.update({\n            'original_shape': original_shape,\n            'restored_shape': restored_img.shape,\n            'task': task,\n            'device': str(self.device)\n        })\n\n        return restored_img, metadata\n\n    except Exception as e:\n        raise RuntimeError(f\"Image restoration failed: {e}\")\n</code></pre>"},{"location":"api/engines.html#doctra.engines.image_restoration.DocResEngine.restore_pdf","title":"<code>restore_pdf(pdf_path, output_path=None, task='appearance', dpi=200)</code>","text":"<p>Restore an entire PDF document using DocRes</p> <p>Parameters:</p> Name Type Description Default <code>pdf_path</code> <code>str</code> <p>Path to the input PDF file</p> required <code>output_path</code> <code>str | None</code> <p>Path for the enhanced PDF (if None, auto-generates)</p> <code>None</code> <code>task</code> <code>str</code> <p>DocRes restoration task (default: \"appearance\")</p> <code>'appearance'</code> <code>dpi</code> <code>int</code> <p>DPI for PDF rendering (default: 200)</p> <code>200</code> <p>Returns:</p> Type Description <code>str | None</code> <p>Path to the enhanced PDF or None if failed</p> Source code in <code>doctra/engines/image_restoration/docres_engine.py</code> <pre><code>def restore_pdf(\n    self, \n    pdf_path: str, \n    output_path: str | None = None,\n    task: str = \"appearance\",\n    dpi: int = 200\n) -&gt; str | None:\n    \"\"\"\n    Restore an entire PDF document using DocRes\n\n    Args:\n        pdf_path: Path to the input PDF file\n        output_path: Path for the enhanced PDF (if None, auto-generates)\n        task: DocRes restoration task (default: \"appearance\")\n        dpi: DPI for PDF rendering (default: 200)\n\n    Returns:\n        Path to the enhanced PDF or None if failed\n    \"\"\"\n    try:\n        from PIL import Image\n        from doctra.utils.pdf_io import render_pdf_to_images\n\n        # Generate output path if not provided\n        if output_path is None:\n            pdf_dir = os.path.dirname(pdf_path)\n            pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n            output_path = os.path.join(pdf_dir, f\"{pdf_name}_enhanced.pdf\")\n\n        print(f\"\ud83d\udd04 Processing PDF with DocRes: {os.path.basename(pdf_path)}\")\n\n        # Render all pages to images\n        pil_pages = [im for (im, _, _) in render_pdf_to_images(pdf_path, dpi=dpi)]\n\n        if not pil_pages:\n            print(\"\u274c No pages found in PDF\")\n            return None\n\n        # Process each page with DocRes\n        enhanced_pages = []\n\n        # Detect environment for progress bar\n        is_notebook = \"ipykernel\" in sys.modules or \"jupyter\" in sys.modules\n\n        # Create progress bar for page processing\n        if is_notebook:\n            progress_bar = create_notebook_friendly_bar(\n                total=len(pil_pages), \n                desc=\"Processing pages\"\n            )\n        else:\n            progress_bar = create_beautiful_progress_bar(\n                total=len(pil_pages), \n                desc=\"Processing pages\",\n                leave=True\n            )\n\n        with progress_bar:\n            for i, page_img in enumerate(pil_pages):\n                try:\n                    # Convert PIL to numpy array\n                    img_array = np.array(page_img)\n\n                    # Apply DocRes restoration\n                    restored_img, _ = self.restore_image(img_array, task)\n\n                    # Convert back to PIL Image\n                    enhanced_page = Image.fromarray(restored_img)\n                    enhanced_pages.append(enhanced_page)\n\n                    progress_bar.set_description(f\"\u2705 Page {i+1}/{len(pil_pages)} processed\")\n                    progress_bar.update(1)\n\n                except Exception as e:\n                    print(f\"  \u26a0\ufe0f Page {i+1} processing failed: {e}, using original\")\n                    enhanced_pages.append(page_img)\n                    progress_bar.set_description(f\"\u26a0\ufe0f Page {i+1} failed, using original\")\n                    progress_bar.update(1)\n\n        # Create enhanced PDF\n        if enhanced_pages:\n            enhanced_pages[0].save(\n                output_path,\n                \"PDF\",\n                resolution=100.0,\n                save_all=True,\n                append_images=enhanced_pages[1:] if len(enhanced_pages) &gt; 1 else []\n            )\n\n            print(f\"\u2705 Enhanced PDF saved: {output_path}\")\n            return output_path\n        else:\n            print(\"\u274c No pages to save\")\n            return None\n\n    except ImportError as e:\n        print(f\"\u274c Required dependencies not available: {e}\")\n        print(\"Install with: pip install PyMuPDF\")\n        return None\n    except Exception as e:\n        print(f\"\u274c Error processing PDF with DocRes: {e}\")\n        return None\n</code></pre>"},{"location":"api/engines.html#quick-reference","title":"Quick Reference","text":""},{"location":"api/engines.html#docresengine_1","title":"DocResEngine","text":"<pre><code>from doctra import DocResEngine\n\n# Initialize engine\nengine = DocResEngine(\n    device: str = None,  # \"cuda\", \"cpu\", or None for auto-detect\n    use_half_precision: bool = False,\n    model_path: str = None,\n    mbd_path: str = None\n)\n\n# Restore single image\nrestored_img, metadata = engine.restore_image(\n    image: Union[str, np.ndarray, PIL.Image.Image],\n    task: str = \"appearance\"\n)\n\n# Restore PDF\noutput_path = engine.restore_pdf(\n    pdf_path: str,\n    output_path: str = None,\n    task: str = \"appearance\",\n    dpi: int = 200\n)\n</code></pre>"},{"location":"api/engines.html#parameter-reference","title":"Parameter Reference","text":""},{"location":"api/engines.html#initialization-parameters","title":"Initialization Parameters","text":"Parameter Type Default Description <code>device</code> str None Processing device: \"cuda\", \"cpu\", or None (auto-detect) <code>use_half_precision</code> bool False Use FP16 for faster GPU processing <code>model_path</code> str None Custom path to restoration model <code>mbd_path</code> str None Custom path to MBD model"},{"location":"api/engines.html#restoration-tasks","title":"Restoration Tasks","text":"Task Description Use Case <code>\"appearance\"</code> General appearance enhancement Most documents (default) <code>\"dewarping\"</code> Correct perspective distortion Scanned with perspective issues <code>\"deshadowing\"</code> Remove shadows and lighting artifacts Poor lighting conditions <code>\"deblurring\"</code> Reduce blur and improve sharpness Motion blur, focus issues <code>\"binarization\"</code> Convert to black and white Clean text extraction <code>\"end2end\"</code> Complete restoration pipeline Severely degraded documents"},{"location":"api/engines.html#methods","title":"Methods","text":""},{"location":"api/engines.html#restore_image","title":"restore_image()","text":"<p>Restore a single image.</p> <p>Parameters:</p> <ul> <li><code>image</code> (str | np.ndarray | PIL.Image.Image): Input image (path, numpy array, or PIL Image)</li> <li><code>task</code> (str): Restoration task to perform</li> </ul> <p>Returns:</p> <ul> <li><code>restored_img</code> (PIL.Image.Image): Restored image</li> <li><code>metadata</code> (dict): Processing metadata including task, device, and timing</li> </ul> <p>Example:</p> <pre><code>from doctra import DocResEngine\n\nengine = DocResEngine(device=\"cuda\")\nrestored, meta = engine.restore_image(\"blurry.jpg\", task=\"deblurring\")\n\nprint(f\"Task: {meta['task']}\")\nprint(f\"Device: {meta['device']}\")\nprint(f\"Time: {meta['processing_time']:.2f}s\")\n\n# Save restored image\nrestored.save(\"restored.jpg\")\n</code></pre>"},{"location":"api/engines.html#restore_pdf","title":"restore_pdf()","text":"<p>Restore all pages in a PDF document.</p> <p>Parameters:</p> <ul> <li><code>pdf_path</code> (str): Path to input PDF</li> <li><code>output_path</code> (str, optional): Path for output PDF (auto-generated if None)</li> <li><code>task</code> (str): Restoration task to perform</li> <li><code>dpi</code> (int): Resolution for processing</li> </ul> <p>Returns:</p> <ul> <li><code>output_path</code> (str): Path to the restored PDF</li> </ul> <p>Example:</p> <pre><code>from doctra import DocResEngine\n\nengine = DocResEngine(device=\"cuda\")\nrestored_pdf = engine.restore_pdf(\n    pdf_path=\"low_quality.pdf\",\n    output_path=\"enhanced.pdf\",\n    task=\"appearance\",\n    dpi=300\n)\n\nprint(f\"Restored PDF saved to: {restored_pdf}\")\n</code></pre>"},{"location":"api/engines.html#device-selection","title":"Device Selection","text":""},{"location":"api/engines.html#auto-detection","title":"Auto-Detection","text":"<pre><code># Automatically uses GPU if available, otherwise CPU\nengine = DocResEngine()\n</code></pre>"},{"location":"api/engines.html#explicit-gpu","title":"Explicit GPU","text":"<pre><code># Force GPU usage (will error if CUDA not available)\nengine = DocResEngine(device=\"cuda\")\n</code></pre>"},{"location":"api/engines.html#explicit-cpu","title":"Explicit CPU","text":"<pre><code># Force CPU usage (slower but always available)\nengine = DocResEngine(device=\"cpu\")\n</code></pre>"},{"location":"api/engines.html#check-device","title":"Check Device","text":"<pre><code>import torch\n\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n</code></pre>"},{"location":"api/engines.html#performance-optimization","title":"Performance Optimization","text":""},{"location":"api/engines.html#half-precision","title":"Half Precision","text":"<p>Use FP16 for ~2x speed on modern GPUs:</p> <pre><code>engine = DocResEngine(\n    device=\"cuda\",\n    use_half_precision=True  # Faster, minimal quality loss\n)\n</code></pre> <p>Requirements: - NVIDIA GPU with compute capability 7.0+ (Volta or newer) - Examples: RTX 20xx, RTX 30xx, RTX 40xx, A100, V100</p>"},{"location":"api/engines.html#batch-processing","title":"Batch Processing","text":"<p>Process multiple images efficiently:</p> <pre><code>from doctra import DocResEngine\n\nengine = DocResEngine(device=\"cuda\")\n\n# Process image list\nimages = [\"doc1.jpg\", \"doc2.jpg\", \"doc3.jpg\"]\nrestored_images = []\n\nfor img_path in images:\n    restored, _ = engine.restore_image(img_path, task=\"appearance\")\n    restored_images.append(restored)\n    restored.save(f\"restored_{img_path}\")\n</code></pre>"},{"location":"api/engines.html#dpi-considerations","title":"DPI Considerations","text":"DPI Quality Speed Memory Best For 100 Low Fast Low Quick previews 150 Medium Medium Medium General use 200 Good Slow Medium Default setting 300 High Very Slow High High-quality scans"},{"location":"api/engines.html#metadata","title":"Metadata","text":"<p>The <code>restore_image()</code> method returns metadata:</p> <pre><code>restored, metadata = engine.restore_image(\"doc.jpg\", \"appearance\")\n\nprint(metadata)\n# {\n#     'task': 'appearance',\n#     'device': 'cuda',\n#     'processing_time': 1.23,\n#     'input_size': (1920, 1080),\n#     'output_size': (1920, 1080)\n# }\n</code></pre>"},{"location":"api/engines.html#error-handling","title":"Error Handling","text":"<pre><code>from doctra import DocResEngine\n\nengine = DocResEngine(device=\"cuda\")\n\ntry:\n    restored, meta = engine.restore_image(\"document.jpg\", \"appearance\")\nexcept FileNotFoundError:\n    print(\"Image not found\")\nexcept RuntimeError as e:\n    print(f\"CUDA error: {e}\")\n    # Fall back to CPU\n    engine = DocResEngine(device=\"cpu\")\n    restored, meta = engine.restore_image(\"document.jpg\", \"appearance\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"api/engines.html#integration-with-parsers","title":"Integration with Parsers","text":"<p>DocResEngine is integrated into EnhancedPDFParser:</p> <pre><code>from doctra import EnhancedPDFParser\n\n# This internally uses DocResEngine\nparser = EnhancedPDFParser(\n    use_image_restoration=True,\n    restoration_task=\"appearance\",\n    restoration_device=\"cuda\"\n)\n\nparser.parse(\"document.pdf\")\n</code></pre> <p>For standalone restoration:</p> <pre><code>from doctra import DocResEngine\n\n# Step 1: Restore PDF\nengine = DocResEngine(device=\"cuda\")\nenhanced_pdf = engine.restore_pdf(\n    pdf_path=\"low_quality.pdf\",\n    output_path=\"enhanced.pdf\",\n    task=\"appearance\"\n)\n\n# Step 2: Parse enhanced PDF\nfrom doctra import StructuredPDFParser\n\nparser = StructuredPDFParser()\nparser.parse(enhanced_pdf)\n</code></pre>"},{"location":"api/engines.html#examples","title":"Examples","text":""},{"location":"api/engines.html#example-1-dewarp-scanned-document","title":"Example 1: Dewarp Scanned Document","text":"<pre><code>from doctra import DocResEngine\n\nengine = DocResEngine(device=\"cuda\")\n\n# Fix perspective distortion\nrestored, meta = engine.restore_image(\n    \"scanned_with_distortion.jpg\",\n    task=\"dewarping\"\n)\n\nrestored.save(\"dewarped.jpg\")\nprint(f\"Processed in {meta['processing_time']:.2f}s\")\n</code></pre>"},{"location":"api/engines.html#example-2-remove-shadows","title":"Example 2: Remove Shadows","text":"<pre><code>from doctra import DocResEngine\n\nengine = DocResEngine(device=\"cuda\")\n\n# Remove shadow artifacts\nrestored, meta = engine.restore_image(\n    \"document_with_shadows.jpg\",\n    task=\"deshadowing\"\n)\n\nrestored.save(\"no_shadows.jpg\")\n</code></pre>"},{"location":"api/engines.html#example-3-batch-pdf-restoration","title":"Example 3: Batch PDF Restoration","text":"<pre><code>import os\nfrom doctra import DocResEngine\n\nengine = DocResEngine(device=\"cuda\", use_half_precision=True)\n\npdf_dir = \"input_pdfs\"\noutput_dir = \"restored_pdfs\"\nos.makedirs(output_dir, exist_ok=True)\n\nfor filename in os.listdir(pdf_dir):\n    if filename.endswith(\".pdf\"):\n        input_path = os.path.join(pdf_dir, filename)\n        output_path = os.path.join(output_dir, f\"restored_{filename}\")\n\n        print(f\"Processing {filename}...\")\n        engine.restore_pdf(\n            pdf_path=input_path,\n            output_path=output_path,\n            task=\"appearance\",\n            dpi=200\n        )\n</code></pre>"},{"location":"api/engines.html#see-also","title":"See Also","text":"<ul> <li>Enhanced Parser - Using restoration with parsing</li> <li>Core Concepts - Understanding image restoration</li> <li>Examples - Advanced usage patterns</li> </ul>"},{"location":"api/exporters.html","title":"Exporters API Reference","text":"<p>Documentation for Doctra's export functionality.</p>"},{"location":"api/exporters.html#overview","title":"Overview","text":"<p>Exporters handle converting parsed document content into various output formats.</p>"},{"location":"api/exporters.html#available-exporters","title":"Available Exporters","text":""},{"location":"api/exporters.html#markdownwriter","title":"MarkdownWriter","text":"<p>Generates human-readable Markdown files with embedded images.</p>"},{"location":"api/exporters.html#htmlwriter","title":"HTMLWriter","text":"<p>Produces styled HTML documents for web viewing.</p>"},{"location":"api/exporters.html#excelwriter","title":"ExcelWriter","text":"<p>Creates Excel spreadsheets with structured data from tables and charts.</p>"},{"location":"api/exporters.html#imagesaver","title":"ImageSaver","text":"<p>Saves cropped images of visual elements (figures, charts, tables).</p>"},{"location":"api/exporters.html#usage","title":"Usage","text":"<p>Exporters are used automatically by parsers. Output format is determined by parser configuration.</p>"},{"location":"api/exporters.html#output-files","title":"Output Files","text":"<pre><code>outputs/\n\u2514\u2500\u2500 document/\n    \u251c\u2500\u2500 result.md          # MarkdownWriter\n    \u251c\u2500\u2500 result.html        # HTMLWriter\n    \u251c\u2500\u2500 tables.xlsx        # ExcelWriter (with VLM)\n    \u251c\u2500\u2500 tables.html        # HTMLWriter (with VLM)\n    \u2514\u2500\u2500 images/            # ImageSaver\n        \u251c\u2500\u2500 figures/\n        \u251c\u2500\u2500 charts/\n        \u2514\u2500\u2500 tables/\n</code></pre>"},{"location":"api/exporters.html#see-also","title":"See Also","text":"<ul> <li>Parsers API - Main parsing functionality</li> <li>Export Formats - Detailed format documentation</li> </ul>"},{"location":"api/parsers.html","title":"Parsers API Reference","text":"<p>Complete API documentation for all Doctra parsers.</p>"},{"location":"api/parsers.html#structuredpdfparser","title":"StructuredPDFParser","text":"<p>The base parser for comprehensive PDF document processing.</p>"},{"location":"api/parsers.html#doctra.parsers.structured_pdf_parser.StructuredPDFParser","title":"<code>doctra.parsers.structured_pdf_parser.StructuredPDFParser</code>","text":"<pre><code>Comprehensive PDF parser for extracting all types of content.\n\nProcesses PDF documents to extract text, tables, charts, and figures.\nSupports OCR for text extraction and optional VLM processing for\nconverting visual elements into structured data.\n\n:param use_vlm: Whether to use VLM for structured data extraction (default: False)\n:param vlm_provider: VLM provider to use (\"gemini\", \"openai\", \"anthropic\", or \"openrouter\", default: \"gemini\")\n:param vlm_model: Model name to use (defaults to provider-specific defaults)\n:param vlm_api_key: API key for VLM provider (required if use_vlm is True)\n:param layout_model_name: Layout detection model name (default: \"PP-DocLayout_plus-L\")\n:param dpi: DPI for PDF rendering (default: 200)\n:param min_score: Minimum confidence score for layout detection (default: 0.0)\n:param ocr_lang: OCR language code (default: \"eng\")\n:param ocr_psm: Tesseract page segmentation mode (default: 4)\n:param ocr_oem: Tesseract OCR engine mode (default: 3)\n:param ocr_extra_config: Additional Tesseract configuration (default: \"\")\n:param box_separator: Separator between text boxes in output (default: \"\n</code></pre> <p>\")</p> Source code in <code>doctra/parsers/structured_pdf_parser.py</code> <pre><code>class StructuredPDFParser:\n    \"\"\"\n    Comprehensive PDF parser for extracting all types of content.\n\n    Processes PDF documents to extract text, tables, charts, and figures.\n    Supports OCR for text extraction and optional VLM processing for\n    converting visual elements into structured data.\n\n    :param use_vlm: Whether to use VLM for structured data extraction (default: False)\n    :param vlm_provider: VLM provider to use (\"gemini\", \"openai\", \"anthropic\", or \"openrouter\", default: \"gemini\")\n    :param vlm_model: Model name to use (defaults to provider-specific defaults)\n    :param vlm_api_key: API key for VLM provider (required if use_vlm is True)\n    :param layout_model_name: Layout detection model name (default: \"PP-DocLayout_plus-L\")\n    :param dpi: DPI for PDF rendering (default: 200)\n    :param min_score: Minimum confidence score for layout detection (default: 0.0)\n    :param ocr_lang: OCR language code (default: \"eng\")\n    :param ocr_psm: Tesseract page segmentation mode (default: 4)\n    :param ocr_oem: Tesseract OCR engine mode (default: 3)\n    :param ocr_extra_config: Additional Tesseract configuration (default: \"\")\n    :param box_separator: Separator between text boxes in output (default: \"\\n\")\n    \"\"\"\n\n    def __init__(\n            self,\n            *,\n            use_vlm: bool = False,\n            vlm_provider: str = \"gemini\",\n            vlm_model: str | None = None,\n            vlm_api_key: str | None = None,\n            layout_model_name: str = \"PP-DocLayout_plus-L\",\n            dpi: int = 200,\n            min_score: float = 0.0,\n            ocr_lang: str = \"eng\",\n            ocr_psm: int = 4,\n            ocr_oem: int = 3,\n            ocr_extra_config: str = \"\",\n            box_separator: str = \"\\n\",\n    ):\n        \"\"\"\n        Initialize the StructuredPDFParser with processing configuration.\n\n        :param use_vlm: Whether to use VLM for structured data extraction (default: False)\n        :param vlm_provider: VLM provider to use (\"gemini\", \"openai\", \"anthropic\", or \"openrouter\", default: \"gemini\")\n        :param vlm_model: Model name to use (defaults to provider-specific defaults)\n        :param vlm_api_key: API key for VLM provider (required if use_vlm is True)\n        :param layout_model_name: Layout detection model name (default: \"PP-DocLayout_plus-L\")\n        :param dpi: DPI for PDF rendering (default: 200)\n        :param min_score: Minimum confidence score for layout detection (default: 0.0)\n        :param ocr_lang: OCR language code (default: \"eng\")\n        :param ocr_psm: Tesseract page segmentation mode (default: 4)\n        :param ocr_oem: Tesseract OCR engine mode (default: 3)\n        :param ocr_extra_config: Additional Tesseract configuration (default: \"\")\n        :param box_separator: Separator between text boxes in output (default: \"\\n\")\n        \"\"\"\n        self.layout_engine = PaddleLayoutEngine(model_name=layout_model_name)\n        self.dpi = dpi\n        self.min_score = min_score\n        self.ocr_engine = PytesseractOCREngine(\n            lang=ocr_lang, psm=ocr_psm, oem=ocr_oem, extra_config=ocr_extra_config\n        )\n        self.box_separator = box_separator\n        self.use_vlm = use_vlm\n        self.vlm = None\n        if self.use_vlm:\n            try:\n                self.vlm = VLMStructuredExtractor(\n                    vlm_provider=vlm_provider,\n                    vlm_model=vlm_model,\n                    api_key=vlm_api_key,\n                )\n            except Exception as e:\n                self.vlm = None\n\n    def parse(self, pdf_path: str) -&gt; None:\n        \"\"\"\n        Parse a PDF document and extract all content types.\n\n        :param pdf_path: Path to the input PDF file\n        :return: None\n        \"\"\"\n        pdf_filename = os.path.splitext(os.path.basename(pdf_path))[0]\n        out_dir = f\"outputs/{pdf_filename}/full_parse\"\n\n        os.makedirs(out_dir, exist_ok=True)\n        ensure_output_dirs(out_dir, IMAGE_SUBDIRS)\n\n        pages: List[LayoutPage] = self.layout_engine.predict_pdf(\n            pdf_path, batch_size=1, layout_nms=True, dpi=self.dpi, min_score=self.min_score\n        )\n        pil_pages = [im for (im, _, _) in render_pdf_to_images(pdf_path, dpi=self.dpi)]\n\n        fig_count = sum(sum(1 for b in p.boxes if b.label == \"figure\") for p in pages)\n        chart_count = sum(sum(1 for b in p.boxes if b.label == \"chart\") for p in pages)\n        table_count = sum(sum(1 for b in p.boxes if b.label == \"table\") for p in pages)\n\n        md_lines: List[str] = [\"# Extracted Content\\n\"]\n        html_lines: List[str] = [\"&lt;h1&gt;Extracted Content&lt;/h1&gt;\"]  # For direct HTML generation\n        structured_items: List[Dict[str, Any]] = []\n\n        charts_desc = \"Charts (VLM \u2192 table)\" if self.use_vlm else \"Charts (cropped)\"\n        tables_desc = \"Tables (VLM \u2192 table)\" if self.use_vlm else \"Tables (cropped)\"\n        figures_desc = \"Figures (cropped)\"\n\n        with ExitStack() as stack:\n            is_notebook = \"ipykernel\" in sys.modules or \"jupyter\" in sys.modules\n            is_terminal = hasattr(sys.stdout, 'isatty') and sys.stdout.isatty()\n            if is_notebook:\n                charts_bar = stack.enter_context(\n                    create_notebook_friendly_bar(total=chart_count, desc=charts_desc)) if chart_count else None\n                tables_bar = stack.enter_context(\n                    create_notebook_friendly_bar(total=table_count, desc=tables_desc)) if table_count else None\n                figures_bar = stack.enter_context(\n                    create_notebook_friendly_bar(total=fig_count, desc=figures_desc)) if fig_count else None\n            else:\n                charts_bar = stack.enter_context(\n                    create_beautiful_progress_bar(total=chart_count, desc=charts_desc, leave=True)) if chart_count else None\n                tables_bar = stack.enter_context(\n                    create_beautiful_progress_bar(total=table_count, desc=tables_desc, leave=True)) if table_count else None\n                figures_bar = stack.enter_context(\n                    create_beautiful_progress_bar(total=fig_count, desc=figures_desc, leave=True)) if fig_count else None\n\n            for p in pages:\n                page_num = p.page_index\n                page_img: Image.Image = pil_pages[page_num - 1]\n                md_lines.append(f\"\\n## Page {page_num}\\n\")\n                html_lines.append(f\"&lt;h2&gt;Page {page_num}&lt;/h2&gt;\")\n\n                for i, box in enumerate(sorted(p.boxes, key=reading_order_key), start=1):\n                    if box.label in EXCLUDE_LABELS:\n                        img_path = save_box_image(page_img, box, out_dir, page_num, i, IMAGE_SUBDIRS)\n                        abs_img_path = os.path.abspath(img_path)\n                        rel = os.path.relpath(abs_img_path, out_dir)\n\n                        if box.label == \"figure\":\n                            figure_md = f\"![Figure \u2014 page {page_num}]({rel})\\n\"\n                            figure_html = f'&lt;img src=\"{rel}\" alt=\"Figure \u2014 page {page_num}\" /&gt;'\n                            md_lines.append(figure_md)\n                            html_lines.append(figure_html)\n                            if figures_bar: figures_bar.update(1)\n\n                        elif box.label == \"chart\":\n                            if self.use_vlm and self.vlm:\n                                wrote_table = False\n                                try:\n                                    chart = self.vlm.extract_chart(abs_img_path)\n                                    item = to_structured_dict(chart)\n                                    if item:\n                                        # Add page and type information to structured item\n                                        item[\"page\"] = page_num\n                                        item[\"type\"] = \"Chart\"\n                                        structured_items.append(item)\n\n                                        # Generate both markdown and HTML tables\n                                        table_md = render_markdown_table(item.get(\"headers\"), item.get(\"rows\"),\n                                                                         title=item.get(\"title\"))\n                                        table_html = render_html_table(item.get(\"headers\"), item.get(\"rows\"),\n                                                                       title=item.get(\"title\"))\n\n                                        md_lines.append(table_md)\n                                        html_lines.append(table_html)\n                                        wrote_table = True\n                                except Exception as e:\n                                    pass\n                                if not wrote_table:\n                                    chart_md = f\"![Chart \u2014 page {page_num}]({rel})\\n\"\n                                    chart_html = f'&lt;img src=\"{rel}\" alt=\"Chart \u2014 page {page_num}\" /&gt;'\n                                    md_lines.append(chart_md)\n                                    html_lines.append(chart_html)\n                            else:\n                                chart_md = f\"![Chart \u2014 page {page_num}]({rel})\\n\"\n                                chart_html = f'&lt;img src=\"{rel}\" alt=\"Chart \u2014 page {page_num}\" /&gt;'\n                                md_lines.append(chart_md)\n                                html_lines.append(chart_html)\n                            if charts_bar: charts_bar.update(1)\n\n                        elif box.label == \"table\":\n                            if self.use_vlm and self.vlm:\n                                wrote_table = False\n                                try:\n                                    table = self.vlm.extract_table(abs_img_path)\n                                    item = to_structured_dict(table)\n                                    if item:\n                                        # Add page and type information to structured item\n                                        item[\"page\"] = page_num\n                                        item[\"type\"] = \"Table\"\n                                        structured_items.append(item)\n\n                                        # Generate both markdown and HTML tables\n                                        table_md = render_markdown_table(item.get(\"headers\"), item.get(\"rows\"),\n                                                                         title=item.get(\"title\"))\n                                        table_html = render_html_table(item.get(\"headers\"), item.get(\"rows\"),\n                                                                       title=item.get(\"title\"))\n\n                                        md_lines.append(table_md)\n                                        html_lines.append(table_html)\n                                        wrote_table = True\n                                except Exception as e:\n                                    pass\n                                if not wrote_table:\n                                    table_md = f\"![Table \u2014 page {page_num}]({rel})\\n\"\n                                    table_html = f'&lt;img src=\"{rel}\" alt=\"Table \u2014 page {page_num}\" /&gt;'\n                                    md_lines.append(table_md)\n                                    html_lines.append(table_html)\n                            else:\n                                table_md = f\"![Table \u2014 page {page_num}]({rel})\\n\"\n                                table_html = f'&lt;img src=\"{rel}\" alt=\"Table \u2014 page {page_num}\" /&gt;'\n                                md_lines.append(table_md)\n                                html_lines.append(table_html)\n                            if tables_bar: tables_bar.update(1)\n                    else:\n                        text = ocr_box_text(self.ocr_engine, page_img, box)\n                        if text:\n                            md_lines.append(text)\n                            md_lines.append(self.box_separator if self.box_separator else \"\")\n                            # Convert text to HTML (basic conversion)\n                            html_text = text.replace('\\n', '&lt;br&gt;')\n                            html_lines.append(f\"&lt;p&gt;{html_text}&lt;/p&gt;\")\n                            if self.box_separator:\n                                html_lines.append(\"&lt;br&gt;\")\n\n        md_path = write_markdown(md_lines, out_dir)\n\n        # Use HTML lines if VLM is enabled for better table formatting\n        if self.use_vlm and html_lines:\n            html_path = write_html_from_lines(html_lines, out_dir)\n        else:\n            html_path = write_html(md_lines, out_dir)\n\n        excel_path = None\n        html_structured_path = None\n        if self.use_vlm and structured_items:\n            excel_path = os.path.join(out_dir, \"tables.xlsx\")\n            write_structured_excel(excel_path, structured_items)\n            html_structured_path = os.path.join(out_dir, \"tables.html\")\n            write_structured_html(html_structured_path, structured_items)\n\n        print(f\"\u2705 Parsing completed successfully!\")\n        print(f\"\ud83d\udcc1 Output directory: {out_dir}\")\n\n    def display_pages_with_boxes(self, pdf_path: str, num_pages: int = 3, cols: int = 2,\n                                 page_width: int = 800, spacing: int = 40, save_path: str = None) -&gt; None:\n        \"\"\"\n        Display the first N pages of a PDF with bounding boxes and labels overlaid in a modern grid layout.\n\n        Creates a visualization showing layout detection results with bounding boxes,\n        labels, and confidence scores overlaid on the PDF pages in a grid format.\n\n        :param pdf_path: Path to the input PDF file\n        :param num_pages: Number of pages to display (default: 3)\n        :param cols: Number of columns in the grid layout (default: 2)\n        :param page_width: Width to resize each page to in pixels (default: 800)\n        :param spacing: Spacing between pages in pixels (default: 40)\n        :param save_path: Optional path to save the visualization (if None, displays only)\n        :return: None\n        \"\"\"\n        pages: List[LayoutPage] = self.layout_engine.predict_pdf(\n            pdf_path, batch_size=1, layout_nms=True, dpi=self.dpi, min_score=self.min_score\n        )\n        pil_pages = [im for (im, _, _) in render_pdf_to_images(pdf_path, dpi=self.dpi)]\n\n        pages_to_show = min(num_pages, len(pages))\n\n        if pages_to_show == 0:\n            print(\"No pages to display\")\n            return\n\n        rows = (pages_to_show + cols - 1) // cols\n\n        used_labels = set()\n        for idx in range(pages_to_show):\n            page = pages[idx]\n            for box in page.boxes:\n                used_labels.add(box.label.lower())\n\n        base_colors = ['#3B82F6', '#EF4444', '#10B981', '#F59E0B', '#8B5CF6',\n                       '#F97316', '#EC4899', '#6B7280', '#84CC16', '#06B6D4',\n                       '#DC2626', '#059669', '#7C3AED', '#DB2777', '#0891B2']\n\n        dynamic_label_colors = {}\n        for i, label in enumerate(sorted(used_labels)):\n            dynamic_label_colors[label] = base_colors[i % len(base_colors)]\n\n        processed_pages = []\n\n        for idx in range(pages_to_show):\n            page = pages[idx]\n            page_img = pil_pages[idx].copy()\n\n            scale_factor = page_width / page_img.width\n            new_height = int(page_img.height * scale_factor)\n            page_img = page_img.resize((page_width, new_height), Image.LANCZOS)\n\n            draw = ImageDraw.Draw(page_img)\n\n            try:\n                font = ImageFont.truetype(\"arial.ttf\", 24)\n                small_font = ImageFont.truetype(\"arial.ttf\", 18)\n            except:\n                try:\n                    font = ImageFont.load_default()\n                    small_font = ImageFont.load_default()\n                except:\n                    font = None\n                    small_font = None\n\n            for box in page.boxes:\n                x1 = int(box.x1 * scale_factor)\n                y1 = int(box.y1 * scale_factor)\n                x2 = int(box.x2 * scale_factor)\n                y2 = int(box.y2 * scale_factor)\n\n                color = dynamic_label_colors.get(box.label.lower(), '#000000')\n\n                draw.rectangle([x1, y1, x2, y2], outline=color, width=3)\n\n                label_text = f\"{box.label} ({box.score:.2f})\"\n                if font:\n                    bbox = draw.textbbox((0, 0), label_text, font=small_font)\n                    text_width = bbox[2] - bbox[0]\n                    text_height = bbox[3] - bbox[1]\n                else:\n                    text_width = len(label_text) * 8\n                    text_height = 15\n\n                label_x = x1\n                label_y = max(0, y1 - text_height - 8)\n\n                padding = 4\n                draw.rectangle([\n                    label_x - padding,\n                    label_y - padding,\n                    label_x + text_width + padding,\n                    label_y + text_height + padding\n                ], fill='white', outline=color, width=2)\n\n                draw.text((label_x, label_y), label_text, fill=color, font=small_font)\n\n            title_text = f\"Page {page.page_index} ({len(page.boxes)} boxes)\"\n            if font:\n                title_bbox = draw.textbbox((0, 0), title_text, font=font)\n                title_width = title_bbox[2] - title_bbox[0]\n            else:\n                title_width = len(title_text) * 12\n\n            title_x = (page_width - title_width) // 2\n            title_y = 10\n            draw.rectangle([title_x - 10, title_y - 5, title_x + title_width + 10, title_y + 35],\n                           fill='white', outline='#1F2937', width=2)\n            draw.text((title_x, title_y), title_text, fill='#1F2937', font=font)\n\n            processed_pages.append(page_img)\n\n        legend_width = 250\n        grid_width = cols * page_width + (cols - 1) * spacing\n        total_width = grid_width + legend_width + spacing\n        grid_height = rows * (processed_pages[0].height if processed_pages else 600) + (rows - 1) * spacing\n\n        final_img = Image.new('RGB', (total_width, grid_height), '#F8FAFC')\n\n        for idx, page_img in enumerate(processed_pages):\n            row = idx // cols\n            col = idx % cols\n\n            x_pos = col * (page_width + spacing)\n            y_pos = row * (page_img.height + spacing)\n\n            final_img.paste(page_img, (x_pos, y_pos))\n\n        legend_x = grid_width + spacing\n        legend_y = 20\n\n        draw_legend = ImageDraw.Draw(final_img)\n\n        legend_title = \"Element Types\"\n        if font:\n            title_bbox = draw_legend.textbbox((0, 0), legend_title, font=font)\n            title_width = title_bbox[2] - title_bbox[0]\n            title_height = title_bbox[3] - title_bbox[1]\n        else:\n            title_width = len(legend_title) * 12\n            title_height = 20\n\n        legend_bg_height = len(used_labels) * 35 + title_height + 40\n        draw_legend.rectangle([legend_x - 10, legend_y - 10,\n                               legend_x + legend_width - 10, legend_y + legend_bg_height],\n                              fill='white', outline='#E5E7EB', width=2)\n\n        draw_legend.text((legend_x + 10, legend_y + 5), legend_title,\n                         fill='#1F2937', font=font)\n\n        current_y = legend_y + title_height + 20\n\n        for label in sorted(used_labels):\n            color = dynamic_label_colors[label]\n\n            square_size = 20\n            draw_legend.rectangle([legend_x + 10, current_y,\n                                   legend_x + 10 + square_size, current_y + square_size],\n                                  fill=color, outline='#6B7280', width=1)\n\n            draw_legend.text((legend_x + 40, current_y + 2), label.title(),\n                             fill='#374151', font=small_font)\n\n            current_y += 30\n\n        if save_path:\n            final_img.save(save_path, quality=95, optimize=True)\n            print(f\"Layout visualization saved to: {save_path}\")\n        else:\n            final_img.show()\n\n        print(f\"\\n\ud83d\udcca Layout Detection Summary for {os.path.basename(pdf_path)}:\")\n        print(f\"Pages processed: {pages_to_show}\")\n\n        total_counts = {}\n        for idx in range(pages_to_show):\n            page = pages[idx]\n            for box in page.boxes:\n                total_counts[box.label] = total_counts.get(box.label, 0) + 1\n\n        print(\"\\nTotal elements detected:\")\n        for label, count in sorted(total_counts.items()):\n            print(f\"  - {label}: {count}\")\n\n        return final_img\n</code></pre>"},{"location":"api/parsers.html#doctra.parsers.structured_pdf_parser.StructuredPDFParser.__init__","title":"<code>__init__(*, use_vlm=False, vlm_provider='gemini', vlm_model=None, vlm_api_key=None, layout_model_name='PP-DocLayout_plus-L', dpi=200, min_score=0.0, ocr_lang='eng', ocr_psm=4, ocr_oem=3, ocr_extra_config='', box_separator='\\n')</code>","text":"<pre><code>    Initialize the StructuredPDFParser with processing configuration.\n\n    :param use_vlm: Whether to use VLM for structured data extraction (default: False)\n    :param vlm_provider: VLM provider to use (\"gemini\", \"openai\", \"anthropic\", or \"openrouter\", default: \"gemini\")\n    :param vlm_model: Model name to use (defaults to provider-specific defaults)\n    :param vlm_api_key: API key for VLM provider (required if use_vlm is True)\n    :param layout_model_name: Layout detection model name (default: \"PP-DocLayout_plus-L\")\n    :param dpi: DPI for PDF rendering (default: 200)\n    :param min_score: Minimum confidence score for layout detection (default: 0.0)\n    :param ocr_lang: OCR language code (default: \"eng\")\n    :param ocr_psm: Tesseract page segmentation mode (default: 4)\n    :param ocr_oem: Tesseract OCR engine mode (default: 3)\n    :param ocr_extra_config: Additional Tesseract configuration (default: \"\")\n    :param box_separator: Separator between text boxes in output (default: \"\n</code></pre> <p>\")</p> Source code in <code>doctra/parsers/structured_pdf_parser.py</code> <pre><code>def __init__(\n        self,\n        *,\n        use_vlm: bool = False,\n        vlm_provider: str = \"gemini\",\n        vlm_model: str | None = None,\n        vlm_api_key: str | None = None,\n        layout_model_name: str = \"PP-DocLayout_plus-L\",\n        dpi: int = 200,\n        min_score: float = 0.0,\n        ocr_lang: str = \"eng\",\n        ocr_psm: int = 4,\n        ocr_oem: int = 3,\n        ocr_extra_config: str = \"\",\n        box_separator: str = \"\\n\",\n):\n    \"\"\"\n    Initialize the StructuredPDFParser with processing configuration.\n\n    :param use_vlm: Whether to use VLM for structured data extraction (default: False)\n    :param vlm_provider: VLM provider to use (\"gemini\", \"openai\", \"anthropic\", or \"openrouter\", default: \"gemini\")\n    :param vlm_model: Model name to use (defaults to provider-specific defaults)\n    :param vlm_api_key: API key for VLM provider (required if use_vlm is True)\n    :param layout_model_name: Layout detection model name (default: \"PP-DocLayout_plus-L\")\n    :param dpi: DPI for PDF rendering (default: 200)\n    :param min_score: Minimum confidence score for layout detection (default: 0.0)\n    :param ocr_lang: OCR language code (default: \"eng\")\n    :param ocr_psm: Tesseract page segmentation mode (default: 4)\n    :param ocr_oem: Tesseract OCR engine mode (default: 3)\n    :param ocr_extra_config: Additional Tesseract configuration (default: \"\")\n    :param box_separator: Separator between text boxes in output (default: \"\\n\")\n    \"\"\"\n    self.layout_engine = PaddleLayoutEngine(model_name=layout_model_name)\n    self.dpi = dpi\n    self.min_score = min_score\n    self.ocr_engine = PytesseractOCREngine(\n        lang=ocr_lang, psm=ocr_psm, oem=ocr_oem, extra_config=ocr_extra_config\n    )\n    self.box_separator = box_separator\n    self.use_vlm = use_vlm\n    self.vlm = None\n    if self.use_vlm:\n        try:\n            self.vlm = VLMStructuredExtractor(\n                vlm_provider=vlm_provider,\n                vlm_model=vlm_model,\n                api_key=vlm_api_key,\n            )\n        except Exception as e:\n            self.vlm = None\n</code></pre>"},{"location":"api/parsers.html#doctra.parsers.structured_pdf_parser.StructuredPDFParser.display_pages_with_boxes","title":"<code>display_pages_with_boxes(pdf_path, num_pages=3, cols=2, page_width=800, spacing=40, save_path=None)</code>","text":"<p>Display the first N pages of a PDF with bounding boxes and labels overlaid in a modern grid layout.</p> <p>Creates a visualization showing layout detection results with bounding boxes, labels, and confidence scores overlaid on the PDF pages in a grid format.</p> <p>:param pdf_path: Path to the input PDF file :param num_pages: Number of pages to display (default: 3) :param cols: Number of columns in the grid layout (default: 2) :param page_width: Width to resize each page to in pixels (default: 800) :param spacing: Spacing between pages in pixels (default: 40) :param save_path: Optional path to save the visualization (if None, displays only) :return: None</p> Source code in <code>doctra/parsers/structured_pdf_parser.py</code> <pre><code>def display_pages_with_boxes(self, pdf_path: str, num_pages: int = 3, cols: int = 2,\n                             page_width: int = 800, spacing: int = 40, save_path: str = None) -&gt; None:\n    \"\"\"\n    Display the first N pages of a PDF with bounding boxes and labels overlaid in a modern grid layout.\n\n    Creates a visualization showing layout detection results with bounding boxes,\n    labels, and confidence scores overlaid on the PDF pages in a grid format.\n\n    :param pdf_path: Path to the input PDF file\n    :param num_pages: Number of pages to display (default: 3)\n    :param cols: Number of columns in the grid layout (default: 2)\n    :param page_width: Width to resize each page to in pixels (default: 800)\n    :param spacing: Spacing between pages in pixels (default: 40)\n    :param save_path: Optional path to save the visualization (if None, displays only)\n    :return: None\n    \"\"\"\n    pages: List[LayoutPage] = self.layout_engine.predict_pdf(\n        pdf_path, batch_size=1, layout_nms=True, dpi=self.dpi, min_score=self.min_score\n    )\n    pil_pages = [im for (im, _, _) in render_pdf_to_images(pdf_path, dpi=self.dpi)]\n\n    pages_to_show = min(num_pages, len(pages))\n\n    if pages_to_show == 0:\n        print(\"No pages to display\")\n        return\n\n    rows = (pages_to_show + cols - 1) // cols\n\n    used_labels = set()\n    for idx in range(pages_to_show):\n        page = pages[idx]\n        for box in page.boxes:\n            used_labels.add(box.label.lower())\n\n    base_colors = ['#3B82F6', '#EF4444', '#10B981', '#F59E0B', '#8B5CF6',\n                   '#F97316', '#EC4899', '#6B7280', '#84CC16', '#06B6D4',\n                   '#DC2626', '#059669', '#7C3AED', '#DB2777', '#0891B2']\n\n    dynamic_label_colors = {}\n    for i, label in enumerate(sorted(used_labels)):\n        dynamic_label_colors[label] = base_colors[i % len(base_colors)]\n\n    processed_pages = []\n\n    for idx in range(pages_to_show):\n        page = pages[idx]\n        page_img = pil_pages[idx].copy()\n\n        scale_factor = page_width / page_img.width\n        new_height = int(page_img.height * scale_factor)\n        page_img = page_img.resize((page_width, new_height), Image.LANCZOS)\n\n        draw = ImageDraw.Draw(page_img)\n\n        try:\n            font = ImageFont.truetype(\"arial.ttf\", 24)\n            small_font = ImageFont.truetype(\"arial.ttf\", 18)\n        except:\n            try:\n                font = ImageFont.load_default()\n                small_font = ImageFont.load_default()\n            except:\n                font = None\n                small_font = None\n\n        for box in page.boxes:\n            x1 = int(box.x1 * scale_factor)\n            y1 = int(box.y1 * scale_factor)\n            x2 = int(box.x2 * scale_factor)\n            y2 = int(box.y2 * scale_factor)\n\n            color = dynamic_label_colors.get(box.label.lower(), '#000000')\n\n            draw.rectangle([x1, y1, x2, y2], outline=color, width=3)\n\n            label_text = f\"{box.label} ({box.score:.2f})\"\n            if font:\n                bbox = draw.textbbox((0, 0), label_text, font=small_font)\n                text_width = bbox[2] - bbox[0]\n                text_height = bbox[3] - bbox[1]\n            else:\n                text_width = len(label_text) * 8\n                text_height = 15\n\n            label_x = x1\n            label_y = max(0, y1 - text_height - 8)\n\n            padding = 4\n            draw.rectangle([\n                label_x - padding,\n                label_y - padding,\n                label_x + text_width + padding,\n                label_y + text_height + padding\n            ], fill='white', outline=color, width=2)\n\n            draw.text((label_x, label_y), label_text, fill=color, font=small_font)\n\n        title_text = f\"Page {page.page_index} ({len(page.boxes)} boxes)\"\n        if font:\n            title_bbox = draw.textbbox((0, 0), title_text, font=font)\n            title_width = title_bbox[2] - title_bbox[0]\n        else:\n            title_width = len(title_text) * 12\n\n        title_x = (page_width - title_width) // 2\n        title_y = 10\n        draw.rectangle([title_x - 10, title_y - 5, title_x + title_width + 10, title_y + 35],\n                       fill='white', outline='#1F2937', width=2)\n        draw.text((title_x, title_y), title_text, fill='#1F2937', font=font)\n\n        processed_pages.append(page_img)\n\n    legend_width = 250\n    grid_width = cols * page_width + (cols - 1) * spacing\n    total_width = grid_width + legend_width + spacing\n    grid_height = rows * (processed_pages[0].height if processed_pages else 600) + (rows - 1) * spacing\n\n    final_img = Image.new('RGB', (total_width, grid_height), '#F8FAFC')\n\n    for idx, page_img in enumerate(processed_pages):\n        row = idx // cols\n        col = idx % cols\n\n        x_pos = col * (page_width + spacing)\n        y_pos = row * (page_img.height + spacing)\n\n        final_img.paste(page_img, (x_pos, y_pos))\n\n    legend_x = grid_width + spacing\n    legend_y = 20\n\n    draw_legend = ImageDraw.Draw(final_img)\n\n    legend_title = \"Element Types\"\n    if font:\n        title_bbox = draw_legend.textbbox((0, 0), legend_title, font=font)\n        title_width = title_bbox[2] - title_bbox[0]\n        title_height = title_bbox[3] - title_bbox[1]\n    else:\n        title_width = len(legend_title) * 12\n        title_height = 20\n\n    legend_bg_height = len(used_labels) * 35 + title_height + 40\n    draw_legend.rectangle([legend_x - 10, legend_y - 10,\n                           legend_x + legend_width - 10, legend_y + legend_bg_height],\n                          fill='white', outline='#E5E7EB', width=2)\n\n    draw_legend.text((legend_x + 10, legend_y + 5), legend_title,\n                     fill='#1F2937', font=font)\n\n    current_y = legend_y + title_height + 20\n\n    for label in sorted(used_labels):\n        color = dynamic_label_colors[label]\n\n        square_size = 20\n        draw_legend.rectangle([legend_x + 10, current_y,\n                               legend_x + 10 + square_size, current_y + square_size],\n                              fill=color, outline='#6B7280', width=1)\n\n        draw_legend.text((legend_x + 40, current_y + 2), label.title(),\n                         fill='#374151', font=small_font)\n\n        current_y += 30\n\n    if save_path:\n        final_img.save(save_path, quality=95, optimize=True)\n        print(f\"Layout visualization saved to: {save_path}\")\n    else:\n        final_img.show()\n\n    print(f\"\\n\ud83d\udcca Layout Detection Summary for {os.path.basename(pdf_path)}:\")\n    print(f\"Pages processed: {pages_to_show}\")\n\n    total_counts = {}\n    for idx in range(pages_to_show):\n        page = pages[idx]\n        for box in page.boxes:\n            total_counts[box.label] = total_counts.get(box.label, 0) + 1\n\n    print(\"\\nTotal elements detected:\")\n    for label, count in sorted(total_counts.items()):\n        print(f\"  - {label}: {count}\")\n\n    return final_img\n</code></pre>"},{"location":"api/parsers.html#doctra.parsers.structured_pdf_parser.StructuredPDFParser.parse","title":"<code>parse(pdf_path)</code>","text":"<p>Parse a PDF document and extract all content types.</p> <p>:param pdf_path: Path to the input PDF file :return: None</p> Source code in <code>doctra/parsers/structured_pdf_parser.py</code> <pre><code>def parse(self, pdf_path: str) -&gt; None:\n    \"\"\"\n    Parse a PDF document and extract all content types.\n\n    :param pdf_path: Path to the input PDF file\n    :return: None\n    \"\"\"\n    pdf_filename = os.path.splitext(os.path.basename(pdf_path))[0]\n    out_dir = f\"outputs/{pdf_filename}/full_parse\"\n\n    os.makedirs(out_dir, exist_ok=True)\n    ensure_output_dirs(out_dir, IMAGE_SUBDIRS)\n\n    pages: List[LayoutPage] = self.layout_engine.predict_pdf(\n        pdf_path, batch_size=1, layout_nms=True, dpi=self.dpi, min_score=self.min_score\n    )\n    pil_pages = [im for (im, _, _) in render_pdf_to_images(pdf_path, dpi=self.dpi)]\n\n    fig_count = sum(sum(1 for b in p.boxes if b.label == \"figure\") for p in pages)\n    chart_count = sum(sum(1 for b in p.boxes if b.label == \"chart\") for p in pages)\n    table_count = sum(sum(1 for b in p.boxes if b.label == \"table\") for p in pages)\n\n    md_lines: List[str] = [\"# Extracted Content\\n\"]\n    html_lines: List[str] = [\"&lt;h1&gt;Extracted Content&lt;/h1&gt;\"]  # For direct HTML generation\n    structured_items: List[Dict[str, Any]] = []\n\n    charts_desc = \"Charts (VLM \u2192 table)\" if self.use_vlm else \"Charts (cropped)\"\n    tables_desc = \"Tables (VLM \u2192 table)\" if self.use_vlm else \"Tables (cropped)\"\n    figures_desc = \"Figures (cropped)\"\n\n    with ExitStack() as stack:\n        is_notebook = \"ipykernel\" in sys.modules or \"jupyter\" in sys.modules\n        is_terminal = hasattr(sys.stdout, 'isatty') and sys.stdout.isatty()\n        if is_notebook:\n            charts_bar = stack.enter_context(\n                create_notebook_friendly_bar(total=chart_count, desc=charts_desc)) if chart_count else None\n            tables_bar = stack.enter_context(\n                create_notebook_friendly_bar(total=table_count, desc=tables_desc)) if table_count else None\n            figures_bar = stack.enter_context(\n                create_notebook_friendly_bar(total=fig_count, desc=figures_desc)) if fig_count else None\n        else:\n            charts_bar = stack.enter_context(\n                create_beautiful_progress_bar(total=chart_count, desc=charts_desc, leave=True)) if chart_count else None\n            tables_bar = stack.enter_context(\n                create_beautiful_progress_bar(total=table_count, desc=tables_desc, leave=True)) if table_count else None\n            figures_bar = stack.enter_context(\n                create_beautiful_progress_bar(total=fig_count, desc=figures_desc, leave=True)) if fig_count else None\n\n        for p in pages:\n            page_num = p.page_index\n            page_img: Image.Image = pil_pages[page_num - 1]\n            md_lines.append(f\"\\n## Page {page_num}\\n\")\n            html_lines.append(f\"&lt;h2&gt;Page {page_num}&lt;/h2&gt;\")\n\n            for i, box in enumerate(sorted(p.boxes, key=reading_order_key), start=1):\n                if box.label in EXCLUDE_LABELS:\n                    img_path = save_box_image(page_img, box, out_dir, page_num, i, IMAGE_SUBDIRS)\n                    abs_img_path = os.path.abspath(img_path)\n                    rel = os.path.relpath(abs_img_path, out_dir)\n\n                    if box.label == \"figure\":\n                        figure_md = f\"![Figure \u2014 page {page_num}]({rel})\\n\"\n                        figure_html = f'&lt;img src=\"{rel}\" alt=\"Figure \u2014 page {page_num}\" /&gt;'\n                        md_lines.append(figure_md)\n                        html_lines.append(figure_html)\n                        if figures_bar: figures_bar.update(1)\n\n                    elif box.label == \"chart\":\n                        if self.use_vlm and self.vlm:\n                            wrote_table = False\n                            try:\n                                chart = self.vlm.extract_chart(abs_img_path)\n                                item = to_structured_dict(chart)\n                                if item:\n                                    # Add page and type information to structured item\n                                    item[\"page\"] = page_num\n                                    item[\"type\"] = \"Chart\"\n                                    structured_items.append(item)\n\n                                    # Generate both markdown and HTML tables\n                                    table_md = render_markdown_table(item.get(\"headers\"), item.get(\"rows\"),\n                                                                     title=item.get(\"title\"))\n                                    table_html = render_html_table(item.get(\"headers\"), item.get(\"rows\"),\n                                                                   title=item.get(\"title\"))\n\n                                    md_lines.append(table_md)\n                                    html_lines.append(table_html)\n                                    wrote_table = True\n                            except Exception as e:\n                                pass\n                            if not wrote_table:\n                                chart_md = f\"![Chart \u2014 page {page_num}]({rel})\\n\"\n                                chart_html = f'&lt;img src=\"{rel}\" alt=\"Chart \u2014 page {page_num}\" /&gt;'\n                                md_lines.append(chart_md)\n                                html_lines.append(chart_html)\n                        else:\n                            chart_md = f\"![Chart \u2014 page {page_num}]({rel})\\n\"\n                            chart_html = f'&lt;img src=\"{rel}\" alt=\"Chart \u2014 page {page_num}\" /&gt;'\n                            md_lines.append(chart_md)\n                            html_lines.append(chart_html)\n                        if charts_bar: charts_bar.update(1)\n\n                    elif box.label == \"table\":\n                        if self.use_vlm and self.vlm:\n                            wrote_table = False\n                            try:\n                                table = self.vlm.extract_table(abs_img_path)\n                                item = to_structured_dict(table)\n                                if item:\n                                    # Add page and type information to structured item\n                                    item[\"page\"] = page_num\n                                    item[\"type\"] = \"Table\"\n                                    structured_items.append(item)\n\n                                    # Generate both markdown and HTML tables\n                                    table_md = render_markdown_table(item.get(\"headers\"), item.get(\"rows\"),\n                                                                     title=item.get(\"title\"))\n                                    table_html = render_html_table(item.get(\"headers\"), item.get(\"rows\"),\n                                                                   title=item.get(\"title\"))\n\n                                    md_lines.append(table_md)\n                                    html_lines.append(table_html)\n                                    wrote_table = True\n                            except Exception as e:\n                                pass\n                            if not wrote_table:\n                                table_md = f\"![Table \u2014 page {page_num}]({rel})\\n\"\n                                table_html = f'&lt;img src=\"{rel}\" alt=\"Table \u2014 page {page_num}\" /&gt;'\n                                md_lines.append(table_md)\n                                html_lines.append(table_html)\n                        else:\n                            table_md = f\"![Table \u2014 page {page_num}]({rel})\\n\"\n                            table_html = f'&lt;img src=\"{rel}\" alt=\"Table \u2014 page {page_num}\" /&gt;'\n                            md_lines.append(table_md)\n                            html_lines.append(table_html)\n                        if tables_bar: tables_bar.update(1)\n                else:\n                    text = ocr_box_text(self.ocr_engine, page_img, box)\n                    if text:\n                        md_lines.append(text)\n                        md_lines.append(self.box_separator if self.box_separator else \"\")\n                        # Convert text to HTML (basic conversion)\n                        html_text = text.replace('\\n', '&lt;br&gt;')\n                        html_lines.append(f\"&lt;p&gt;{html_text}&lt;/p&gt;\")\n                        if self.box_separator:\n                            html_lines.append(\"&lt;br&gt;\")\n\n    md_path = write_markdown(md_lines, out_dir)\n\n    # Use HTML lines if VLM is enabled for better table formatting\n    if self.use_vlm and html_lines:\n        html_path = write_html_from_lines(html_lines, out_dir)\n    else:\n        html_path = write_html(md_lines, out_dir)\n\n    excel_path = None\n    html_structured_path = None\n    if self.use_vlm and structured_items:\n        excel_path = os.path.join(out_dir, \"tables.xlsx\")\n        write_structured_excel(excel_path, structured_items)\n        html_structured_path = os.path.join(out_dir, \"tables.html\")\n        write_structured_html(html_structured_path, structured_items)\n\n    print(f\"\u2705 Parsing completed successfully!\")\n    print(f\"\ud83d\udcc1 Output directory: {out_dir}\")\n</code></pre>"},{"location":"api/parsers.html#enhancedpdfparser","title":"EnhancedPDFParser","text":"<p>Enhanced parser with image restoration capabilities.</p>"},{"location":"api/parsers.html#doctra.parsers.enhanced_pdf_parser.EnhancedPDFParser","title":"<code>doctra.parsers.enhanced_pdf_parser.EnhancedPDFParser</code>","text":"<p>               Bases: <code>StructuredPDFParser</code></p> <pre><code>Enhanced PDF Parser with Image Restoration capabilities.\n\nExtends the StructuredPDFParser with DocRes image restoration to improve\ndocument quality before processing. This is particularly useful for:\n- Scanned documents with shadows or distortion\n- Low-quality PDFs that need enhancement\n- Documents with perspective issues\n\n:param use_image_restoration: Whether to apply DocRes image restoration (default: True)\n:param restoration_task: DocRes task to use (\"dewarping\", \"deshadowing\", \"appearance\", \"deblurring\", \"binarization\", \"end2end\", default: \"appearance\")\n:param restoration_device: Device for DocRes processing (\"cuda\", \"cpu\", or None for auto-detect, default: None)\n:param restoration_dpi: DPI for restoration processing (default: 200)\n:param use_vlm: Whether to use VLM for structured data extraction (default: False)\n:param vlm_provider: VLM provider to use (\"gemini\", \"openai\", \"anthropic\", or \"openrouter\", default: \"gemini\")\n:param vlm_model: Model name to use (defaults to provider-specific defaults)\n:param vlm_api_key: API key for VLM provider (required if use_vlm is True)\n:param layout_model_name: Layout detection model name (default: \"PP-DocLayout_plus-L\")\n:param dpi: DPI for PDF rendering (default: 200)\n:param min_score: Minimum confidence score for layout detection (default: 0.0)\n:param ocr_lang: OCR language code (default: \"eng\")\n:param ocr_psm: Tesseract page segmentation mode (default: 4)\n:param ocr_oem: Tesseract OCR engine mode (default: 3)\n:param ocr_extra_config: Additional Tesseract configuration (default: \"\")\n:param box_separator: Separator between text boxes in output (default: \"\n</code></pre> <p>\")</p> Source code in <code>doctra/parsers/enhanced_pdf_parser.py</code> <pre><code>class EnhancedPDFParser(StructuredPDFParser):\n    \"\"\"\n    Enhanced PDF Parser with Image Restoration capabilities.\n\n    Extends the StructuredPDFParser with DocRes image restoration to improve\n    document quality before processing. This is particularly useful for:\n    - Scanned documents with shadows or distortion\n    - Low-quality PDFs that need enhancement\n    - Documents with perspective issues\n\n    :param use_image_restoration: Whether to apply DocRes image restoration (default: True)\n    :param restoration_task: DocRes task to use (\"dewarping\", \"deshadowing\", \"appearance\", \"deblurring\", \"binarization\", \"end2end\", default: \"appearance\")\n    :param restoration_device: Device for DocRes processing (\"cuda\", \"cpu\", or None for auto-detect, default: None)\n    :param restoration_dpi: DPI for restoration processing (default: 200)\n    :param use_vlm: Whether to use VLM for structured data extraction (default: False)\n    :param vlm_provider: VLM provider to use (\"gemini\", \"openai\", \"anthropic\", or \"openrouter\", default: \"gemini\")\n    :param vlm_model: Model name to use (defaults to provider-specific defaults)\n    :param vlm_api_key: API key for VLM provider (required if use_vlm is True)\n    :param layout_model_name: Layout detection model name (default: \"PP-DocLayout_plus-L\")\n    :param dpi: DPI for PDF rendering (default: 200)\n    :param min_score: Minimum confidence score for layout detection (default: 0.0)\n    :param ocr_lang: OCR language code (default: \"eng\")\n    :param ocr_psm: Tesseract page segmentation mode (default: 4)\n    :param ocr_oem: Tesseract OCR engine mode (default: 3)\n    :param ocr_extra_config: Additional Tesseract configuration (default: \"\")\n    :param box_separator: Separator between text boxes in output (default: \"\\n\")\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        use_image_restoration: bool = True,\n        restoration_task: str = \"appearance\",\n        restoration_device: Optional[str] = None,\n        restoration_dpi: int = 200,\n        use_vlm: bool = False,\n        vlm_provider: str = \"gemini\",\n        vlm_model: str | None = None,\n        vlm_api_key: str | None = None,\n        layout_model_name: str = \"PP-DocLayout_plus-L\",\n        dpi: int = 200,\n        min_score: float = 0.0,\n        ocr_lang: str = \"eng\",\n        ocr_psm: int = 4,\n        ocr_oem: int = 3,\n        ocr_extra_config: str = \"\",\n        box_separator: str = \"\\n\",\n    ):\n        \"\"\"\n        Initialize the Enhanced PDF Parser with image restoration capabilities.\n        \"\"\"\n        # Initialize parent class\n        super().__init__(\n            use_vlm=use_vlm,\n            vlm_provider=vlm_provider,\n            vlm_model=vlm_model,\n            vlm_api_key=vlm_api_key,\n            layout_model_name=layout_model_name,\n            dpi=dpi,\n            min_score=min_score,\n            ocr_lang=ocr_lang,\n            ocr_psm=ocr_psm,\n            ocr_oem=ocr_oem,\n            ocr_extra_config=ocr_extra_config,\n            box_separator=box_separator,\n        )\n\n        # Image restoration settings\n        self.use_image_restoration = use_image_restoration\n        self.restoration_task = restoration_task\n        self.restoration_device = restoration_device\n        self.restoration_dpi = restoration_dpi\n\n        # Initialize DocRes engine if needed\n        self.docres_engine = None\n        if self.use_image_restoration:\n            try:\n                self.docres_engine = DocResEngine(\n                    device=restoration_device,\n                    use_half_precision=True\n                )\n                print(f\"\u2705 DocRes engine initialized with task: {restoration_task}\")\n            except Exception as e:\n                print(f\"\u26a0\ufe0f DocRes initialization failed: {e}\")\n                print(\"   Continuing without image restoration...\")\n                self.use_image_restoration = False\n                self.docres_engine = None\n\n    def parse(self, pdf_path: str, enhanced_output_dir: str = None) -&gt; None:\n        \"\"\"\n        Parse a PDF document with optional image restoration.\n\n        :param pdf_path: Path to the input PDF file\n        :param enhanced_output_dir: Directory for enhanced images (if None, uses default)\n        :return: None\n        \"\"\"\n        pdf_filename = os.path.splitext(os.path.basename(pdf_path))[0]\n\n        # Set up output directories\n        if enhanced_output_dir is None:\n            out_dir = f\"outputs/{pdf_filename}/enhanced_parse\"\n        else:\n            out_dir = enhanced_output_dir\n\n        os.makedirs(out_dir, exist_ok=True)\n        ensure_output_dirs(out_dir, IMAGE_SUBDIRS)\n\n        # Process PDF pages with optional restoration\n        if self.use_image_restoration and self.docres_engine:\n            print(f\"\ud83d\udd04 Processing PDF with image restoration: {os.path.basename(pdf_path)}\")\n            enhanced_pages = self._process_pages_with_restoration(pdf_path, out_dir)\n\n            # Create enhanced PDF file using the already processed enhanced pages\n            enhanced_pdf_path = os.path.join(out_dir, f\"{pdf_filename}_enhanced.pdf\")\n            try:\n                self._create_enhanced_pdf_from_pages(enhanced_pages, enhanced_pdf_path)\n            except Exception as e:\n                print(f\"\u26a0\ufe0f Failed to create enhanced PDF: {e}\")\n        else:\n            print(f\"\ud83d\udd04 Processing PDF without image restoration: {os.path.basename(pdf_path)}\")\n            enhanced_pages = [im for (im, _, _) in render_pdf_to_images(pdf_path, dpi=self.dpi)]\n\n        # Run layout detection on enhanced pages\n        print(\"\ud83d\udd0d Running layout detection on enhanced pages...\")\n        pages = self.layout_engine.predict_pdf(\n            pdf_path, batch_size=1, layout_nms=True, dpi=self.dpi, min_score=self.min_score\n        )\n\n        # Use enhanced pages for processing\n        pil_pages = enhanced_pages\n\n        # Continue with standard parsing logic\n        self._process_parsing_logic(pages, pil_pages, out_dir, pdf_filename, pdf_path)\n\n    def _process_pages_with_restoration(self, pdf_path: str, out_dir: str) -&gt; List[Image.Image]:\n        \"\"\"\n        Process PDF pages with DocRes image restoration.\n\n        :param pdf_path: Path to the input PDF file\n        :param out_dir: Output directory for enhanced images\n        :return: List of enhanced PIL images\n        \"\"\"\n        # Render original pages\n        original_pages = [im for (im, _, _) in render_pdf_to_images(pdf_path, dpi=self.restoration_dpi)]\n\n        if not original_pages:\n            print(\"\u274c No pages found in PDF\")\n            return []\n\n        # Create progress bar\n        is_notebook = \"ipykernel\" in sys.modules or \"jupyter\" in sys.modules\n        if is_notebook:\n            progress_bar = create_notebook_friendly_bar(\n                total=len(original_pages), \n                desc=f\"DocRes {self.restoration_task}\"\n            )\n        else:\n            progress_bar = create_beautiful_progress_bar(\n                total=len(original_pages), \n                desc=f\"DocRes {self.restoration_task}\",\n                leave=True\n            )\n\n        enhanced_pages = []\n        enhanced_dir = os.path.join(out_dir, \"enhanced_pages\")\n        os.makedirs(enhanced_dir, exist_ok=True)\n\n        try:\n            with progress_bar:\n                for i, page_img in enumerate(original_pages):\n                    try:\n                        # Convert PIL to numpy array\n                        img_array = np.array(page_img)\n\n                        # Apply DocRes restoration\n                        restored_img, metadata = self.docres_engine.restore_image(\n                            img_array, \n                            task=self.restoration_task\n                        )\n\n                        # Convert back to PIL Image\n                        enhanced_page = Image.fromarray(restored_img)\n                        enhanced_pages.append(enhanced_page)\n\n                        # Save enhanced page for reference\n                        enhanced_path = os.path.join(enhanced_dir, f\"page_{i+1:03d}_enhanced.jpg\")\n                        enhanced_page.save(enhanced_path, \"JPEG\", quality=95)\n\n                        progress_bar.set_description(f\"\u2705 Page {i+1}/{len(original_pages)} enhanced\")\n                        progress_bar.update(1)\n\n                    except Exception as e:\n                        print(f\"  \u26a0\ufe0f Page {i+1} restoration failed: {e}, using original\")\n                        enhanced_pages.append(page_img)\n                        progress_bar.set_description(f\"\u26a0\ufe0f Page {i+1} failed, using original\")\n                        progress_bar.update(1)\n\n        finally:\n            if hasattr(progress_bar, 'close'):\n                progress_bar.close()\n\n        return enhanced_pages\n\n    def _process_parsing_logic(self, pages, pil_pages, out_dir, pdf_filename, pdf_path):\n        \"\"\"\n        Process the parsing logic with enhanced pages.\n        This is extracted from the parent class to allow customization.\n        \"\"\"\n\n        fig_count = sum(sum(1 for b in p.boxes if b.label == \"figure\") for p in pages)\n        chart_count = sum(sum(1 for b in p.boxes if b.label == \"chart\") for p in pages)\n        table_count = sum(sum(1 for b in p.boxes if b.label == \"table\") for p in pages)\n\n        md_lines: List[str] = [\"# Enhanced Document Content\\n\"]\n        html_lines: List[str] = [\"&lt;h1&gt;Enhanced Document Content&lt;/h1&gt;\"]  # For direct HTML generation\n        structured_items: List[Dict[str, Any]] = []\n        page_content: Dict[int, List[str]] = {}  # Store content by page\n\n        charts_desc = \"Charts (VLM \u2192 table)\" if self.use_vlm else \"Charts (cropped)\"\n        tables_desc = \"Tables (VLM \u2192 table)\" if self.use_vlm else \"Tables (cropped)\"\n        figures_desc = \"Figures (cropped)\"\n\n        with ExitStack() as stack:\n            is_notebook = \"ipykernel\" in sys.modules or \"jupyter\" in sys.modules\n            if is_notebook:\n                charts_bar = stack.enter_context(\n                    create_notebook_friendly_bar(total=chart_count, desc=charts_desc)) if chart_count else None\n                tables_bar = stack.enter_context(\n                    create_notebook_friendly_bar(total=table_count, desc=tables_desc)) if table_count else None\n                figures_bar = stack.enter_context(\n                    create_notebook_friendly_bar(total=fig_count, desc=figures_desc)) if fig_count else None\n            else:\n                charts_bar = stack.enter_context(\n                    create_beautiful_progress_bar(total=chart_count, desc=charts_desc, leave=True)) if chart_count else None\n                tables_bar = stack.enter_context(\n                    create_beautiful_progress_bar(total=table_count, desc=tables_desc, leave=True)) if table_count else None\n                figures_bar = stack.enter_context(\n                    create_beautiful_progress_bar(total=fig_count, desc=figures_desc, leave=True)) if fig_count else None\n\n            # Initialize page content for all pages first\n            for page_num in range(1, len(pil_pages) + 1):\n                page_content[page_num] = [f\"# Page {page_num} Content\\n\"]\n\n            for p in pages:\n                page_num = p.page_index\n                page_img: Image.Image = pil_pages[page_num - 1]\n                md_lines.append(f\"\\n## Page {page_num}\\n\")\n                html_lines.append(f\"&lt;h2&gt;Page {page_num}&lt;/h2&gt;\")\n\n                for i, box in enumerate(sorted(p.boxes, key=reading_order_key), start=1):\n                    if box.label in EXCLUDE_LABELS:\n                        img_path = save_box_image(page_img, box, out_dir, page_num, i, IMAGE_SUBDIRS)\n                        abs_img_path = os.path.abspath(img_path)\n                        rel = os.path.relpath(abs_img_path, out_dir)\n\n                        if box.label == \"figure\":\n                            figure_md = f\"![Figure \u2014 page {page_num}]({rel})\\n\"\n                            figure_html = f'&lt;img src=\"{rel}\" alt=\"Figure \u2014 page {page_num}\" /&gt;'\n                            md_lines.append(figure_md)\n                            html_lines.append(figure_html)\n                            page_content[page_num].append(figure_md)\n                            if figures_bar: figures_bar.update(1)\n\n                        elif box.label == \"chart\":\n                            if self.use_vlm and self.vlm:\n                                wrote_table = False\n                                try:\n                                    chart = self.vlm.extract_chart(abs_img_path)\n                                    item = to_structured_dict(chart)\n                                    if item:\n                                        # Add page and type information to structured item\n                                        item[\"page\"] = page_num\n                                        item[\"type\"] = \"Chart\"\n                                        structured_items.append(item)\n\n                                        # Generate both markdown and HTML tables\n                                        table_md = render_markdown_table(item.get(\"headers\"), item.get(\"rows\"),\n                                                                         title=item.get(\"title\"))\n                                        table_html = render_html_table(item.get(\"headers\"), item.get(\"rows\"),\n                                                                       title=item.get(\"title\"))\n\n                                        md_lines.append(table_md)\n                                        html_lines.append(table_html)\n                                        page_content[page_num].append(table_md)\n                                        wrote_table = True\n                                except Exception as e:\n                                    pass\n                                if not wrote_table:\n                                    chart_md = f\"![Chart \u2014 page {page_num}]({rel})\\n\"\n                                    chart_html = f'&lt;img src=\"{rel}\" alt=\"Chart \u2014 page {page_num}\" /&gt;'\n                                    md_lines.append(chart_md)\n                                    html_lines.append(chart_html)\n                                    page_content[page_num].append(chart_md)\n                            else:\n                                chart_md = f\"![Chart \u2014 page {page_num}]({rel})\\n\"\n                                chart_html = f'&lt;img src=\"{rel}\" alt=\"Chart \u2014 page {page_num}\" /&gt;'\n                                md_lines.append(chart_md)\n                                html_lines.append(chart_html)\n                                page_content[page_num].append(chart_md)\n                            if charts_bar: charts_bar.update(1)\n\n                        elif box.label == \"table\":\n                            if self.use_vlm and self.vlm:\n                                wrote_table = False\n                                try:\n                                    table = self.vlm.extract_table(abs_img_path)\n                                    item = to_structured_dict(table)\n                                    if item:\n                                        # Add page and type information to structured item\n                                        item[\"page\"] = page_num\n                                        item[\"type\"] = \"Table\"\n                                        structured_items.append(item)\n\n                                        # Generate both markdown and HTML tables\n                                        table_md = render_markdown_table(item.get(\"headers\"), item.get(\"rows\"),\n                                                                         title=item.get(\"title\"))\n                                        table_html = render_html_table(item.get(\"headers\"), item.get(\"rows\"),\n                                                                       title=item.get(\"title\"))\n\n                                        md_lines.append(table_md)\n                                        html_lines.append(table_html)\n                                        page_content[page_num].append(table_md)\n                                        wrote_table = True\n                                except Exception as e:\n                                    pass\n                                if not wrote_table:\n                                    table_md = f\"![Table \u2014 page {page_num}]({rel})\\n\"\n                                    table_html = f'&lt;img src=\"{rel}\" alt=\"Table \u2014 page {page_num}\" /&gt;'\n                                    md_lines.append(table_md)\n                                    html_lines.append(table_html)\n                                    page_content[page_num].append(table_md)\n                            else:\n                                table_md = f\"![Table \u2014 page {page_num}]({rel})\\n\"\n                                table_html = f'&lt;img src=\"{rel}\" alt=\"Table \u2014 page {page_num}\" /&gt;'\n                                md_lines.append(table_md)\n                                html_lines.append(table_html)\n                                page_content[page_num].append(table_md)\n                            if tables_bar: tables_bar.update(1)\n                    else:\n                        text = ocr_box_text(self.ocr_engine, page_img, box)\n                        if text:\n                            md_lines.append(text)\n                            md_lines.append(self.box_separator if self.box_separator else \"\")\n                            # Convert text to HTML (basic conversion)\n                            html_text = text.replace('\\n', '&lt;br&gt;')\n                            html_lines.append(f\"&lt;p&gt;{html_text}&lt;/p&gt;\")\n                            if self.box_separator:\n                                html_lines.append(\"&lt;br&gt;\")\n                            page_content[page_num].append(text)\n                            page_content[page_num].append(self.box_separator if self.box_separator else \"\")\n\n        md_path = write_markdown(md_lines, out_dir)\n\n        # Use HTML lines if VLM is enabled for better table formatting\n        if self.use_vlm and html_lines:\n            html_path = write_html_from_lines(html_lines, out_dir)\n        else:\n            html_path = write_html(md_lines, out_dir)\n\n        # Create pages folder and save individual page markdown files\n        pages_dir = os.path.join(out_dir, \"pages\")\n        os.makedirs(pages_dir, exist_ok=True)\n\n        for page_num, content_lines in page_content.items():\n            page_md_path = os.path.join(pages_dir, f\"page_{page_num:03d}.md\")\n            write_markdown(content_lines, os.path.dirname(page_md_path), os.path.basename(page_md_path))\n\n        excel_path = None\n        html_structured_path = None\n        if self.use_vlm and structured_items:\n            excel_path = os.path.join(out_dir, \"tables.xlsx\")\n            write_structured_excel(excel_path, structured_items)\n            html_structured_path = os.path.join(out_dir, \"tables.html\")\n            write_structured_html(html_structured_path, structured_items)\n\n        print(f\"\u2705 Enhanced parsing completed successfully!\")\n        print(f\"\ud83d\udcc1 Output directory: {out_dir}\")\n\n    def _create_enhanced_pdf_from_pages(self, enhanced_pages: List[Image.Image], output_path: str) -&gt; None:\n        \"\"\"\n        Create an enhanced PDF from already processed enhanced pages.\n\n        :param enhanced_pages: List of enhanced PIL images\n        :param output_path: Path for the enhanced PDF\n        \"\"\"\n        if not enhanced_pages:\n            raise ValueError(\"No enhanced pages provided\")\n\n        try:\n            # Create enhanced PDF from the processed pages\n            enhanced_pages[0].save(\n                output_path,\n                \"PDF\",\n                resolution=100.0,\n                save_all=True,\n                append_images=enhanced_pages[1:] if len(enhanced_pages) &gt; 1 else []\n            )\n            print(f\"\u2705 Enhanced PDF saved from processed pages: {output_path}\")\n        except Exception as e:\n            print(f\"\u274c Error creating enhanced PDF from pages: {e}\")\n            raise\n\n    def restore_pdf_only(self, pdf_path: str, output_path: str = None, task: str = None) -&gt; str:\n        \"\"\"\n        Apply DocRes restoration to a PDF without parsing.\n\n        :param pdf_path: Path to the input PDF file\n        :param output_path: Path for the enhanced PDF (if None, auto-generates)\n        :param task: DocRes restoration task (if None, uses instance default)\n        :return: Path to the enhanced PDF or None if failed\n        \"\"\"\n        if not self.use_image_restoration or not self.docres_engine:\n            raise RuntimeError(\"Image restoration is not enabled or DocRes engine is not available\")\n\n        task = task or self.restoration_task\n        return self.docres_engine.restore_pdf(pdf_path, output_path, task, self.restoration_dpi)\n\n    def get_restoration_info(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get information about the current restoration configuration.\n\n        :return: Dictionary with restoration settings and status\n        \"\"\"\n        return {\n            'enabled': self.use_image_restoration,\n            'task': self.restoration_task,\n            'device': self.restoration_device,\n            'dpi': self.restoration_dpi,\n            'engine_available': self.docres_engine is not None,\n            'supported_tasks': self.docres_engine.get_supported_tasks() if self.docres_engine else []\n        }\n</code></pre>"},{"location":"api/parsers.html#doctra.parsers.enhanced_pdf_parser.EnhancedPDFParser.__init__","title":"<code>__init__(*, use_image_restoration=True, restoration_task='appearance', restoration_device=None, restoration_dpi=200, use_vlm=False, vlm_provider='gemini', vlm_model=None, vlm_api_key=None, layout_model_name='PP-DocLayout_plus-L', dpi=200, min_score=0.0, ocr_lang='eng', ocr_psm=4, ocr_oem=3, ocr_extra_config='', box_separator='\\n')</code>","text":"<p>Initialize the Enhanced PDF Parser with image restoration capabilities.</p> Source code in <code>doctra/parsers/enhanced_pdf_parser.py</code> <pre><code>def __init__(\n    self,\n    *,\n    use_image_restoration: bool = True,\n    restoration_task: str = \"appearance\",\n    restoration_device: Optional[str] = None,\n    restoration_dpi: int = 200,\n    use_vlm: bool = False,\n    vlm_provider: str = \"gemini\",\n    vlm_model: str | None = None,\n    vlm_api_key: str | None = None,\n    layout_model_name: str = \"PP-DocLayout_plus-L\",\n    dpi: int = 200,\n    min_score: float = 0.0,\n    ocr_lang: str = \"eng\",\n    ocr_psm: int = 4,\n    ocr_oem: int = 3,\n    ocr_extra_config: str = \"\",\n    box_separator: str = \"\\n\",\n):\n    \"\"\"\n    Initialize the Enhanced PDF Parser with image restoration capabilities.\n    \"\"\"\n    # Initialize parent class\n    super().__init__(\n        use_vlm=use_vlm,\n        vlm_provider=vlm_provider,\n        vlm_model=vlm_model,\n        vlm_api_key=vlm_api_key,\n        layout_model_name=layout_model_name,\n        dpi=dpi,\n        min_score=min_score,\n        ocr_lang=ocr_lang,\n        ocr_psm=ocr_psm,\n        ocr_oem=ocr_oem,\n        ocr_extra_config=ocr_extra_config,\n        box_separator=box_separator,\n    )\n\n    # Image restoration settings\n    self.use_image_restoration = use_image_restoration\n    self.restoration_task = restoration_task\n    self.restoration_device = restoration_device\n    self.restoration_dpi = restoration_dpi\n\n    # Initialize DocRes engine if needed\n    self.docres_engine = None\n    if self.use_image_restoration:\n        try:\n            self.docres_engine = DocResEngine(\n                device=restoration_device,\n                use_half_precision=True\n            )\n            print(f\"\u2705 DocRes engine initialized with task: {restoration_task}\")\n        except Exception as e:\n            print(f\"\u26a0\ufe0f DocRes initialization failed: {e}\")\n            print(\"   Continuing without image restoration...\")\n            self.use_image_restoration = False\n            self.docres_engine = None\n</code></pre>"},{"location":"api/parsers.html#doctra.parsers.enhanced_pdf_parser.EnhancedPDFParser.get_restoration_info","title":"<code>get_restoration_info()</code>","text":"<p>Get information about the current restoration configuration.</p> <p>:return: Dictionary with restoration settings and status</p> Source code in <code>doctra/parsers/enhanced_pdf_parser.py</code> <pre><code>def get_restoration_info(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get information about the current restoration configuration.\n\n    :return: Dictionary with restoration settings and status\n    \"\"\"\n    return {\n        'enabled': self.use_image_restoration,\n        'task': self.restoration_task,\n        'device': self.restoration_device,\n        'dpi': self.restoration_dpi,\n        'engine_available': self.docres_engine is not None,\n        'supported_tasks': self.docres_engine.get_supported_tasks() if self.docres_engine else []\n    }\n</code></pre>"},{"location":"api/parsers.html#doctra.parsers.enhanced_pdf_parser.EnhancedPDFParser.parse","title":"<code>parse(pdf_path, enhanced_output_dir=None)</code>","text":"<p>Parse a PDF document with optional image restoration.</p> <p>:param pdf_path: Path to the input PDF file :param enhanced_output_dir: Directory for enhanced images (if None, uses default) :return: None</p> Source code in <code>doctra/parsers/enhanced_pdf_parser.py</code> <pre><code>def parse(self, pdf_path: str, enhanced_output_dir: str = None) -&gt; None:\n    \"\"\"\n    Parse a PDF document with optional image restoration.\n\n    :param pdf_path: Path to the input PDF file\n    :param enhanced_output_dir: Directory for enhanced images (if None, uses default)\n    :return: None\n    \"\"\"\n    pdf_filename = os.path.splitext(os.path.basename(pdf_path))[0]\n\n    # Set up output directories\n    if enhanced_output_dir is None:\n        out_dir = f\"outputs/{pdf_filename}/enhanced_parse\"\n    else:\n        out_dir = enhanced_output_dir\n\n    os.makedirs(out_dir, exist_ok=True)\n    ensure_output_dirs(out_dir, IMAGE_SUBDIRS)\n\n    # Process PDF pages with optional restoration\n    if self.use_image_restoration and self.docres_engine:\n        print(f\"\ud83d\udd04 Processing PDF with image restoration: {os.path.basename(pdf_path)}\")\n        enhanced_pages = self._process_pages_with_restoration(pdf_path, out_dir)\n\n        # Create enhanced PDF file using the already processed enhanced pages\n        enhanced_pdf_path = os.path.join(out_dir, f\"{pdf_filename}_enhanced.pdf\")\n        try:\n            self._create_enhanced_pdf_from_pages(enhanced_pages, enhanced_pdf_path)\n        except Exception as e:\n            print(f\"\u26a0\ufe0f Failed to create enhanced PDF: {e}\")\n    else:\n        print(f\"\ud83d\udd04 Processing PDF without image restoration: {os.path.basename(pdf_path)}\")\n        enhanced_pages = [im for (im, _, _) in render_pdf_to_images(pdf_path, dpi=self.dpi)]\n\n    # Run layout detection on enhanced pages\n    print(\"\ud83d\udd0d Running layout detection on enhanced pages...\")\n    pages = self.layout_engine.predict_pdf(\n        pdf_path, batch_size=1, layout_nms=True, dpi=self.dpi, min_score=self.min_score\n    )\n\n    # Use enhanced pages for processing\n    pil_pages = enhanced_pages\n\n    # Continue with standard parsing logic\n    self._process_parsing_logic(pages, pil_pages, out_dir, pdf_filename, pdf_path)\n</code></pre>"},{"location":"api/parsers.html#doctra.parsers.enhanced_pdf_parser.EnhancedPDFParser.restore_pdf_only","title":"<code>restore_pdf_only(pdf_path, output_path=None, task=None)</code>","text":"<p>Apply DocRes restoration to a PDF without parsing.</p> <p>:param pdf_path: Path to the input PDF file :param output_path: Path for the enhanced PDF (if None, auto-generates) :param task: DocRes restoration task (if None, uses instance default) :return: Path to the enhanced PDF or None if failed</p> Source code in <code>doctra/parsers/enhanced_pdf_parser.py</code> <pre><code>def restore_pdf_only(self, pdf_path: str, output_path: str = None, task: str = None) -&gt; str:\n    \"\"\"\n    Apply DocRes restoration to a PDF without parsing.\n\n    :param pdf_path: Path to the input PDF file\n    :param output_path: Path for the enhanced PDF (if None, auto-generates)\n    :param task: DocRes restoration task (if None, uses instance default)\n    :return: Path to the enhanced PDF or None if failed\n    \"\"\"\n    if not self.use_image_restoration or not self.docres_engine:\n        raise RuntimeError(\"Image restoration is not enabled or DocRes engine is not available\")\n\n    task = task or self.restoration_task\n    return self.docres_engine.restore_pdf(pdf_path, output_path, task, self.restoration_dpi)\n</code></pre>"},{"location":"api/parsers.html#charttablepdfparser","title":"ChartTablePDFParser","text":"<p>Specialized parser for extracting charts and tables.</p>"},{"location":"api/parsers.html#doctra.parsers.table_chart_extractor.ChartTablePDFParser","title":"<code>doctra.parsers.table_chart_extractor.ChartTablePDFParser</code>","text":"<p>Specialized PDF parser for extracting charts and tables.</p> <p>Focuses specifically on chart and table extraction from PDF documents, with optional VLM (Vision Language Model) processing to convert visual elements into structured data.</p> <p>:param extract_charts: Whether to extract charts from the document (default: True) :param extract_tables: Whether to extract tables from the document (default: True) :param use_vlm: Whether to use VLM for structured data extraction (default: False) :param vlm_provider: VLM provider to use (\"gemini\", \"openai\", \"anthropic\", or \"openrouter\", default: \"gemini\") :param vlm_model: Model name to use (defaults to provider-specific defaults) :param vlm_api_key: API key for VLM provider (required if use_vlm is True) :param layout_model_name: Layout detection model name (default: \"PP-DocLayout_plus-L\") :param dpi: DPI for PDF rendering (default: 200) :param min_score: Minimum confidence score for layout detection (default: 0.0)</p> Source code in <code>doctra/parsers/table_chart_extractor.py</code> <pre><code>class ChartTablePDFParser:\n    \"\"\"\n    Specialized PDF parser for extracting charts and tables.\n\n    Focuses specifically on chart and table extraction from PDF documents,\n    with optional VLM (Vision Language Model) processing to convert visual\n    elements into structured data.\n\n    :param extract_charts: Whether to extract charts from the document (default: True)\n    :param extract_tables: Whether to extract tables from the document (default: True)\n    :param use_vlm: Whether to use VLM for structured data extraction (default: False)\n    :param vlm_provider: VLM provider to use (\"gemini\", \"openai\", \"anthropic\", or \"openrouter\", default: \"gemini\")\n    :param vlm_model: Model name to use (defaults to provider-specific defaults)\n    :param vlm_api_key: API key for VLM provider (required if use_vlm is True)\n    :param layout_model_name: Layout detection model name (default: \"PP-DocLayout_plus-L\")\n    :param dpi: DPI for PDF rendering (default: 200)\n    :param min_score: Minimum confidence score for layout detection (default: 0.0)\n    \"\"\"\n\n    def __init__(\n            self,\n            *,\n            extract_charts: bool = True,\n            extract_tables: bool = True,\n            use_vlm: bool = False,\n            vlm_provider: str = \"gemini\",\n            vlm_model: str | None = None,\n            vlm_api_key: str | None = None,\n            layout_model_name: str = \"PP-DocLayout_plus-L\",\n            dpi: int = 200,\n            min_score: float = 0.0,\n    ):\n        \"\"\"\n        Initialize the ChartTablePDFParser with extraction configuration.\n\n        :param extract_charts: Whether to extract charts from the document (default: True)\n        :param extract_tables: Whether to extract tables from the document (default: True)\n        :param use_vlm: Whether to use VLM for structured data extraction (default: False)\n        :param vlm_provider: VLM provider to use (\"gemini\", \"openai\", \"anthropic\", or \"openrouter\", default: \"gemini\")\n        :param vlm_model: Model name to use (defaults to provider-specific defaults)\n        :param vlm_api_key: API key for VLM provider (required if use_vlm is True)\n        :param layout_model_name: Layout detection model name (default: \"PP-DocLayout_plus-L\")\n        :param dpi: DPI for PDF rendering (default: 200)\n        :param min_score: Minimum confidence score for layout detection (default: 0.0)\n        \"\"\"\n        if not extract_charts and not extract_tables:\n            raise ValueError(\"At least one of extract_charts or extract_tables must be True\")\n\n        self.extract_charts = extract_charts\n        self.extract_tables = extract_tables\n        self.layout_engine = PaddleLayoutEngine(model_name=layout_model_name)\n        self.dpi = dpi\n        self.min_score = min_score\n\n        self.use_vlm = use_vlm\n        self.vlm = None\n        if self.use_vlm:\n            self.vlm = VLMStructuredExtractor(\n                vlm_provider=vlm_provider,\n                vlm_model=vlm_model,\n                api_key=vlm_api_key,\n            )\n\n    def parse(self, pdf_path: str, output_base_dir: str = \"outputs\") -&gt; None:\n        \"\"\"\n        Parse a PDF document and extract charts and/or tables.\n\n        :param pdf_path: Path to the input PDF file\n        :param output_base_dir: Base directory for output files (default: \"outputs\")\n        :return: None\n        \"\"\"\n        pdf_name = Path(pdf_path).stem\n        out_dir = os.path.join(output_base_dir, pdf_name, \"structured_parsing\")\n        os.makedirs(out_dir, exist_ok=True)\n\n        charts_dir = None\n        tables_dir = None\n\n        if self.extract_charts:\n            charts_dir = os.path.join(out_dir, \"charts\")\n            os.makedirs(charts_dir, exist_ok=True)\n\n        if self.extract_tables:\n            tables_dir = os.path.join(out_dir, \"tables\")\n            os.makedirs(tables_dir, exist_ok=True)\n\n        pages: List[LayoutPage] = self.layout_engine.predict_pdf(\n            pdf_path, batch_size=1, layout_nms=True, dpi=self.dpi, min_score=self.min_score\n        )\n        pil_pages = [im for (im, _, _) in render_pdf_to_images(pdf_path, dpi=self.dpi)]\n\n        target_labels = []\n        if self.extract_charts:\n            target_labels.append(\"chart\")\n        if self.extract_tables:\n            target_labels.append(\"table\")\n\n        chart_count = sum(sum(1 for b in p.boxes if b.label == \"chart\") for p in pages) if self.extract_charts else 0\n        table_count = sum(sum(1 for b in p.boxes if b.label == \"table\") for p in pages) if self.extract_tables else 0\n\n        if self.use_vlm:\n            md_lines: List[str] = [\"# Extracted Charts and Tables\\n\"]\n            structured_items: List[Dict[str, Any]] = []\n            vlm_items: List[Dict[str, Any]] = []\n\n        charts_desc = \"Charts (VLM \u2192 table)\" if self.use_vlm else \"Charts (cropped)\"\n        tables_desc = \"Tables (VLM \u2192 table)\" if self.use_vlm else \"Tables (cropped)\"\n\n        chart_counter = 1\n        table_counter = 1\n\n        with ExitStack() as stack:\n            is_notebook = \"ipykernel\" in sys.modules or \"jupyter\" in sys.modules\n            is_terminal = hasattr(sys.stdout, 'isatty') and sys.stdout.isatty()\n\n            if is_notebook:\n                charts_bar = stack.enter_context(\n                    create_notebook_friendly_bar(total=chart_count, desc=charts_desc)) if chart_count else None\n                tables_bar = stack.enter_context(\n                    create_notebook_friendly_bar(total=table_count, desc=tables_desc)) if table_count else None\n            else:\n                charts_bar = stack.enter_context(\n                    create_beautiful_progress_bar(total=chart_count, desc=charts_desc, leave=True)) if chart_count else None\n                tables_bar = stack.enter_context(\n                    create_beautiful_progress_bar(total=table_count, desc=tables_desc, leave=True)) if table_count else None\n\n            for p in pages:\n                page_num = p.page_index\n                page_img: Image.Image = pil_pages[page_num - 1]\n\n                target_items = [box for box in p.boxes if box.label in target_labels]\n\n                if target_items and self.use_vlm:\n                    md_lines.append(f\"\\n## Page {page_num}\\n\")\n\n                for box in sorted(target_items, key=reading_order_key):\n                    if box.label == \"chart\" and self.extract_charts:\n                        chart_filename = f\"chart_{chart_counter:03d}.png\"\n                        chart_path = os.path.join(charts_dir, chart_filename)\n\n                        cropped_img = page_img.crop((box.x1, box.y1, box.x2, box.y2))\n                        cropped_img.save(chart_path)\n\n                        if self.use_vlm and self.vlm:\n                            rel_path = os.path.join(\"charts\", chart_filename)\n                            wrote_table = False\n\n                            try:\n                                extracted_chart = self.vlm.extract_chart(chart_path)\n                                structured_item = to_structured_dict(extracted_chart)\n                                if structured_item:\n                                    # Add page and type information to structured item\n                                    structured_item[\"page\"] = page_num\n                                    structured_item[\"type\"] = \"Chart\"\n                                    structured_items.append(structured_item)\n                                    vlm_items.append({\n                                        \"kind\": \"chart\",\n                                        \"page\": page_num,\n                                        \"image_rel_path\": rel_path,\n                                        \"title\": structured_item.get(\"title\"),\n                                        \"headers\": structured_item.get(\"headers\"),\n                                        \"rows\": structured_item.get(\"rows\"),\n                                    })\n                                    md_lines.append(\n                                        render_markdown_table(\n                                            structured_item.get(\"headers\"),\n                                            structured_item.get(\"rows\"),\n                                            title=structured_item.get(\n                                                \"title\") or f\"Chart {chart_counter} \u2014 page {page_num}\"\n                                        )\n                                    )\n                                    wrote_table = True\n                            except Exception:\n                                pass\n\n                            if not wrote_table:\n                                md_lines.append(f\"![Chart {chart_counter} \u2014 page {page_num}]({rel_path})\\n\")\n\n                        chart_counter += 1\n                        if charts_bar:\n                            charts_bar.update(1)\n\n                    elif box.label == \"table\" and self.extract_tables:\n                        table_filename = f\"table_{table_counter:03d}.png\"\n                        table_path = os.path.join(tables_dir, table_filename)\n\n                        cropped_img = page_img.crop((box.x1, box.y1, box.x2, box.y2))\n                        cropped_img.save(table_path)\n\n                        if self.use_vlm and self.vlm:\n                            rel_path = os.path.join(\"tables\", table_filename)\n                            wrote_table = False\n\n                            try:\n                                extracted_table = self.vlm.extract_table(table_path)\n                                structured_item = to_structured_dict(extracted_table)\n                                if structured_item:\n                                    # Add page and type information to structured item\n                                    structured_item[\"page\"] = page_num\n                                    structured_item[\"type\"] = \"Table\"\n                                    structured_items.append(structured_item)\n                                    vlm_items.append({\n                                        \"kind\": \"table\",\n                                        \"page\": page_num,\n                                        \"image_rel_path\": rel_path,\n                                        \"title\": structured_item.get(\"title\"),\n                                        \"headers\": structured_item.get(\"headers\"),\n                                        \"rows\": structured_item.get(\"rows\"),\n                                    })\n                                    md_lines.append(\n                                        render_markdown_table(\n                                            structured_item.get(\"headers\"),\n                                            structured_item.get(\"rows\"),\n                                            title=structured_item.get(\n                                                \"title\") or f\"Table {table_counter} \u2014 page {page_num}\"\n                                        )\n                                    )\n                                    wrote_table = True\n                            except Exception:\n                                pass\n\n                            if not wrote_table:\n                                md_lines.append(f\"![Table {table_counter} \u2014 page {page_num}]({rel_path})\\n\")\n\n                        table_counter += 1\n                        if tables_bar:\n                            tables_bar.update(1)\n\n        excel_path = None\n\n        if self.use_vlm:\n\n            if structured_items:\n                if self.extract_charts and self.extract_tables:\n                    excel_filename = \"parsed_tables_charts.xlsx\"\n                elif self.extract_charts:\n                    excel_filename = \"parsed_charts.xlsx\"\n                elif self.extract_tables:\n                    excel_filename = \"parsed_tables.xlsx\"\n                else:\n                    excel_filename = \"parsed_data.xlsx\"  # fallback\n\n\n                excel_path = os.path.join(out_dir, excel_filename)\n                write_structured_excel(excel_path, structured_items)\n\n                html_filename = excel_filename.replace('.xlsx', '.html')\n                html_path = os.path.join(out_dir, html_filename)\n                write_structured_html(html_path, structured_items)\n\n            if 'vlm_items' in locals() and vlm_items:\n                with open(os.path.join(out_dir, \"vlm_items.json\"), 'w', encoding='utf-8') as jf:\n                    json.dump(vlm_items, jf, ensure_ascii=False, indent=2)\n\n        extraction_types = []\n        if self.extract_charts:\n            extraction_types.append(\"charts\")\n        if self.extract_tables:\n            extraction_types.append(\"tables\")\n\n        print(f\"\u2705 Parsing completed successfully!\")\n        print(f\"\ud83d\udcc1 Output directory: {out_dir}\")\n</code></pre>"},{"location":"api/parsers.html#doctra.parsers.table_chart_extractor.ChartTablePDFParser.__init__","title":"<code>__init__(*, extract_charts=True, extract_tables=True, use_vlm=False, vlm_provider='gemini', vlm_model=None, vlm_api_key=None, layout_model_name='PP-DocLayout_plus-L', dpi=200, min_score=0.0)</code>","text":"<p>Initialize the ChartTablePDFParser with extraction configuration.</p> <p>:param extract_charts: Whether to extract charts from the document (default: True) :param extract_tables: Whether to extract tables from the document (default: True) :param use_vlm: Whether to use VLM for structured data extraction (default: False) :param vlm_provider: VLM provider to use (\"gemini\", \"openai\", \"anthropic\", or \"openrouter\", default: \"gemini\") :param vlm_model: Model name to use (defaults to provider-specific defaults) :param vlm_api_key: API key for VLM provider (required if use_vlm is True) :param layout_model_name: Layout detection model name (default: \"PP-DocLayout_plus-L\") :param dpi: DPI for PDF rendering (default: 200) :param min_score: Minimum confidence score for layout detection (default: 0.0)</p> Source code in <code>doctra/parsers/table_chart_extractor.py</code> <pre><code>def __init__(\n        self,\n        *,\n        extract_charts: bool = True,\n        extract_tables: bool = True,\n        use_vlm: bool = False,\n        vlm_provider: str = \"gemini\",\n        vlm_model: str | None = None,\n        vlm_api_key: str | None = None,\n        layout_model_name: str = \"PP-DocLayout_plus-L\",\n        dpi: int = 200,\n        min_score: float = 0.0,\n):\n    \"\"\"\n    Initialize the ChartTablePDFParser with extraction configuration.\n\n    :param extract_charts: Whether to extract charts from the document (default: True)\n    :param extract_tables: Whether to extract tables from the document (default: True)\n    :param use_vlm: Whether to use VLM for structured data extraction (default: False)\n    :param vlm_provider: VLM provider to use (\"gemini\", \"openai\", \"anthropic\", or \"openrouter\", default: \"gemini\")\n    :param vlm_model: Model name to use (defaults to provider-specific defaults)\n    :param vlm_api_key: API key for VLM provider (required if use_vlm is True)\n    :param layout_model_name: Layout detection model name (default: \"PP-DocLayout_plus-L\")\n    :param dpi: DPI for PDF rendering (default: 200)\n    :param min_score: Minimum confidence score for layout detection (default: 0.0)\n    \"\"\"\n    if not extract_charts and not extract_tables:\n        raise ValueError(\"At least one of extract_charts or extract_tables must be True\")\n\n    self.extract_charts = extract_charts\n    self.extract_tables = extract_tables\n    self.layout_engine = PaddleLayoutEngine(model_name=layout_model_name)\n    self.dpi = dpi\n    self.min_score = min_score\n\n    self.use_vlm = use_vlm\n    self.vlm = None\n    if self.use_vlm:\n        self.vlm = VLMStructuredExtractor(\n            vlm_provider=vlm_provider,\n            vlm_model=vlm_model,\n            api_key=vlm_api_key,\n        )\n</code></pre>"},{"location":"api/parsers.html#doctra.parsers.table_chart_extractor.ChartTablePDFParser.parse","title":"<code>parse(pdf_path, output_base_dir='outputs')</code>","text":"<p>Parse a PDF document and extract charts and/or tables.</p> <p>:param pdf_path: Path to the input PDF file :param output_base_dir: Base directory for output files (default: \"outputs\") :return: None</p> Source code in <code>doctra/parsers/table_chart_extractor.py</code> <pre><code>def parse(self, pdf_path: str, output_base_dir: str = \"outputs\") -&gt; None:\n    \"\"\"\n    Parse a PDF document and extract charts and/or tables.\n\n    :param pdf_path: Path to the input PDF file\n    :param output_base_dir: Base directory for output files (default: \"outputs\")\n    :return: None\n    \"\"\"\n    pdf_name = Path(pdf_path).stem\n    out_dir = os.path.join(output_base_dir, pdf_name, \"structured_parsing\")\n    os.makedirs(out_dir, exist_ok=True)\n\n    charts_dir = None\n    tables_dir = None\n\n    if self.extract_charts:\n        charts_dir = os.path.join(out_dir, \"charts\")\n        os.makedirs(charts_dir, exist_ok=True)\n\n    if self.extract_tables:\n        tables_dir = os.path.join(out_dir, \"tables\")\n        os.makedirs(tables_dir, exist_ok=True)\n\n    pages: List[LayoutPage] = self.layout_engine.predict_pdf(\n        pdf_path, batch_size=1, layout_nms=True, dpi=self.dpi, min_score=self.min_score\n    )\n    pil_pages = [im for (im, _, _) in render_pdf_to_images(pdf_path, dpi=self.dpi)]\n\n    target_labels = []\n    if self.extract_charts:\n        target_labels.append(\"chart\")\n    if self.extract_tables:\n        target_labels.append(\"table\")\n\n    chart_count = sum(sum(1 for b in p.boxes if b.label == \"chart\") for p in pages) if self.extract_charts else 0\n    table_count = sum(sum(1 for b in p.boxes if b.label == \"table\") for p in pages) if self.extract_tables else 0\n\n    if self.use_vlm:\n        md_lines: List[str] = [\"# Extracted Charts and Tables\\n\"]\n        structured_items: List[Dict[str, Any]] = []\n        vlm_items: List[Dict[str, Any]] = []\n\n    charts_desc = \"Charts (VLM \u2192 table)\" if self.use_vlm else \"Charts (cropped)\"\n    tables_desc = \"Tables (VLM \u2192 table)\" if self.use_vlm else \"Tables (cropped)\"\n\n    chart_counter = 1\n    table_counter = 1\n\n    with ExitStack() as stack:\n        is_notebook = \"ipykernel\" in sys.modules or \"jupyter\" in sys.modules\n        is_terminal = hasattr(sys.stdout, 'isatty') and sys.stdout.isatty()\n\n        if is_notebook:\n            charts_bar = stack.enter_context(\n                create_notebook_friendly_bar(total=chart_count, desc=charts_desc)) if chart_count else None\n            tables_bar = stack.enter_context(\n                create_notebook_friendly_bar(total=table_count, desc=tables_desc)) if table_count else None\n        else:\n            charts_bar = stack.enter_context(\n                create_beautiful_progress_bar(total=chart_count, desc=charts_desc, leave=True)) if chart_count else None\n            tables_bar = stack.enter_context(\n                create_beautiful_progress_bar(total=table_count, desc=tables_desc, leave=True)) if table_count else None\n\n        for p in pages:\n            page_num = p.page_index\n            page_img: Image.Image = pil_pages[page_num - 1]\n\n            target_items = [box for box in p.boxes if box.label in target_labels]\n\n            if target_items and self.use_vlm:\n                md_lines.append(f\"\\n## Page {page_num}\\n\")\n\n            for box in sorted(target_items, key=reading_order_key):\n                if box.label == \"chart\" and self.extract_charts:\n                    chart_filename = f\"chart_{chart_counter:03d}.png\"\n                    chart_path = os.path.join(charts_dir, chart_filename)\n\n                    cropped_img = page_img.crop((box.x1, box.y1, box.x2, box.y2))\n                    cropped_img.save(chart_path)\n\n                    if self.use_vlm and self.vlm:\n                        rel_path = os.path.join(\"charts\", chart_filename)\n                        wrote_table = False\n\n                        try:\n                            extracted_chart = self.vlm.extract_chart(chart_path)\n                            structured_item = to_structured_dict(extracted_chart)\n                            if structured_item:\n                                # Add page and type information to structured item\n                                structured_item[\"page\"] = page_num\n                                structured_item[\"type\"] = \"Chart\"\n                                structured_items.append(structured_item)\n                                vlm_items.append({\n                                    \"kind\": \"chart\",\n                                    \"page\": page_num,\n                                    \"image_rel_path\": rel_path,\n                                    \"title\": structured_item.get(\"title\"),\n                                    \"headers\": structured_item.get(\"headers\"),\n                                    \"rows\": structured_item.get(\"rows\"),\n                                })\n                                md_lines.append(\n                                    render_markdown_table(\n                                        structured_item.get(\"headers\"),\n                                        structured_item.get(\"rows\"),\n                                        title=structured_item.get(\n                                            \"title\") or f\"Chart {chart_counter} \u2014 page {page_num}\"\n                                    )\n                                )\n                                wrote_table = True\n                        except Exception:\n                            pass\n\n                        if not wrote_table:\n                            md_lines.append(f\"![Chart {chart_counter} \u2014 page {page_num}]({rel_path})\\n\")\n\n                    chart_counter += 1\n                    if charts_bar:\n                        charts_bar.update(1)\n\n                elif box.label == \"table\" and self.extract_tables:\n                    table_filename = f\"table_{table_counter:03d}.png\"\n                    table_path = os.path.join(tables_dir, table_filename)\n\n                    cropped_img = page_img.crop((box.x1, box.y1, box.x2, box.y2))\n                    cropped_img.save(table_path)\n\n                    if self.use_vlm and self.vlm:\n                        rel_path = os.path.join(\"tables\", table_filename)\n                        wrote_table = False\n\n                        try:\n                            extracted_table = self.vlm.extract_table(table_path)\n                            structured_item = to_structured_dict(extracted_table)\n                            if structured_item:\n                                # Add page and type information to structured item\n                                structured_item[\"page\"] = page_num\n                                structured_item[\"type\"] = \"Table\"\n                                structured_items.append(structured_item)\n                                vlm_items.append({\n                                    \"kind\": \"table\",\n                                    \"page\": page_num,\n                                    \"image_rel_path\": rel_path,\n                                    \"title\": structured_item.get(\"title\"),\n                                    \"headers\": structured_item.get(\"headers\"),\n                                    \"rows\": structured_item.get(\"rows\"),\n                                })\n                                md_lines.append(\n                                    render_markdown_table(\n                                        structured_item.get(\"headers\"),\n                                        structured_item.get(\"rows\"),\n                                        title=structured_item.get(\n                                            \"title\") or f\"Table {table_counter} \u2014 page {page_num}\"\n                                    )\n                                )\n                                wrote_table = True\n                        except Exception:\n                            pass\n\n                        if not wrote_table:\n                            md_lines.append(f\"![Table {table_counter} \u2014 page {page_num}]({rel_path})\\n\")\n\n                    table_counter += 1\n                    if tables_bar:\n                        tables_bar.update(1)\n\n    excel_path = None\n\n    if self.use_vlm:\n\n        if structured_items:\n            if self.extract_charts and self.extract_tables:\n                excel_filename = \"parsed_tables_charts.xlsx\"\n            elif self.extract_charts:\n                excel_filename = \"parsed_charts.xlsx\"\n            elif self.extract_tables:\n                excel_filename = \"parsed_tables.xlsx\"\n            else:\n                excel_filename = \"parsed_data.xlsx\"  # fallback\n\n\n            excel_path = os.path.join(out_dir, excel_filename)\n            write_structured_excel(excel_path, structured_items)\n\n            html_filename = excel_filename.replace('.xlsx', '.html')\n            html_path = os.path.join(out_dir, html_filename)\n            write_structured_html(html_path, structured_items)\n\n        if 'vlm_items' in locals() and vlm_items:\n            with open(os.path.join(out_dir, \"vlm_items.json\"), 'w', encoding='utf-8') as jf:\n                json.dump(vlm_items, jf, ensure_ascii=False, indent=2)\n\n    extraction_types = []\n    if self.extract_charts:\n        extraction_types.append(\"charts\")\n    if self.extract_tables:\n        extraction_types.append(\"tables\")\n\n    print(f\"\u2705 Parsing completed successfully!\")\n    print(f\"\ud83d\udcc1 Output directory: {out_dir}\")\n</code></pre>"},{"location":"api/parsers.html#quick-reference","title":"Quick Reference","text":""},{"location":"api/parsers.html#structuredpdfparser_1","title":"StructuredPDFParser","text":"<pre><code>from doctra import StructuredPDFParser\n\nparser = StructuredPDFParser(\n    # Layout Detection\n    layout_model_name: str = \"PP-DocLayout_plus-L\",\n    dpi: int = 200,\n    min_score: float = 0.0,\n\n    # OCR Settings\n    ocr_lang: str = \"eng\",\n    ocr_psm: int = 4,\n    ocr_oem: int = 3,\n    ocr_extra_config: str = \"\",\n\n    # VLM Settings\n    use_vlm: bool = False,\n    vlm_provider: str = None,\n    vlm_api_key: str = None,\n    vlm_model: str = None,\n\n    # Output Settings\n    box_separator: str = \"\\n\"\n)\n\n# Parse document\nparser.parse(\n    pdf_path: str,\n    output_base_dir: str = \"outputs\"\n)\n\n# Visualize layout\nparser.display_pages_with_boxes(\n    pdf_path: str,\n    num_pages: int = 3,\n    cols: int = 2,\n    page_width: int = 800,\n    spacing: int = 40,\n    save_path: str = None\n)\n</code></pre>"},{"location":"api/parsers.html#enhancedpdfparser_1","title":"EnhancedPDFParser","text":"<pre><code>from doctra import EnhancedPDFParser\n\nparser = EnhancedPDFParser(\n    # Image Restoration\n    use_image_restoration: bool = True,\n    restoration_task: str = \"appearance\",\n    restoration_device: str = None,\n    restoration_dpi: int = 200,\n\n    # All StructuredPDFParser parameters...\n)\n\n# Parse with enhancement\nparser.parse(\n    pdf_path: str,\n    output_base_dir: str = \"outputs\"\n)\n</code></pre>"},{"location":"api/parsers.html#charttablepdfparser_1","title":"ChartTablePDFParser","text":"<pre><code>from doctra import ChartTablePDFParser\n\nparser = ChartTablePDFParser(\n    # Extraction Settings\n    extract_charts: bool = True,\n    extract_tables: bool = True,\n\n    # VLM Settings\n    use_vlm: bool = False,\n    vlm_provider: str = None,\n    vlm_api_key: str = None,\n    vlm_model: str = None,\n\n    # Layout Detection\n    layout_model_name: str = \"PP-DocLayout_plus-L\",\n    dpi: int = 200,\n    min_score: float = 0.0\n)\n\n# Extract charts/tables\nparser.parse(\n    pdf_path: str,\n    output_base_dir: str = \"outputs\"\n)\n</code></pre>"},{"location":"api/parsers.html#parameter-reference","title":"Parameter Reference","text":""},{"location":"api/parsers.html#layout-detection-parameters","title":"Layout Detection Parameters","text":"Parameter Type Default Description <code>layout_model_name</code> str \"PP-DocLayout_plus-L\" PaddleOCR layout detection model <code>dpi</code> int 200 Image resolution for rendering PDF pages <code>min_score</code> float 0.0 Minimum confidence score for detected elements"},{"location":"api/parsers.html#ocr-parameters","title":"OCR Parameters","text":"Parameter Type Default Description <code>ocr_lang</code> str \"eng\" Tesseract language code <code>ocr_psm</code> int 4 Page segmentation mode <code>ocr_oem</code> int 3 OCR engine mode <code>ocr_extra_config</code> str \"\" Additional Tesseract configuration"},{"location":"api/parsers.html#vlm-parameters","title":"VLM Parameters","text":"Parameter Type Default Description <code>use_vlm</code> bool False Enable VLM processing <code>vlm_provider</code> str None Provider: \"openai\", \"gemini\", \"anthropic\", \"openrouter\" <code>vlm_api_key</code> str None API key for the VLM provider <code>vlm_model</code> str None Specific model to use (provider-dependent)"},{"location":"api/parsers.html#image-restoration-parameters","title":"Image Restoration Parameters","text":"Parameter Type Default Description <code>use_image_restoration</code> bool True Enable image restoration <code>restoration_task</code> str \"appearance\" Restoration task type <code>restoration_device</code> str None Device: \"cuda\", \"cpu\", or None (auto-detect) <code>restoration_dpi</code> int 200 DPI for restoration processing"},{"location":"api/parsers.html#extraction-parameters","title":"Extraction Parameters","text":"Parameter Type Default Description <code>extract_charts</code> bool True Extract chart elements <code>extract_tables</code> bool True Extract table elements"},{"location":"api/parsers.html#output-parameters","title":"Output Parameters","text":"Parameter Type Default Description <code>box_separator</code> str \"\\n\" Separator between detected elements"},{"location":"api/parsers.html#return-values","title":"Return Values","text":""},{"location":"api/parsers.html#parse-method","title":"parse() Method","text":"<p>Returns: <code>None</code></p> <p>Generates output files in the specified <code>output_base_dir</code>:</p> <pre><code>outputs/\n\u2514\u2500\u2500 &lt;document_name&gt;/\n    \u251c\u2500\u2500 full_parse/  # or 'enhanced_parse/', 'structured_parsing/'\n    \u2502   \u251c\u2500\u2500 result.md\n    \u2502   \u251c\u2500\u2500 result.html\n    \u2502   \u251c\u2500\u2500 tables.xlsx  # If VLM enabled\n    \u2502   \u251c\u2500\u2500 tables.html  # If VLM enabled\n    \u2502   \u251c\u2500\u2500 vlm_items.json  # If VLM enabled\n    \u2502   \u2514\u2500\u2500 images/\n    \u2502       \u251c\u2500\u2500 figures/\n    \u2502       \u251c\u2500\u2500 charts/\n    \u2502       \u2514\u2500\u2500 tables/\n</code></pre>"},{"location":"api/parsers.html#display_pages_with_boxes-method","title":"display_pages_with_boxes() Method","text":"<p>Returns: <code>None</code></p> <p>Displays or saves visualization of layout detection.</p>"},{"location":"api/parsers.html#error-handling","title":"Error Handling","text":"<p>All parsers may raise:</p> <ul> <li><code>FileNotFoundError</code>: PDF file not found</li> <li><code>ValueError</code>: Invalid parameter values</li> <li><code>RuntimeError</code>: Processing errors (e.g., Poppler not found)</li> <li><code>APIError</code>: VLM API errors (when VLM enabled)</li> </ul> <p>Example error handling:</p> <pre><code>from doctra import StructuredPDFParser\n\nparser = StructuredPDFParser()\n\ntry:\n    parser.parse(\"document.pdf\")\nexcept FileNotFoundError:\n    print(\"PDF file not found!\")\nexcept ValueError as e:\n    print(f\"Invalid parameter: {e}\")\nexcept RuntimeError as e:\n    print(f\"Processing error: {e}\")\n</code></pre>"},{"location":"api/parsers.html#examples","title":"Examples","text":"<p>See the Examples section for detailed usage examples.</p>"},{"location":"api/utils.html","title":"Utilities API Reference","text":"<p>Documentation for Doctra's utility functions and helpers.</p>"},{"location":"api/utils.html#overview","title":"Overview","text":"<p>Utility modules provide helper functions for common tasks.</p>"},{"location":"api/utils.html#available-utilities","title":"Available Utilities","text":""},{"location":"api/utils.html#file-operations","title":"File Operations","text":"<ul> <li>PDF I/O operations</li> <li>Image loading and saving</li> <li>Directory management</li> </ul>"},{"location":"api/utils.html#bounding-box-operations","title":"Bounding Box Operations","text":"<ul> <li>Coordinate transformations</li> <li>Box intersection and union</li> <li>Box filtering and sorting</li> </ul>"},{"location":"api/utils.html#progress-tracking","title":"Progress Tracking","text":"<ul> <li>Progress bar management</li> <li>Status reporting</li> </ul>"},{"location":"api/utils.html#ocr-utilities","title":"OCR Utilities","text":"<ul> <li>Text cleaning and normalization</li> <li>Language detection helpers</li> </ul>"},{"location":"api/utils.html#see-also","title":"See Also","text":"<ul> <li>Core Concepts - Understanding the architecture</li> <li>API Reference - Main API documentation</li> </ul>"},{"location":"contributing/code-of-conduct.html","title":"Code of Conduct","text":"<p>See our CODE_OF_CONDUCT.md in the repository root.</p>"},{"location":"contributing/development.html","title":"Development Guide","text":"<p>Thank you for your interest in contributing to Doctra! This guide will help you get started.</p>"},{"location":"contributing/development.html#getting-started","title":"Getting Started","text":""},{"location":"contributing/development.html#development-setup","title":"Development Setup","text":"<ol> <li>Fork and Clone</li> </ol> <pre><code>git clone https://github.com/YOUR_USERNAME/Doctra.git\ncd Doctra\n</code></pre> <ol> <li>Create Virtual Environment</li> </ol> <pre><code>python -m venv venv\nsource venv/bin/activate  # Linux/Mac\n# or\n.\\venv\\Scripts\\activate  # Windows\n</code></pre> <ol> <li>Install Development Dependencies</li> </ol> <pre><code>pip install -e \".[dev]\"\n</code></pre> <p>This installs Doctra in editable mode with development tools.</p> <ol> <li>Install System Dependencies</li> </ol> <p>Follow the Installation Guide for Poppler.</p>"},{"location":"contributing/development.html#project-structure","title":"Project Structure","text":"<pre><code>Doctra/\n\u251c\u2500\u2500 doctra/              # Main package\n\u2502   \u251c\u2500\u2500 parsers/         # PDF parsers\n\u2502   \u251c\u2500\u2500 engines/         # Processing engines\n\u2502   \u251c\u2500\u2500 exporters/       # Output formatters\n\u2502   \u251c\u2500\u2500 ui/              # Web interface\n\u2502   \u251c\u2500\u2500 cli/             # Command line interface\n\u2502   \u2514\u2500\u2500 utils/           # Utilities\n\u251c\u2500\u2500 tests/               # Test suite\n\u251c\u2500\u2500 docs/                # Documentation\n\u251c\u2500\u2500 examples/            # Example scripts\n\u251c\u2500\u2500 notebooks/           # Jupyter notebooks\n\u2514\u2500\u2500 setup.py             # Package configuration\n</code></pre>"},{"location":"contributing/development.html#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/development.html#1-create-a-branch","title":"1. Create a Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n</code></pre> <p>Branch naming conventions:</p> <ul> <li><code>feature/</code> - New features</li> <li><code>fix/</code> - Bug fixes</li> <li><code>docs/</code> - Documentation updates</li> <li><code>refactor/</code> - Code refactoring</li> <li><code>test/</code> - Test additions/updates</li> </ul>"},{"location":"contributing/development.html#2-make-changes","title":"2. Make Changes","text":"<p>Write clean, well-documented code following our Code Style.</p>"},{"location":"contributing/development.html#3-run-tests","title":"3. Run Tests","text":"<pre><code>pytest tests/\n</code></pre> <p>Run specific test:</p> <pre><code>pytest tests/test_structured_pdf_parser.py\n</code></pre> <p>With coverage:</p> <pre><code>pytest --cov=doctra tests/\n</code></pre>"},{"location":"contributing/development.html#4-format-code","title":"4. Format Code","text":"<pre><code># Format with Black\nblack doctra tests\n\n# Sort imports\nisort doctra tests\n\n# Lint with Flake8\nflake8 doctra tests\n</code></pre>"},{"location":"contributing/development.html#5-type-checking","title":"5. Type Checking","text":"<pre><code>mypy doctra\n</code></pre>"},{"location":"contributing/development.html#6-commit-changes","title":"6. Commit Changes","text":"<pre><code>git add .\ngit commit -m \"feat: add new feature description\"\n</code></pre> <p>Commit message format:</p> <ul> <li><code>feat:</code> - New feature</li> <li><code>fix:</code> - Bug fix</li> <li><code>docs:</code> - Documentation</li> <li><code>style:</code> - Formatting</li> <li><code>refactor:</code> - Code restructuring</li> <li><code>test:</code> - Tests</li> <li><code>chore:</code> - Maintenance</li> </ul>"},{"location":"contributing/development.html#7-push-and-create-pr","title":"7. Push and Create PR","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then create a Pull Request on GitHub.</p>"},{"location":"contributing/development.html#code-style","title":"Code Style","text":""},{"location":"contributing/development.html#python-style-guide","title":"Python Style Guide","text":"<p>We follow PEP 8 with these configurations:</p> <pre><code># .flake8\n[flake8]\nmax-line-length = 88\nextend-ignore = E203, W503\nexclude = .git,__pycache__,docs,build,dist\n</code></pre>"},{"location":"contributing/development.html#code-formatting","title":"Code Formatting","text":"<pre><code># Black configuration in pyproject.toml\n[tool.black]\nline-length = 88\ntarget-version = ['py38', 'py39', 'py310', 'py311', 'py312']\n</code></pre>"},{"location":"contributing/development.html#import-sorting","title":"Import Sorting","text":"<pre><code># isort configuration in pyproject.toml\n[tool.isort]\nprofile = \"black\"\nmulti_line_output = 3\n</code></pre>"},{"location":"contributing/development.html#example-code","title":"Example Code","text":"<pre><code>\"\"\"Module docstring explaining purpose.\"\"\"\n\nfrom typing import Optional, Union\n\nimport numpy as np\nfrom PIL import Image\n\nfrom doctra.utils import helper_function\n\n\nclass MyParser:\n    \"\"\"Class docstring explaining purpose.\n\n    Args:\n        param1: Description of param1\n        param2: Description of param2\n\n    Attributes:\n        attribute1: Description\n    \"\"\"\n\n    def __init__(self, param1: str, param2: int = 10):\n        \"\"\"Initialize the parser.\"\"\"\n        self.param1 = param1\n        self.param2 = param2\n\n    def process(self, input_data: Union[str, np.ndarray]) -&gt; Optional[Image.Image]:\n        \"\"\"Process input data.\n\n        Args:\n            input_data: Input to process\n\n        Returns:\n            Processed image or None\n\n        Raises:\n            ValueError: If input is invalid\n        \"\"\"\n        if not self._validate(input_data):\n            raise ValueError(\"Invalid input\")\n\n        return self._do_process(input_data)\n\n    def _validate(self, data) -&gt; bool:\n        \"\"\"Private helper method.\"\"\"\n        return data is not None\n</code></pre>"},{"location":"contributing/development.html#testing","title":"Testing","text":""},{"location":"contributing/development.html#writing-tests","title":"Writing Tests","text":"<p>Create tests in <code>tests/</code> directory:</p> <pre><code>import pytest\nfrom doctra.parsers import StructuredPDFParser\n\n\ndef test_parser_initialization():\n    \"\"\"Test parser can be initialized.\"\"\"\n    parser = StructuredPDFParser()\n    assert parser is not None\n\n\ndef test_parse_basic_pdf():\n    \"\"\"Test parsing a basic PDF.\"\"\"\n    parser = StructuredPDFParser()\n    result = parser.parse(\"test_data/sample.pdf\")\n    assert result is not None\n\n\n@pytest.mark.parametrize(\"dpi\", [100, 200, 300])\ndef test_different_dpi_settings(dpi):\n    \"\"\"Test parser with different DPI settings.\"\"\"\n    parser = StructuredPDFParser(dpi=dpi)\n    assert parser.dpi == dpi\n</code></pre>"},{"location":"contributing/development.html#running-tests","title":"Running Tests","text":"<pre><code># All tests\npytest\n\n# Specific file\npytest tests/test_parsers.py\n\n# Specific test\npytest tests/test_parsers.py::test_parser_initialization\n\n# With verbose output\npytest -v\n\n# With coverage\npytest --cov=doctra --cov-report=html\n\n# Stop on first failure\npytest -x\n</code></pre>"},{"location":"contributing/development.html#test-coverage","title":"Test Coverage","text":"<p>Aim for &gt;80% code coverage:</p> <pre><code>pytest --cov=doctra --cov-report=term-missing\n</code></pre>"},{"location":"contributing/development.html#documentation","title":"Documentation","text":""},{"location":"contributing/development.html#building-documentation","title":"Building Documentation","text":"<pre><code># Install documentation dependencies\npip install -r docs/requirements.txt\n\n# Build and serve locally\nmkdocs serve\n\n# Build static site\nmkdocs build\n</code></pre> <p>View at: http://127.0.0.1:8000</p>"},{"location":"contributing/development.html#writing-documentation","title":"Writing Documentation","text":"<ul> <li>Use Markdown for all documentation</li> <li>Add docstrings to all public APIs</li> <li>Include code examples</li> <li>Update relevant docs when adding features</li> </ul>"},{"location":"contributing/development.html#docstring-format","title":"Docstring Format","text":"<p>We use Google-style docstrings:</p> <pre><code>def function(param1: str, param2: int) -&gt; bool:\n    \"\"\"Short description.\n\n    Longer description if needed.\n\n    Args:\n        param1: Description of param1\n        param2: Description of param2\n\n    Returns:\n        Description of return value\n\n    Raises:\n        ValueError: When param1 is invalid\n\n    Examples:\n        &gt;&gt;&gt; function(\"test\", 5)\n        True\n    \"\"\"\n    pass\n</code></pre>"},{"location":"contributing/development.html#pull-request-guidelines","title":"Pull Request Guidelines","text":""},{"location":"contributing/development.html#before-submitting","title":"Before Submitting","text":"<ul> <li> Tests pass: <code>pytest</code></li> <li> Code formatted: <code>black doctra tests</code></li> <li> Imports sorted: <code>isort doctra tests</code></li> <li> Linting clean: <code>flake8 doctra tests</code></li> <li> Type checking: <code>mypy doctra</code></li> <li> Documentation updated</li> <li> CHANGELOG.md updated</li> </ul>"},{"location":"contributing/development.html#pr-description-template","title":"PR Description Template","text":"<pre><code>## Description\nBrief description of changes\n\n## Type of Change\n- [ ] Bug fix\n- [ ] New feature\n- [ ] Breaking change\n- [ ] Documentation update\n\n## Testing\nDescribe testing done\n\n## Checklist\n- [ ] Tests pass\n- [ ] Code formatted\n- [ ] Documentation updated\n- [ ] CHANGELOG updated\n</code></pre>"},{"location":"contributing/development.html#review-process","title":"Review Process","text":"<ol> <li>Automated checks run (tests, linting)</li> <li>Code review by maintainers</li> <li>Requested changes addressed</li> <li>Approved and merged</li> </ol>"},{"location":"contributing/development.html#common-tasks","title":"Common Tasks","text":""},{"location":"contributing/development.html#adding-a-new-parser","title":"Adding a New Parser","text":"<ol> <li>Create parser file: <code>doctra/parsers/new_parser.py</code></li> <li>Implement parser class</li> <li>Add tests: <code>tests/test_new_parser.py</code></li> <li>Update <code>doctra/__init__.py</code></li> <li>Add documentation: <code>docs/user-guide/parsers/new-parser.md</code></li> <li>Add API reference: <code>docs/api/parsers.md</code></li> </ol>"},{"location":"contributing/development.html#adding-a-new-feature","title":"Adding a New Feature","text":"<ol> <li>Create feature branch</li> <li>Implement feature with tests</li> <li>Update documentation</li> <li>Submit PR with description</li> </ol>"},{"location":"contributing/development.html#fixing-a-bug","title":"Fixing a Bug","text":"<ol> <li>Create test that reproduces bug</li> <li>Fix bug</li> <li>Verify test passes</li> <li>Submit PR referencing issue</li> </ol>"},{"location":"contributing/development.html#development-tools","title":"Development Tools","text":""},{"location":"contributing/development.html#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Install pre-commit hooks:</p> <pre><code>pre-commit install\n</code></pre> <p>This runs checks before each commit:</p> <ul> <li>Black formatting</li> <li>isort import sorting</li> <li>Flake8 linting</li> <li>Trailing whitespace removal</li> </ul>"},{"location":"contributing/development.html#ide-setup","title":"IDE Setup","text":""},{"location":"contributing/development.html#vs-code","title":"VS Code","text":"<p>Recommended <code>settings.json</code>:</p> <pre><code>{\n    \"python.formatting.provider\": \"black\",\n    \"python.linting.enabled\": true,\n    \"python.linting.flake8Enabled\": true,\n    \"python.linting.mypyEnabled\": true,\n    \"editor.formatOnSave\": true,\n    \"[python]\": {\n        \"editor.codeActionsOnSave\": {\n            \"source.organizeImports\": true\n        }\n    }\n}\n</code></pre>"},{"location":"contributing/development.html#pycharm","title":"PyCharm","text":"<ul> <li>Enable Black formatter</li> <li>Enable Flake8 linter</li> <li>Enable mypy type checker</li> </ul>"},{"location":"contributing/development.html#getting-help","title":"Getting Help","text":"<ul> <li>Questions: Open a GitHub Discussion</li> <li>Bugs: Report in GitHub Issues</li> <li>Chat: Join our community (link in README)</li> </ul>"},{"location":"contributing/development.html#code-of-conduct","title":"Code of Conduct","text":"<p>Please read and follow our Code of Conduct.</p>"},{"location":"contributing/development.html#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the MIT License.</p>"},{"location":"examples/advanced-examples.html","title":"Advanced Examples","text":"<p>Advanced usage patterns and integration examples.</p>"},{"location":"examples/advanced-examples.html#multi-stage-processing-pipeline","title":"Multi-Stage Processing Pipeline","text":"<pre><code>from doctra import DocResEngine, StructuredPDFParser\n\n# Stage 1: Restore document\nengine = DocResEngine(device=\"cuda\")\nenhanced_pdf = engine.restore_pdf(\n    pdf_path=\"low_quality.pdf\",\n    output_path=\"enhanced.pdf\",\n    task=\"appearance\"\n)\n\n# Stage 2: Parse enhanced document\nparser = StructuredPDFParser(\n    use_vlm=True,\n    vlm_provider=\"openai\",\n    vlm_api_key=\"your-key\"\n)\nparser.parse(enhanced_pdf)\n</code></pre>"},{"location":"examples/advanced-examples.html#custom-processing-with-different-vlm-providers","title":"Custom Processing with Different VLM Providers","text":"<pre><code>from doctra import ChartTablePDFParser\n\n# Using OpenAI\nparser_openai = ChartTablePDFParser(\n    use_vlm=True,\n    vlm_provider=\"openai\",\n    vlm_api_key=\"sk-xxx\"\n)\n\n# Using Gemini (cost-effective)\nparser_gemini = ChartTablePDFParser(\n    use_vlm=True,\n    vlm_provider=\"gemini\",\n    vlm_api_key=\"gemini-key\"\n)\n\n# Using Qianfan ERNIE (Baidu AI Cloud)\nparser_qianfan = ChartTablePDFParser(\n    use_vlm=True,\n    vlm_provider=\"qianfan\",\n    vlm_api_key=\"qianfan-key\",\n    vlm_model=\"ernie-4.5-turbo-vl-32k\"\n)\n\n# Parse with different providers\nparser_openai.parse(\"doc.pdf\")\nparser_gemini.parse(\"doc.pdf\")\nparser_qianfan.parse(\"doc.pdf\")\n</code></pre>"},{"location":"examples/advanced-examples.html#parallel-batch-processing","title":"Parallel Batch Processing","text":"<pre><code>from concurrent.futures import ThreadPoolExecutor\nfrom doctra import StructuredPDFParser\n\ndef process_pdf(pdf_path):\n    parser = StructuredPDFParser()\n    try:\n        parser.parse(pdf_path)\n        return f\"Success: {pdf_path}\"\n    except Exception as e:\n        return f\"Error {pdf_path}: {e}\"\n\n# Process multiple PDFs in parallel\npdf_files = [\"doc1.pdf\", \"doc2.pdf\", \"doc3.pdf\"]\n\nwith ThreadPoolExecutor(max_workers=3) as executor:\n    results = executor.map(process_pdf, pdf_files)\n\nfor result in results:\n    print(result)\n</code></pre>"},{"location":"examples/advanced-examples.html#dynamic-dpi-selection","title":"Dynamic DPI Selection","text":"<pre><code>import os\nfrom doctra import StructuredPDFParser\n\ndef smart_parse(pdf_path):\n    # Choose DPI based on file size\n    file_size = os.path.getsize(pdf_path) / (1024 * 1024)  # MB\n\n    if file_size &lt; 5:\n        dpi = 200  # Standard quality\n    elif file_size &lt; 20:\n        dpi = 150  # Lower for large files\n    else:\n        dpi = 100  # Very low for huge files\n\n    parser = StructuredPDFParser(dpi=dpi)\n    print(f\"Processing {pdf_path} at {dpi} DPI\")\n    parser.parse(pdf_path)\n\nsmart_parse(\"document.pdf\")\n</code></pre>"},{"location":"examples/advanced-examples.html#integration-with-data-analysis","title":"Integration with Data Analysis","text":"<pre><code>from doctra import ChartTablePDFParser\nimport pandas as pd\n\n# Extract tables\nparser = ChartTablePDFParser(\n    extract_tables=True,\n    use_vlm=True,\n    vlm_provider=\"openai\",\n    vlm_api_key=\"your-key\"\n)\n\nparser.parse(\"financial_report.pdf\")\n\n# Load and analyze extracted data\nexcel_path = \"outputs/financial_report/structured_parsing/parsed_tables_charts.xlsx\"\nxls = pd.ExcelFile(excel_path)\n\nfor sheet_name in xls.sheet_names:\n    df = pd.read_excel(xls, sheet_name=sheet_name)\n    print(f\"\\nTable: {sheet_name}\")\n    print(df.describe())\n</code></pre>"},{"location":"examples/advanced-examples.html#see-also","title":"See Also","text":"<ul> <li>Basic Examples - Simpler examples</li> <li>Integration - Integration patterns</li> <li>API Reference - API documentation</li> </ul>"},{"location":"examples/basic-usage.html","title":"Basic Usage Examples","text":"<p>Practical examples for common Doctra use cases.</p>"},{"location":"examples/basic-usage.html#example-1-parse-a-simple-pdf","title":"Example 1: Parse a Simple PDF","text":"<pre><code>from doctra import StructuredPDFParser\n\n# Initialize parser\nparser = StructuredPDFParser()\n\n# Parse document\nparser.parse(\"document.pdf\")\n\n# Output saved to: outputs/document/full_parse/\n</code></pre>"},{"location":"examples/basic-usage.html#example-2-parse-with-custom-settings","title":"Example 2: Parse with Custom Settings","text":"<pre><code>from doctra import StructuredPDFParser\n\nparser = StructuredPDFParser(\n    dpi=250,  # Higher quality\n    min_score=0.7,  # More confident detections\n    ocr_lang=\"eng\"  # English language\n)\n\nparser.parse(\"document.pdf\", output_base_dir=\"my_results\")\n</code></pre>"},{"location":"examples/basic-usage.html#example-3-enhanced-parsing-for-scanned-documents","title":"Example 3: Enhanced Parsing for Scanned Documents","text":"<pre><code>from doctra import EnhancedPDFParser\n\nparser = EnhancedPDFParser(\n    use_image_restoration=True,\n    restoration_task=\"appearance\",\n    restoration_device=\"cuda\"  # Use GPU\n)\n\nparser.parse(\"scanned_document.pdf\")\n</code></pre>"},{"location":"examples/basic-usage.html#example-4-extract-structured-data-with-vlm","title":"Example 4: Extract Structured Data with VLM","text":"<pre><code>from doctra import StructuredPDFParser\n\nparser = StructuredPDFParser(\n    use_vlm=True,\n    vlm_provider=\"openai\",\n    vlm_api_key=\"your-api-key-here\"\n)\n\nparser.parse(\"data_report.pdf\")\n\n# Output includes:\n# - tables.xlsx with extracted data\n# - tables.html with formatted tables\n# - vlm_items.json with structured data\n</code></pre>"},{"location":"examples/basic-usage.html#example-5-extract-only-charts","title":"Example 5: Extract Only Charts","text":"<pre><code>from doctra import ChartTablePDFParser\n\nparser = ChartTablePDFParser(\n    extract_charts=True,\n    extract_tables=False\n)\n\nparser.parse(\"presentation.pdf\")\n</code></pre>"},{"location":"examples/basic-usage.html#example-6-visualize-layout-detection","title":"Example 6: Visualize Layout Detection","text":"<pre><code>from doctra import StructuredPDFParser\n\nparser = StructuredPDFParser()\n\n# Display layout detection\nparser.display_pages_with_boxes(\n    pdf_path=\"document.pdf\",\n    num_pages=3,\n    save_path=\"layout_visualization.png\"\n)\n</code></pre>"},{"location":"examples/basic-usage.html#example-7-standalone-image-restoration","title":"Example 7: Standalone Image Restoration","text":"<pre><code>from doctra import DocResEngine\n\n# Initialize restoration engine\nengine = DocResEngine(device=\"cuda\")\n\n# Restore a single image\nrestored_img, metadata = engine.restore_image(\n    image=\"blurry_document.jpg\",\n    task=\"deblurring\"\n)\n\n# Save result\nrestored_img.save(\"restored.jpg\")\nprint(f\"Processed in {metadata['processing_time']:.2f}s\")\n</code></pre>"},{"location":"examples/basic-usage.html#example-8-batch-processing","title":"Example 8: Batch Processing","text":"<pre><code>import os\nfrom doctra import StructuredPDFParser\n\nparser = StructuredPDFParser()\n\n# Process all PDFs in directory\npdf_directory = \"documents\"\nfor filename in os.listdir(pdf_directory):\n    if filename.endswith(\".pdf\"):\n        pdf_path = os.path.join(pdf_directory, filename)\n        print(f\"Processing {filename}...\")\n        parser.parse(pdf_path)\n        print(f\"Completed {filename}\")\n</code></pre>"},{"location":"examples/basic-usage.html#example-9-error-handling","title":"Example 9: Error Handling","text":"<pre><code>from doctra import StructuredPDFParser\n\nparser = StructuredPDFParser()\n\ntry:\n    parser.parse(\"document.pdf\")\n    print(\"Processing successful!\")\nexcept FileNotFoundError:\n    print(\"Error: PDF file not found\")\nexcept Exception as e:\n    print(f\"Error during processing: {e}\")\n</code></pre>"},{"location":"examples/basic-usage.html#example-10-using-the-web-ui","title":"Example 10: Using the Web UI","text":"<pre><code>from doctra import launch_ui\n\n# Launch web interface\nlaunch_ui()\n\n# Opens browser at http://127.0.0.1:7860\n</code></pre>"},{"location":"examples/basic-usage.html#next-steps","title":"Next Steps","text":"<ul> <li>Advanced Examples - Complex use cases</li> <li>Integration Examples - Integrate with other tools</li> <li>API Reference - Detailed API documentation</li> </ul>"},{"location":"examples/integration.html","title":"Integration Examples","text":"<p>Examples of integrating Doctra with other tools and frameworks.</p>"},{"location":"examples/integration.html#flask-web-application","title":"Flask Web Application","text":"<pre><code>from flask import Flask, request, jsonify, send_file\nfrom doctra import StructuredPDFParser\nimport os\n\napp = Flask(__name__)\nparser = StructuredPDFParser()\n\n@app.route('/parse', methods=['POST'])\ndef parse_document():\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n\n    file = request.files['file']\n\n    # Save uploaded file\n    pdf_path = f\"uploads/{file.filename}\"\n    file.save(pdf_path)\n\n    # Parse document\n    try:\n        parser.parse(pdf_path)\n        return jsonify({\n            'status': 'success',\n            'output_dir': f\"outputs/{file.filename.replace('.pdf', '')}\"\n        })\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\nif __name__ == '__main__':\n    os.makedirs('uploads', exist_ok=True)\n    app.run(debug=True)\n</code></pre>"},{"location":"examples/integration.html#fastapi-service","title":"FastAPI Service","text":"<pre><code>from fastapi import FastAPI, File, UploadFile, BackgroundTasks\nfrom doctra import StructuredPDFParser\nimport shutil\n\napp = FastAPI()\nparser = StructuredPDFParser()\n\n@app.post(\"/parse\")\nasync def parse_pdf(\n    background_tasks: BackgroundTasks,\n    file: UploadFile = File(...)\n):\n    # Save file\n    file_path = f\"temp/{file.filename}\"\n    with open(file_path, \"wb\") as buffer:\n        shutil.copyfileobj(file.file, buffer)\n\n    # Queue processing\n    background_tasks.add_task(parser.parse, file_path)\n\n    return {\"status\": \"processing\", \"filename\": file.filename}\n</code></pre>"},{"location":"examples/integration.html#database-integration","title":"Database Integration","text":"<pre><code>from doctra import StructuredPDFParser\nimport sqlite3\nimport json\n\ndef store_results_in_db(pdf_path, db_path=\"documents.db\"):\n    # Parse document\n    parser = StructuredPDFParser(use_vlm=True)\n    parser.parse(pdf_path)\n\n    # Connect to database\n    conn = sqlite3.connect(db_path)\n    cursor = conn.cursor()\n\n    # Create table\n    cursor.execute('''\n        CREATE TABLE IF NOT EXISTS documents (\n            id INTEGER PRIMARY KEY,\n            filename TEXT,\n            num_pages INTEGER,\n            content TEXT,\n            metadata TEXT\n        )\n    ''')\n\n    # Load results\n    result_path = f\"outputs/{os.path.basename(pdf_path).replace('.pdf', '')}/full_parse/result.md\"\n    with open(result_path) as f:\n        content = f.read()\n\n    # Store in database\n    cursor.execute(\n        \"INSERT INTO documents (filename, content) VALUES (?, ?)\",\n        (pdf_path, content)\n    )\n\n    conn.commit()\n    conn.close()\n</code></pre>"},{"location":"examples/integration.html#aws-lambda-function","title":"AWS Lambda Function","text":"<pre><code>import json\nimport boto3\nfrom doctra import StructuredPDFParser\n\ns3 = boto3.client('s3')\nparser = StructuredPDFParser()\n\ndef lambda_handler(event, context):\n    # Get PDF from S3\n    bucket = event['bucket']\n    key = event['key']\n\n    # Download file\n    local_path = f\"/tmp/{key}\"\n    s3.download_file(bucket, key, local_path)\n\n    # Parse document\n    parser.parse(local_path, output_base_dir=\"/tmp/outputs\")\n\n    # Upload results back to S3\n    output_dir = f\"/tmp/outputs/{key.replace('.pdf', '')}\"\n    # ... upload logic ...\n\n    return {\n        'statusCode': 200,\n        'body': json.dumps('Processing complete')\n    }\n</code></pre>"},{"location":"examples/integration.html#see-also","title":"See Also","text":"<ul> <li>Basic Examples - Getting started</li> <li>Advanced Examples - Complex patterns</li> <li>API Reference - API documentation</li> </ul>"},{"location":"getting-started/installation.html","title":"Installation","text":"<p>This guide will help you install Doctra and its dependencies on your system.</p>"},{"location":"getting-started/installation.html#requirements","title":"Requirements","text":"<ul> <li>Python 3.8 or higher</li> <li>pip package manager</li> <li>Poppler (for PDF processing)</li> <li>Tesseract OCR (automatically handled by dependencies)</li> </ul>"},{"location":"getting-started/installation.html#installing-doctra","title":"Installing Doctra","text":""},{"location":"getting-started/installation.html#from-pypi-recommended","title":"From PyPI (Recommended)","text":"<p>The easiest way to install Doctra is from PyPI using pip:</p> <pre><code>pip install doctra\n</code></pre> <p>This will install Doctra and all Python dependencies automatically.</p>"},{"location":"getting-started/installation.html#from-source","title":"From Source","text":"<p>To install the latest development version from source:</p> <pre><code>git clone https://github.com/AdemBoukhris457/Doctra.git\ncd Doctra\npip install -e .\n</code></pre> <p>The <code>-e</code> flag installs in editable mode, which is useful for development.</p>"},{"location":"getting-started/installation.html#system-dependencies","title":"System Dependencies","text":"<p>Doctra requires Poppler for PDF processing. Follow the instructions for your operating system:</p>"},{"location":"getting-started/installation.html#ubuntudebian","title":"Ubuntu/Debian","text":"<pre><code>sudo apt-get update\nsudo apt-get install poppler-utils\n</code></pre>"},{"location":"getting-started/installation.html#macos","title":"macOS","text":"<p>Using Homebrew:</p> <pre><code>brew install poppler\n</code></pre> <p>If you don't have Homebrew, install it from brew.sh.</p>"},{"location":"getting-started/installation.html#simple-windows-windows","title":":simple-windows: Windows","text":""},{"location":"getting-started/installation.html#option-1-using-conda","title":"Option 1: Using Conda","text":"<pre><code>conda install -c conda-forge poppler\n</code></pre>"},{"location":"getting-started/installation.html#option-2-manual-installation","title":"Option 2: Manual Installation","text":"<ol> <li>Download Poppler for Windows from this link</li> <li>Extract the archive</li> <li>Add the <code>bin</code> directory to your system PATH</li> </ol>"},{"location":"getting-started/installation.html#google-colab","title":"Google Colab","text":"<pre><code>!apt-get install poppler-utils\n</code></pre>"},{"location":"getting-started/installation.html#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"getting-started/installation.html#vlm-providers","title":"VLM Providers","text":"<p>To use Vision Language Models for structured data extraction, install the appropriate provider:</p>"},{"location":"getting-started/installation.html#openai","title":"OpenAI","text":"<pre><code>pip install doctra[openai]\n</code></pre>"},{"location":"getting-started/installation.html#google-gemini","title":"Google Gemini","text":"<pre><code>pip install doctra[gemini]\n</code></pre>"},{"location":"getting-started/installation.html#all-vlm-providers","title":"All VLM Providers","text":"<pre><code>pip install doctra[openai,gemini]\n</code></pre>"},{"location":"getting-started/installation.html#development-dependencies","title":"Development Dependencies","text":"<p>For contributing to Doctra:</p> <pre><code>pip install doctra[dev]\n</code></pre> <p>This installs testing, linting, and formatting tools.</p>"},{"location":"getting-started/installation.html#verifying-installation","title":"Verifying Installation","text":"<p>After installation, verify that Doctra is installed correctly:</p> <pre><code>import doctra\nprint(doctra.__version__)\n</code></pre> <p>You should see the version number printed (e.g., <code>0.4.3</code>).</p>"},{"location":"getting-started/installation.html#check-system-dependencies","title":"Check System Dependencies","text":"<p>To check if Poppler is installed correctly:</p> <pre><code>pdftoppm -v\n</code></pre> <p>You should see the Poppler version information.</p>"},{"location":"getting-started/installation.html#gpu-support","title":"GPU Support","text":""},{"location":"getting-started/installation.html#cuda-for-faster-processing","title":"CUDA for Faster Processing","text":"<p>Doctra can leverage GPU acceleration for image restoration tasks. To enable GPU support:</p> <ol> <li>Install CUDA-compatible PyTorch:</li> </ol> <pre><code>pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n</code></pre> <ol> <li>Verify CUDA is available:</li> </ol> <pre><code>import torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n</code></pre>"},{"location":"getting-started/installation.html#paddlepaddle-gpu-support","title":"PaddlePaddle GPU Support","text":"<p>For GPU-accelerated layout detection:</p> <pre><code>pip uninstall paddlepaddle\npip install paddlepaddle-gpu\n</code></pre> <p>GPU Requirements</p> <p>GPU support requires:</p> <ul> <li>NVIDIA GPU with CUDA Compute Capability 3.5+</li> <li>CUDA 11.8 or higher</li> <li>cuDNN 8.6 or higher</li> </ul>"},{"location":"getting-started/installation.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation.html#importerror-no-module-named-doctra","title":"ImportError: No module named 'doctra'","text":"<p>Solution: Ensure Doctra is installed in your active Python environment:</p> <pre><code>pip list | grep doctra\n</code></pre> <p>If not listed, reinstall with <code>pip install doctra</code>.</p>"},{"location":"getting-started/installation.html#poppler-not-found","title":"Poppler not found","text":"<p>Symptoms: Error message mentioning \"pdftoppm\" or \"Poppler\"</p> <p>Solution: </p> <ol> <li>Verify Poppler installation: <code>pdftoppm -v</code></li> <li>If not installed, follow the System Dependencies section</li> <li>On Windows, ensure Poppler's <code>bin</code> directory is in your PATH</li> </ol>"},{"location":"getting-started/installation.html#cuda-out-of-memory","title":"CUDA out of memory","text":"<p>Solution: Use CPU processing or reduce DPI settings:</p> <pre><code>parser = StructuredPDFParser(\n    dpi=150,  # Reduce from default 200\n    restoration_device=\"cpu\"  # Force CPU usage\n)\n</code></pre>"},{"location":"getting-started/installation.html#paddleocr-model-download-fails","title":"PaddleOCR model download fails","text":"<p>Solution: Manually download models or check your network connection:</p> <pre><code>from doctra.parsers import StructuredPDFParser\n\n# This will trigger model download\nparser = StructuredPDFParser()\n</code></pre> <p>Models are downloaded to <code>~/.paddleocr/</code> on first use.</p>"},{"location":"getting-started/installation.html#next-steps","title":"Next Steps","text":"<p>Now that you have Doctra installed, check out:</p> <ul> <li>Quick Start - Your first Doctra program</li> <li>System Requirements - Detailed hardware requirements</li> <li>User Guide - Learn about core concepts</li> </ul>"},{"location":"getting-started/installation.html#getting-help","title":"Getting Help","text":"<p>If you encounter issues during installation:</p> <ol> <li>Check the GitHub Issues for similar problems</li> <li>Create a new issue with:<ul> <li>Your operating system and version</li> <li>Python version (<code>python --version</code>)</li> <li>Full error message</li> <li>Installation method used</li> </ul> </li> </ol>"},{"location":"getting-started/quick-start.html","title":"Quick Start","text":"<p>This guide will get you started with Doctra in just a few minutes.</p>"},{"location":"getting-started/quick-start.html#your-first-document-parse","title":"Your First Document Parse","text":"<p>Let's parse a PDF document and extract its content:</p> <pre><code>from doctra import StructuredPDFParser\n\n# Initialize the parser\nparser = StructuredPDFParser()\n\n# Parse a document\nparser.parse(\"document.pdf\")\n</code></pre> <p>That's it! Doctra will:</p> <ol> <li>Detect the document layout</li> <li>Extract text using OCR</li> <li>Save images of figures, charts, and tables</li> <li>Generate a Markdown file with all content</li> </ol>"},{"location":"getting-started/quick-start.html#understanding-the-output","title":"Understanding the Output","text":"<p>After parsing, you'll find the following structure:</p> <pre><code>outputs/\n\u2514\u2500\u2500 document/\n    \u251c\u2500\u2500 full_parse/\n    \u2502   \u251c\u2500\u2500 result.md          # Markdown with all content\n    \u2502   \u251c\u2500\u2500 result.html        # HTML version\n    \u2502   \u2514\u2500\u2500 images/            # Extracted visual elements\n    \u2502       \u251c\u2500\u2500 figures/       # Document figures\n    \u2502       \u251c\u2500\u2500 charts/        # Charts and graphs\n    \u2502       \u2514\u2500\u2500 tables/        # Table images\n</code></pre>"},{"location":"getting-started/quick-start.html#basic-examples","title":"Basic Examples","text":""},{"location":"getting-started/quick-start.html#parse-with-custom-output-directory","title":"Parse with Custom Output Directory","text":"<pre><code>from doctra import StructuredPDFParser\n\nparser = StructuredPDFParser()\nparser.parse(\"document.pdf\", output_base_dir=\"my_outputs\")\n</code></pre>"},{"location":"getting-started/quick-start.html#parse-scanned-documents","title":"Parse Scanned Documents","text":"<p>For scanned or low-quality documents, use the enhanced parser:</p> <pre><code>from doctra import EnhancedPDFParser\n\nparser = EnhancedPDFParser(\n    use_image_restoration=True,\n    restoration_task=\"appearance\"  # Improve overall appearance\n)\n\nparser.parse(\"scanned_document.pdf\")\n</code></pre>"},{"location":"getting-started/quick-start.html#extract-only-charts-and-tables","title":"Extract Only Charts and Tables","text":"<p>If you only need charts and tables:</p> <pre><code>from doctra import ChartTablePDFParser\n\nparser = ChartTablePDFParser(\n    extract_charts=True,\n    extract_tables=True\n)\n\nparser.parse(\"data_report.pdf\")\n</code></pre>"},{"location":"getting-started/quick-start.html#using-vision-language-models","title":"Using Vision Language Models","text":"<p>To convert charts and tables to structured data, add VLM support:</p> <pre><code>from doctra import StructuredPDFParser\n\nparser = StructuredPDFParser(\n    use_vlm=True,\n    vlm_provider=\"openai\",\n    vlm_api_key=\"your-api-key-here\"\n)\n\nparser.parse(\"document.pdf\")\n</code></pre> <p>This will generate:</p> <ul> <li><code>tables.xlsx</code> - Excel file with extracted table data</li> <li><code>tables.html</code> - HTML tables for web viewing</li> <li><code>vlm_items.json</code> - JSON with structured data</li> </ul> <p>VLM Providers</p> <p>Doctra supports multiple VLM providers:</p> <ul> <li><code>\"openai\"</code> - GPT-4 Vision and GPT-4o</li> <li><code>\"gemini\"</code> - Google's Gemini models</li> <li><code>\"anthropic\"</code> - Claude with vision</li> <li><code>\"openrouter\"</code> - Access multiple models</li> <li><code>\"qianfan\"</code> - Baidu AI Cloud ERNIE models</li> </ul>"},{"location":"getting-started/quick-start.html#document-restoration","title":"Document Restoration","text":"<p>Enhance document quality before parsing:</p> <pre><code>from doctra import DocResEngine\n\n# Initialize restoration engine\ndocres = DocResEngine(device=\"cuda\")  # Use GPU for speed\n\n# Restore a single image\nrestored_img, metadata = docres.restore_image(\n    image=\"blurry_doc.jpg\",\n    task=\"deblurring\"\n)\n\n# Or enhance an entire PDF\ndocres.restore_pdf(\n    pdf_path=\"low_quality.pdf\",\n    output_path=\"enhanced.pdf\",\n    task=\"appearance\"\n)\n</code></pre> <p>Available restoration tasks:</p> Task Description <code>appearance</code> General appearance enhancement <code>dewarping</code> Correct perspective distortion <code>deshadowing</code> Remove shadows <code>deblurring</code> Reduce blur <code>binarization</code> Convert to black and white <code>end2end</code> Complete restoration pipeline"},{"location":"getting-started/quick-start.html#using-the-web-ui","title":"Using the Web UI","text":"<p>Launch the graphical interface for easy document processing:</p> <pre><code>from doctra import launch_ui\n\n# Launch web interface\nlaunch_ui()\n</code></pre> <p>Or from the command line:</p> <pre><code>python -m doctra.ui.app\n</code></pre> <p>Then open your browser to the displayed URL (typically <code>http://127.0.0.1:7860</code>).</p>"},{"location":"getting-started/quick-start.html#command-line-interface","title":"Command Line Interface","text":"<p>Doctra provides a powerful CLI:</p> <pre><code># Parse a document\ndoctra parse document.pdf\n\n# Enhanced parsing\ndoctra enhance document.pdf --restoration-task appearance\n\n# Extract charts and tables\ndoctra extract both document.pdf --use-vlm\n\n# Visualize layout\ndoctra visualize document.pdf\n</code></pre> <p>See the CLI Reference for all available commands.</p>"},{"location":"getting-started/quick-start.html#layout-visualization","title":"Layout Visualization","text":"<p>Visualize how Doctra detects document elements:</p> <pre><code>from doctra import StructuredPDFParser\n\nparser = StructuredPDFParser()\n\n# Display layout detection results\nparser.display_pages_with_boxes(\n    pdf_path=\"document.pdf\",\n    num_pages=3,  # First 3 pages\n    save_path=\"layout_viz.png\"\n)\n</code></pre> <p>This creates a visual representation showing:</p> <ul> <li>Detected text regions (blue boxes)</li> <li>Tables (red boxes)</li> <li>Charts (green boxes)</li> <li>Figures (orange boxes)</li> <li>Confidence scores for each element</li> </ul>"},{"location":"getting-started/quick-start.html#configuration-options","title":"Configuration Options","text":""},{"location":"getting-started/quick-start.html#parser-configuration","title":"Parser Configuration","text":"<pre><code>parser = StructuredPDFParser(\n    # Layout Detection\n    layout_model_name=\"PP-DocLayout_plus-L\",  # Model choice\n    dpi=200,  # Image resolution\n    min_score=0.5,  # Confidence threshold\n\n    # OCR Settings\n    ocr_lang=\"eng\",  # Language code\n    ocr_psm=6,  # Page segmentation mode\n\n    # Output\n    box_separator=\"\\n\"  # Separator between elements\n)\n</code></pre>"},{"location":"getting-started/quick-start.html#enhanced-parser-configuration","title":"Enhanced Parser Configuration","text":"<pre><code>from doctra import EnhancedPDFParser\n\nparser = EnhancedPDFParser(\n    # Image Restoration\n    use_image_restoration=True,\n    restoration_task=\"dewarping\",\n    restoration_device=\"cuda\",  # or \"cpu\"\n    restoration_dpi=300,\n\n    # All StructuredPDFParser options also available\n    use_vlm=True,\n    vlm_provider=\"openai\",\n    vlm_api_key=\"your-key\"\n)\n</code></pre>"},{"location":"getting-started/quick-start.html#common-patterns","title":"Common Patterns","text":""},{"location":"getting-started/quick-start.html#batch-processing","title":"Batch Processing","text":"<pre><code>import os\nfrom doctra import StructuredPDFParser\n\nparser = StructuredPDFParser()\n\n# Process all PDFs in a directory\npdf_dir = \"documents\"\nfor filename in os.listdir(pdf_dir):\n    if filename.endswith(\".pdf\"):\n        pdf_path = os.path.join(pdf_dir, filename)\n        print(f\"Processing {filename}...\")\n        parser.parse(pdf_path)\n</code></pre>"},{"location":"getting-started/quick-start.html#error-handling","title":"Error Handling","text":"<pre><code>from doctra import StructuredPDFParser\n\nparser = StructuredPDFParser()\n\ntry:\n    parser.parse(\"document.pdf\")\nexcept FileNotFoundError:\n    print(\"Document not found!\")\nexcept Exception as e:\n    print(f\"Error parsing document: {e}\")\n</code></pre>"},{"location":"getting-started/quick-start.html#progress-tracking","title":"Progress Tracking","text":"<pre><code>from doctra import StructuredPDFParser\n\nparser = StructuredPDFParser()\n\n# Progress bars are shown automatically\nparser.parse(\"large_document.pdf\")\n</code></pre>"},{"location":"getting-started/quick-start.html#next-steps","title":"Next Steps","text":"<p>Now that you've learned the basics:</p> <ol> <li>Dive Deeper: Read the User Guide for detailed explanations</li> <li>Explore Parsers: Learn about each parser's capabilities</li> <li>Advanced Examples: Check out Advanced Examples</li> <li>API Reference: Browse the API Documentation</li> </ol>"},{"location":"getting-started/quick-start.html#getting-help","title":"Getting Help","text":"<ul> <li> Read the full documentation</li> <li> Check GitHub issues</li> <li> Ask questions in discussions</li> </ul>"},{"location":"getting-started/quick-start.html#common-issues","title":"Common Issues","text":""},{"location":"getting-started/quick-start.html#poppler-not-found-error","title":"\"Poppler not found\" Error","text":"<p>Install Poppler (see Installation).</p>"},{"location":"getting-started/quick-start.html#low-ocr-accuracy","title":"Low OCR Accuracy","text":"<p>Try the enhanced parser with image restoration:</p> <pre><code>from doctra import EnhancedPDFParser\n\nparser = EnhancedPDFParser(\n    use_image_restoration=True,\n    restoration_task=\"appearance\"\n)\n</code></pre>"},{"location":"getting-started/quick-start.html#slow-processing","title":"Slow Processing","text":"<p>Use GPU acceleration:</p> <pre><code>parser = EnhancedPDFParser(\n    restoration_device=\"cuda\"  # Use GPU\n)\n</code></pre> <p>Or reduce DPI:</p> <pre><code>parser = StructuredPDFParser(\n    dpi=150  # Lower resolution\n)\n</code></pre>"},{"location":"getting-started/system-requirements.html","title":"System Requirements","text":"<p>This page outlines the hardware and software requirements for running Doctra effectively.</p>"},{"location":"getting-started/system-requirements.html#python-requirements","title":"Python Requirements","text":"<ul> <li>Python Version: 3.8 or higher</li> <li>Operating Systems: <ul> <li>Linux (Ubuntu, Debian, CentOS, etc.)</li> <li>macOS (10.13 or higher)</li> <li>Windows (10 or higher)</li> </ul> </li> </ul>"},{"location":"getting-started/system-requirements.html#hardware-requirements","title":"Hardware Requirements","text":""},{"location":"getting-started/system-requirements.html#minimum-requirements","title":"Minimum Requirements","text":"Component Specification CPU Dual-core processor, 2.0 GHz RAM 4 GB Disk Space 2 GB for installation + space for outputs GPU Not required (CPU processing available)"},{"location":"getting-started/system-requirements.html#recommended-requirements","title":"Recommended Requirements","text":"Component Specification CPU Quad-core processor, 3.0 GHz or higher RAM 8 GB or more Disk Space 10 GB for installation + models + outputs GPU NVIDIA GPU with 4+ GB VRAM (for acceleration)"},{"location":"getting-started/system-requirements.html#performance-considerations","title":"Performance Considerations","text":""},{"location":"getting-started/system-requirements.html#processing-speed","title":"Processing Speed","text":"<p>Typical processing times for a 10-page PDF:</p> Configuration Time CPU only (4 cores) ~2-3 minutes GPU (NVIDIA GTX 1060) ~1-2 minutes GPU (NVIDIA RTX 3080) ~30-60 seconds <p>Factors Affecting Performance</p> <ul> <li>Document complexity (number of images, tables, charts)</li> <li>Image resolution (DPI setting)</li> <li>Image restoration enabled/disabled</li> <li>VLM processing (requires network calls)</li> </ul>"},{"location":"getting-started/system-requirements.html#memory-usage","title":"Memory Usage","text":"<p>Expected RAM usage:</p> <ul> <li>Basic parsing: 500 MB - 2 GB</li> <li>Enhanced parsing: 1 GB - 4 GB</li> <li>VLM processing: Additional 500 MB - 1 GB</li> <li>High DPI (300+): Additional 2-4 GB</li> </ul>"},{"location":"getting-started/system-requirements.html#software-dependencies","title":"Software Dependencies","text":""},{"location":"getting-started/system-requirements.html#required","title":"Required","text":"<ol> <li> <p>Poppler - PDF rendering and processing</p> <ul> <li>Version: Latest stable release</li> <li>Installation: See Installation Guide</li> </ul> </li> <li> <p>Tesseract OCR - Text extraction</p> <ul> <li>Automatically installed via Python dependencies</li> <li>No manual installation required</li> </ul> </li> </ol>"},{"location":"getting-started/system-requirements.html#optional","title":"Optional","text":"<ol> <li> <p>CUDA Toolkit - For GPU acceleration</p> <ul> <li>Version: 11.8 or higher</li> <li>Required only for GPU processing</li> <li>Download: NVIDIA CUDA Downloads</li> </ul> </li> <li> <p>cuDNN - Deep learning GPU acceleration</p> <ul> <li>Version: 8.6 or higher</li> <li>Required only for GPU processing</li> <li>Download: NVIDIA cuDNN Downloads</li> </ul> </li> </ol>"},{"location":"getting-started/system-requirements.html#gpu-support","title":"GPU Support","text":""},{"location":"getting-started/system-requirements.html#cuda-requirements","title":"CUDA Requirements","text":"<p>For GPU-accelerated processing:</p> <ul> <li>GPU: NVIDIA GPU with Compute Capability 3.5 or higher</li> <li>CUDA: Version 11.8 or higher</li> <li>cuDNN: Version 8.6 or higher</li> <li>Driver: Compatible NVIDIA driver</li> </ul>"},{"location":"getting-started/system-requirements.html#supported-gpus","title":"Supported GPUs","text":"<p>Doctra's image restoration works with CUDA-capable NVIDIA GPUs:</p> GPU Series Support Level GeForce GTX 10xx and newer \u2705 Full support GeForce RTX series \u2705 Full support Tesla series \u2705 Full support Quadro series \u2705 Full support AMD GPUs \u274c Not supported Intel GPUs \u274c Not supported"},{"location":"getting-started/system-requirements.html#checking-gpu-compatibility","title":"Checking GPU Compatibility","text":"<p>Verify CUDA availability:</p> <pre><code>import torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"CUDA version: {torch.version.cuda}\")\nprint(f\"GPU: {torch.cuda.get_device_name(0)}\")\n</code></pre>"},{"location":"getting-started/system-requirements.html#network-requirements","title":"Network Requirements","text":""},{"location":"getting-started/system-requirements.html#model-downloads","title":"Model Downloads","text":"<p>On first use, Doctra downloads AI models:</p> <ul> <li>PaddleOCR models: ~300 MB</li> <li>DocRes models: ~200 MB</li> <li>Total: ~500 MB initial download</li> </ul> <p>Models are cached locally after first download.</p>"},{"location":"getting-started/system-requirements.html#vlm-api-access","title":"VLM API Access","text":"<p>If using Vision Language Models:</p> <ul> <li>Stable internet connection required</li> <li>API rate limits apply (provider-dependent)</li> <li>Bandwidth: Minimal (images are compressed before sending)</li> </ul>"},{"location":"getting-started/system-requirements.html#storage-requirements","title":"Storage Requirements","text":""},{"location":"getting-started/system-requirements.html#installation","title":"Installation","text":"Component Size Doctra package ~50 MB Python dependencies ~500 MB AI models (downloaded on first use) ~500 MB Total ~1 GB"},{"location":"getting-started/system-requirements.html#processing-outputs","title":"Processing Outputs","text":"<p>Expected output sizes per document:</p> Document Size Output Size (approx.) 10-page report 5-20 MB 50-page document 25-100 MB 100-page book 50-200 MB <p>Storage Planning</p> <p>Plan for 2-10x the original PDF size for outputs, depending on:</p> <ul> <li>Number of images in the document</li> <li>DPI settings used</li> <li>Whether image restoration is enabled</li> </ul>"},{"location":"getting-started/system-requirements.html#browser-requirements-web-ui","title":"Browser Requirements (Web UI)","text":"<p>For the Gradio-based web interface:</p> <ul> <li>Modern Browser: Chrome 90+, Firefox 88+, Safari 14+, Edge 90+</li> <li>JavaScript: Must be enabled</li> <li>Local Network: Access to localhost required</li> </ul>"},{"location":"getting-started/system-requirements.html#cloud-deployment","title":"Cloud Deployment","text":"<p>Doctra can run on cloud platforms:</p>"},{"location":"getting-started/system-requirements.html#recommended-cloud-specs","title":"Recommended Cloud Specs","text":"Provider Instance Type vCPUs RAM GPU AWS t3.xlarge 4 16 GB Optional GCP n1-standard-4 4 15 GB Optional Azure Standard_D4s_v3 4 16 GB Optional <p>For GPU processing:</p> Provider Instance Type GPU VRAM AWS g4dn.xlarge T4 16 GB GCP n1-standard-4 + T4 T4 16 GB Azure NC6 K80 12 GB"},{"location":"getting-started/system-requirements.html#google-colab","title":"Google Colab","text":"<p>Doctra works perfectly in Google Colab:</p> <ul> <li>Free Tier: Sufficient for most use cases</li> <li>GPU: Available in free tier</li> <li>RAM: 12-13 GB in free tier</li> <li>Disk: 100+ GB temporary storage</li> </ul>"},{"location":"getting-started/system-requirements.html#operating-system-specific-notes","title":"Operating System Specific Notes","text":""},{"location":"getting-started/system-requirements.html#linux","title":"Linux","text":"<ul> <li>Best Performance: Generally fastest due to better CUDA support</li> <li>Easy Setup: Package managers make dependency installation simple</li> <li>Docker: Easy containerization for deployment</li> </ul>"},{"location":"getting-started/system-requirements.html#macos","title":"macOS","text":"<ul> <li>No GPU Support: CUDA not available on macOS</li> <li>Good CPU Performance: Efficient on Apple Silicon (M1/M2)</li> <li>Poppler: Easy installation via Homebrew</li> </ul>"},{"location":"getting-started/system-requirements.html#windows","title":"Windows","text":"<ul> <li>GPU Support: Full CUDA support available</li> <li>Poppler Setup: Requires manual installation or conda</li> <li>Path Configuration: May need to add Poppler to PATH</li> </ul>"},{"location":"getting-started/system-requirements.html#performance-optimization","title":"Performance Optimization","text":""},{"location":"getting-started/system-requirements.html#for-cpu-only-systems","title":"For CPU-Only Systems","text":"<pre><code>parser = StructuredPDFParser(\n    dpi=150,  # Lower resolution\n    min_score=0.7  # Higher threshold = fewer elements\n)\n</code></pre>"},{"location":"getting-started/system-requirements.html#for-gpu-systems","title":"For GPU Systems","text":"<pre><code>from doctra import EnhancedPDFParser\n\nparser = EnhancedPDFParser(\n    use_image_restoration=True,\n    restoration_device=\"cuda\",  # Use GPU\n    restoration_dpi=300  # Higher quality\n)\n</code></pre>"},{"location":"getting-started/system-requirements.html#memory-optimization","title":"Memory Optimization","text":"<pre><code># Process documents in batches\nimport os\nfrom doctra import StructuredPDFParser\n\nparser = StructuredPDFParser()\n\n# Process one at a time to manage memory\nfor pdf_file in pdf_files:\n    parser.parse(pdf_file)\n    # Parser is reused, memory is cleaned between documents\n</code></pre>"},{"location":"getting-started/system-requirements.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/system-requirements.html#out-of-memory-errors","title":"Out of Memory Errors","text":"<p>Solutions:</p> <ol> <li>Reduce DPI: <code>dpi=100</code></li> <li>Disable image restoration</li> <li>Close other applications</li> <li>Process fewer pages at once</li> </ol>"},{"location":"getting-started/system-requirements.html#slow-processing","title":"Slow Processing","text":"<p>Solutions:</p> <ol> <li>Enable GPU: <code>restoration_device=\"cuda\"</code></li> <li>Reduce DPI: <code>dpi=150</code></li> <li>Upgrade hardware</li> <li>Process during off-peak hours</li> </ol>"},{"location":"getting-started/system-requirements.html#model-download-failures","title":"Model Download Failures","text":"<p>Solutions:</p> <ol> <li>Check internet connection</li> <li>Verify firewall settings</li> <li>Use VPN if behind restrictive network</li> <li>Manual model download (see troubleshooting guide)</li> </ol>"},{"location":"getting-started/system-requirements.html#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide - Install Doctra</li> <li>Quick Start - Start using Doctra</li> <li>Performance Tips - Optimize your setup</li> </ul>"},{"location":"interfaces/cli.html","title":"Command Line Interface","text":"<p>Doctra provides a powerful CLI for document processing automation.</p>"},{"location":"interfaces/cli.html#installation","title":"Installation","text":"<p>The CLI is automatically installed with Doctra:</p> <pre><code>pip install doctra\n</code></pre> <p>Verify installation:</p> <pre><code>doctra --version\n</code></pre>"},{"location":"interfaces/cli.html#basic-usage","title":"Basic Usage","text":"<pre><code>doctra [COMMAND] [OPTIONS] [ARGUMENTS]\n</code></pre>"},{"location":"interfaces/cli.html#commands","title":"Commands","text":""},{"location":"interfaces/cli.html#parse","title":"parse","text":"<p>Parse a PDF document with full processing.</p> <pre><code>doctra parse &lt;pdf_file&gt; [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--output-dir PATH</code>: Output directory (default: <code>outputs</code>)</li> <li><code>--dpi INTEGER</code>: Image resolution (default: 200)</li> <li><code>--min-score FLOAT</code>: Minimum confidence score (default: 0.0)</li> <li><code>--ocr-lang TEXT</code>: OCR language code (default: <code>eng</code>)</li> <li><code>--use-vlm</code>: Enable VLM processing</li> <li><code>--vlm-provider TEXT</code>: VLM provider (<code>openai</code>, <code>gemini</code>, <code>anthropic</code>, <code>openrouter</code>)</li> <li><code>--vlm-api-key TEXT</code>: VLM API key</li> <li><code>--vlm-model TEXT</code>: Specific VLM model</li> </ul> <p>Example:</p> <pre><code># Basic parsing\ndoctra parse document.pdf\n\n# With custom settings\ndoctra parse document.pdf --dpi 300 --output-dir my_outputs\n\n# With VLM\ndoctra parse document.pdf --use-vlm --vlm-provider openai --vlm-api-key sk-xxx\n</code></pre>"},{"location":"interfaces/cli.html#enhance","title":"enhance","text":"<p>Parse with image restoration for low-quality documents.</p> <pre><code>doctra enhance &lt;pdf_file&gt; [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li>All <code>parse</code> options, plus:</li> <li><code>--restoration-task TEXT</code>: Restoration task (default: <code>appearance</code>)<ul> <li>Choices: <code>appearance</code>, <code>dewarping</code>, <code>deshadowing</code>, <code>deblurring</code>, <code>binarization</code>, <code>end2end</code></li> </ul> </li> <li><code>--restoration-device TEXT</code>: Device (<code>cuda</code>, <code>cpu</code>, or auto)</li> <li><code>--restoration-dpi INTEGER</code>: DPI for restoration (default: 200)</li> </ul> <p>Example:</p> <pre><code># Basic enhancement\ndoctra enhance scanned.pdf\n\n# Dewarp with GPU\ndoctra enhance scanned.pdf --restoration-task dewarping --restoration-device cuda\n\n# Full enhancement with VLM\ndoctra enhance scanned.pdf \\\n  --restoration-task appearance \\\n  --restoration-device cuda \\\n  --use-vlm \\\n  --vlm-provider openai \\\n  --vlm-api-key sk-xxx\n</code></pre>"},{"location":"interfaces/cli.html#extract","title":"extract","text":"<p>Extract only charts and/or tables from a document.</p> <pre><code>doctra extract &lt;type&gt; &lt;pdf_file&gt; [OPTIONS]\n</code></pre> <p>Type:</p> <ul> <li><code>charts</code>: Extract only charts</li> <li><code>tables</code>: Extract only tables</li> <li><code>both</code>: Extract both charts and tables</li> </ul> <p>Options:</p> <ul> <li><code>--output-dir PATH</code>: Output directory (default: <code>outputs</code>)</li> <li><code>--dpi INTEGER</code>: Image resolution (default: 200)</li> <li><code>--use-vlm</code>: Enable VLM for structured data</li> <li><code>--vlm-provider TEXT</code>: VLM provider</li> <li><code>--vlm-api-key TEXT</code>: VLM API key</li> <li><code>--vlm-model TEXT</code>: Specific VLM model</li> </ul> <p>Examples:</p> <pre><code># Extract charts only\ndoctra extract charts report.pdf\n\n# Extract tables with VLM\ndoctra extract tables report.pdf --use-vlm --vlm-provider gemini --vlm-api-key xxx\n\n# Extract both\ndoctra extract both report.pdf --output-dir data_extracts\n</code></pre>"},{"location":"interfaces/cli.html#visualize","title":"visualize","text":"<p>Visualize layout detection results.</p> <pre><code>doctra visualize &lt;pdf_file&gt; [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--num-pages INTEGER</code>: Number of pages to visualize (default: 3)</li> <li><code>--cols INTEGER</code>: Number of columns in grid (default: 2)</li> <li><code>--page-width INTEGER</code>: Width of each page (default: 800)</li> <li><code>--spacing INTEGER</code>: Spacing between pages (default: 40)</li> <li><code>--output PATH</code>: Save to file instead of displaying</li> <li><code>--dpi INTEGER</code>: Image resolution (default: 200)</li> </ul> <p>Examples:</p> <pre><code># Display first 3 pages\ndoctra visualize document.pdf\n\n# Save visualization of 6 pages\ndoctra visualize document.pdf --num-pages 6 --output layout.png\n\n# Custom grid layout\ndoctra visualize document.pdf --num-pages 9 --cols 3 --page-width 600\n</code></pre>"},{"location":"interfaces/cli.html#analyze","title":"analyze","text":"<p>Quick document analysis showing structure.</p> <pre><code>doctra analyze &lt;pdf_file&gt; [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--dpi INTEGER</code>: Image resolution (default: 200)</li> </ul> <p>Example:</p> <pre><code>doctra analyze document.pdf\n</code></pre> <p>Output shows:</p> <pre><code>Document Analysis: document.pdf\n=====================================\nTotal pages: 10\n\nPage 1:\n  - Text regions: 5\n  - Tables: 1\n  - Charts: 0\n  - Figures: 2\n\nPage 2:\n  ...\n</code></pre>"},{"location":"interfaces/cli.html#info","title":"info","text":"<p>Display system and configuration information.</p> <pre><code>doctra info\n</code></pre> <p>Shows:</p> <ul> <li>Doctra version</li> <li>Python version</li> <li>Installed dependencies</li> <li>GPU availability</li> <li>System information</li> </ul> <p>Example output:</p> <pre><code>Doctra Information\n==================\nVersion: 0.4.3\nPython: 3.10.11\n\nDependencies:\n  - PaddlePaddle: 2.5.0\n  - PaddleOCR: 2.7.0\n  - PyTesseract: 0.3.10\n  - Pillow: 10.0.0\n\nSystem:\n  - OS: Windows 10\n  - CUDA Available: Yes\n  - GPU: NVIDIA GeForce RTX 3080\n</code></pre>"},{"location":"interfaces/cli.html#batch-processing","title":"Batch Processing","text":""},{"location":"interfaces/cli.html#process-multiple-files","title":"Process Multiple Files","text":"<pre><code># Using shell globbing\ndoctra parse *.pdf --output-dir batch_results\n\n# Using find (Linux/Mac)\nfind ./documents -name \"*.pdf\" -exec doctra parse {} \\;\n\n# Using PowerShell (Windows)\nGet-ChildItem *.pdf | ForEach-Object { doctra parse $_.FullName }\n</code></pre>"},{"location":"interfaces/cli.html#process-directory","title":"Process Directory","text":"<pre><code># Parse all PDFs in directory\nfor pdf in directory/*.pdf; do\n    doctra parse \"$pdf\" --output-dir results/\ndone\n</code></pre>"},{"location":"interfaces/cli.html#environment-variables","title":"Environment Variables","text":"<p>Set default values using environment variables:</p> <pre><code># VLM Configuration\nexport DOCTRA_VLM_PROVIDER=openai\nexport DOCTRA_VLM_API_KEY=sk-xxx\nexport DOCTRA_VLM_MODEL=gpt-4o\n\n# Processing Settings\nexport DOCTRA_DPI=200\nexport DOCTRA_OCR_LANG=eng\nexport DOCTRA_DEVICE=cuda\n\n# Then use without flags\ndoctra parse document.pdf --use-vlm\n</code></pre>"},{"location":"interfaces/cli.html#configuration-file","title":"Configuration File","text":"<p>Create <code>.doctra.yml</code> in your project directory:</p> <pre><code># .doctra.yml\nvlm:\n  provider: openai\n  api_key: sk-xxx\n  model: gpt-4o\n\nprocessing:\n  dpi: 200\n  ocr_lang: eng\n  device: cuda\n\noutput:\n  base_dir: outputs\n</code></pre> <p>Then run commands without options:</p> <pre><code>doctra parse document.pdf\n</code></pre>"},{"location":"interfaces/cli.html#output-structure","title":"Output Structure","text":""},{"location":"interfaces/cli.html#standard-parse","title":"Standard Parse","text":"<pre><code>outputs/\n\u2514\u2500\u2500 document/\n    \u2514\u2500\u2500 full_parse/\n        \u251c\u2500\u2500 result.md\n        \u251c\u2500\u2500 result.html\n        \u2514\u2500\u2500 images/\n            \u251c\u2500\u2500 figures/\n            \u251c\u2500\u2500 charts/\n            \u2514\u2500\u2500 tables/\n</code></pre>"},{"location":"interfaces/cli.html#enhanced-parse","title":"Enhanced Parse","text":"<pre><code>outputs/\n\u2514\u2500\u2500 document/\n    \u2514\u2500\u2500 enhanced_parse/\n        \u251c\u2500\u2500 result.md\n        \u251c\u2500\u2500 result.html\n        \u251c\u2500\u2500 document_enhanced.pdf  # Restored PDF\n        \u251c\u2500\u2500 enhanced_pages/  # Restored page images\n        \u2514\u2500\u2500 images/\n</code></pre>"},{"location":"interfaces/cli.html#extract_1","title":"Extract","text":"<pre><code>outputs/\n\u2514\u2500\u2500 document/\n    \u2514\u2500\u2500 structured_parsing/\n        \u251c\u2500\u2500 charts/  # Chart images\n        \u251c\u2500\u2500 tables/  # Table images\n        \u251c\u2500\u2500 parsed_tables_charts.xlsx  # If VLM enabled\n        \u251c\u2500\u2500 parsed_tables_charts.html  # If VLM enabled\n        \u2514\u2500\u2500 vlm_items.json  # If VLM enabled\n</code></pre>"},{"location":"interfaces/cli.html#examples","title":"Examples","text":""},{"location":"interfaces/cli.html#example-1-basic-document-processing","title":"Example 1: Basic Document Processing","text":"<pre><code># Parse a financial report\ndoctra parse financial_report.pdf\n\n# Output: outputs/financial_report/full_parse/\n</code></pre>"},{"location":"interfaces/cli.html#example-2-enhanced-processing-with-vlm","title":"Example 2: Enhanced Processing with VLM","text":"<pre><code># Process scanned document with enhancement and VLM\ndoctra enhance scanned_document.pdf \\\n  --restoration-task appearance \\\n  --restoration-device cuda \\\n  --use-vlm \\\n  --vlm-provider openai \\\n  --vlm-api-key $OPENAI_API_KEY \\\n  --output-dir enhanced_results\n</code></pre>"},{"location":"interfaces/cli.html#example-3-extract-data-for-analysis","title":"Example 3: Extract Data for Analysis","text":"<pre><code># Extract all tables with VLM to get structured data\ndoctra extract tables data_report.pdf \\\n  --use-vlm \\\n  --vlm-provider gemini \\\n  --vlm-api-key $GEMINI_API_KEY\n\n# Result: outputs/data_report/structured_parsing/parsed_tables_charts.xlsx\n</code></pre>"},{"location":"interfaces/cli.html#example-4-batch-processing-pipeline","title":"Example 4: Batch Processing Pipeline","text":"<pre><code>#!/bin/bash\n# process_documents.sh\n\nINPUT_DIR=\"./input_pdfs\"\nOUTPUT_DIR=\"./processed\"\n\nfor pdf in \"$INPUT_DIR\"/*.pdf; do\n    echo \"Processing: $pdf\"\n\n    # First enhance the document\n    doctra enhance \"$pdf\" \\\n      --restoration-task appearance \\\n      --restoration-device cuda \\\n      --output-dir \"$OUTPUT_DIR\"\n\n    echo \"Completed: $pdf\"\ndone\n\necho \"All documents processed!\"\n</code></pre>"},{"location":"interfaces/cli.html#example-5-quality-check-with-visualization","title":"Example 5: Quality Check with Visualization","text":"<pre><code># Visualize layout detection before full processing\ndoctra visualize document.pdf --num-pages 5 --output viz_check.png\n\n# Review viz_check.png to ensure good detection\n\n# Then proceed with full processing\ndoctra parse document.pdf --use-vlm\n</code></pre>"},{"location":"interfaces/cli.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"interfaces/cli.html#command-not-found","title":"Command Not Found","text":"<p>Problem: <code>doctra: command not found</code></p> <p>Solution:</p> <pre><code># Ensure Doctra is installed\npip install doctra\n\n# Or use module syntax\npython -m doctra.cli.main parse document.pdf\n</code></pre>"},{"location":"interfaces/cli.html#api-key-errors","title":"API Key Errors","text":"<p>Problem: VLM API key not recognized</p> <p>Solution:</p> <pre><code># Set environment variable\nexport OPENAI_API_KEY=sk-xxx\n\n# Or pass directly\ndoctra parse document.pdf --use-vlm --vlm-api-key sk-xxx\n</code></pre>"},{"location":"interfaces/cli.html#poppler-errors","title":"Poppler Errors","text":"<p>Problem: <code>pdftoppm not found</code></p> <p>Solution: Install Poppler (see Installation Guide)</p>"},{"location":"interfaces/cli.html#memory-errors","title":"Memory Errors","text":"<p>Problem: Out of memory during processing</p> <p>Solution:</p> <pre><code># Reduce DPI\ndoctra parse large.pdf --dpi 150\n\n# Or process pages individually\ndoctra parse large.pdf --max-pages 10\n</code></pre>"},{"location":"interfaces/cli.html#advanced-usage","title":"Advanced Usage","text":""},{"location":"interfaces/cli.html#custom-scripts","title":"Custom Scripts","text":"<p>Combine CLI with shell scripts:</p> <pre><code>#!/bin/bash\n# Smart processing script\n\nPDF=$1\n\n# Check file size\nSIZE=$(du -k \"$PDF\" | cut -f1)\n\nif [ $SIZE -gt 10000 ]; then\n    echo \"Large file, using lower DPI...\"\n    doctra parse \"$PDF\" --dpi 150\nelse\n    echo \"Standard processing...\"\n    doctra parse \"$PDF\" --dpi 200 --use-vlm\nfi\n</code></pre>"},{"location":"interfaces/cli.html#integration-with-other-tools","title":"Integration with Other Tools","text":"<pre><code># OCR + Search Pipeline\ndoctra parse document.pdf\ngrep \"keyword\" outputs/document/full_parse/result.md\n\n# Extract data and analyze\ndoctra extract tables report.pdf --use-vlm\npython analyze_tables.py outputs/report/structured_parsing/parsed_tables_charts.xlsx\n</code></pre>"},{"location":"interfaces/cli.html#see-also","title":"See Also","text":"<ul> <li>Python API - Programmatic usage</li> <li>Web UI - Graphical interface</li> <li>Examples - Usage examples</li> </ul>"},{"location":"interfaces/web-ui.html","title":"Web UI","text":"<p>Guide to using Doctra's Gradio-based web interface.</p>"},{"location":"interfaces/web-ui.html#overview","title":"Overview","text":"<p>Doctra provides a user-friendly web interface for document processing without writing code.</p>"},{"location":"interfaces/web-ui.html#launching-the-ui","title":"Launching the UI","text":""},{"location":"interfaces/web-ui.html#python","title":"Python","text":"<pre><code>from doctra import launch_ui\n\n# Launch web interface\nlaunch_ui()\n</code></pre>"},{"location":"interfaces/web-ui.html#command-line","title":"Command Line","text":"<pre><code>python -m doctra.ui.app\n</code></pre>"},{"location":"interfaces/web-ui.html#module-script","title":"Module Script","text":"<pre><code>python gradio_app.py\n</code></pre> <p>The UI opens at: <code>http://127.0.0.1:7860</code></p>"},{"location":"interfaces/web-ui.html#interface-tabs","title":"Interface Tabs","text":""},{"location":"interfaces/web-ui.html#1-full-parse","title":"1. Full Parse","text":"<p>Complete document processing:</p> <ul> <li>Upload PDF</li> <li>Configure settings</li> <li>View results</li> <li>Download outputs</li> </ul>"},{"location":"interfaces/web-ui.html#2-tables-charts","title":"2. Tables &amp; Charts","text":"<p>Specialized extraction:</p> <ul> <li>Extract charts and/or tables</li> <li>Enable VLM processing</li> <li>Configure API keys</li> <li>Download structured data</li> </ul>"},{"location":"interfaces/web-ui.html#3-docres","title":"3. DocRes","text":"<p>Image restoration:</p> <ul> <li>Upload images or PDFs</li> <li>Select restoration task</li> <li>Compare before/after</li> <li>Download enhanced files</li> </ul>"},{"location":"interfaces/web-ui.html#4-enhanced-parser","title":"4. Enhanced Parser","text":"<p>Combined restoration and parsing:</p> <ul> <li>Upload PDF</li> <li>Configure restoration</li> <li>Enable VLM</li> <li>Get comprehensive results</li> </ul>"},{"location":"interfaces/web-ui.html#features","title":"Features","text":"<ul> <li>Drag &amp; Drop: Easy file upload</li> <li>Real-time Progress: See processing status</li> <li>Preview Results: View output in browser</li> <li>Download ZIP: Get all results packaged</li> <li>Configuration: Adjust all settings</li> <li>API Key Management: Secure key input</li> </ul>"},{"location":"interfaces/web-ui.html#configuration-options","title":"Configuration Options","text":"<p>Each tab provides settings for:</p> <ul> <li>DPI resolution</li> <li>Language selection</li> <li>VLM provider and API key</li> <li>Restoration tasks</li> <li>Output preferences</li> </ul>"},{"location":"interfaces/web-ui.html#sharing-the-ui","title":"Sharing the UI","text":"<p>Launch with public URL:</p> <pre><code>from doctra import build_demo\n\ndemo = build_demo()\ndemo.launch(share=True)\n</code></pre> <p>This generates a temporary public URL for sharing.</p>"},{"location":"interfaces/web-ui.html#use-cases","title":"Use Cases","text":"<ul> <li>Non-technical Users: No coding required</li> <li>Quick Processing: Fast one-off document processing</li> <li>Experimentation: Try different settings</li> <li>Demonstrations: Show Doctra capabilities</li> <li>Prototyping: Test before integrating</li> </ul>"},{"location":"interfaces/web-ui.html#see-also","title":"See Also","text":"<ul> <li>CLI Reference - Command line interface</li> <li>API Reference - Python API</li> <li>Examples - Usage examples</li> </ul>"},{"location":"user-guide/core-concepts.html","title":"Core Concepts","text":"<p>Understanding Doctra's core concepts will help you use the library effectively.</p>"},{"location":"user-guide/core-concepts.html#document-processing-pipeline","title":"Document Processing Pipeline","text":"<p>Doctra follows a multi-stage pipeline for document processing:</p> <pre><code>graph LR\n    A[PDF Input] --&gt; B[Layout Detection]\n    B --&gt; C[Element Classification]\n    C --&gt; D[OCR Processing]\n    D --&gt; E[VLM Processing]\n    E --&gt; F[Output Generation]\n\n    B -.Optional.-&gt; G[Image Restoration]\n    G --&gt; C</code></pre>"},{"location":"user-guide/core-concepts.html#pipeline-stages","title":"Pipeline Stages","text":"<ol> <li>Layout Detection: Analyzes document structure using PaddleOCR</li> <li>Element Classification: Identifies text, tables, charts, and figures</li> <li>OCR Processing: Extracts text from identified regions</li> <li>VLM Processing (Optional): Converts visual elements to structured data</li> <li>Output Generation: Creates Markdown, Excel, HTML, and JSON files</li> </ol>"},{"location":"user-guide/core-concepts.html#architecture-overview","title":"Architecture Overview","text":"<p>Doctra is organized into several key components:</p>"},{"location":"user-guide/core-concepts.html#parsers","title":"Parsers","text":"<p>Parsers are the main entry point for document processing. They orchestrate the entire pipeline.</p> StructuredPDFParser The base parser for general PDF processing. Handles layout detection, OCR, and output generation. EnhancedPDFParser Extends StructuredPDFParser with image restoration capabilities for low-quality documents. ChartTablePDFParser Specialized parser focused on extracting only charts and tables."},{"location":"user-guide/core-concepts.html#engines","title":"Engines","text":"<p>Engines provide specific processing capabilities:</p> Layout Detection PaddleOCR-based layout analysis to identify document structure. OCR Engine Tesseract-based text extraction from images. DocRes Engine Image restoration for document enhancement. VLM Service Vision Language Model integration for structured data extraction."},{"location":"user-guide/core-concepts.html#exporters","title":"Exporters","text":"<p>Exporters handle output generation in various formats:</p> <ul> <li>MarkdownWriter: Creates human-readable Markdown files</li> <li>ExcelWriter: Generates spreadsheets with structured data</li> <li>HTMLWriter: Produces web-ready HTML documents</li> <li>ImageSaver: Saves cropped visual elements</li> </ul>"},{"location":"user-guide/core-concepts.html#element-types","title":"Element Types","text":"<p>Doctra classifies document elements into four main types:</p>"},{"location":"user-guide/core-concepts.html#text-elements","title":"Text Elements","text":"<p>Regular text content including:</p> <ul> <li>Paragraphs</li> <li>Headings</li> <li>Lists</li> <li>Captions</li> </ul> <p>Processing: OCR \u2192 Text extraction \u2192 Markdown formatting</p>"},{"location":"user-guide/core-concepts.html#tables","title":"Tables","text":"<p>Tabular data with rows and columns.</p> <p>Processing Options:</p> <ol> <li>Without VLM: Saved as images only</li> <li>With VLM: Converted to Excel/HTML + saved as images</li> </ol> <p>Output: <code>tables.xlsx</code>, <code>tables.html</code>, cropped images</p>"},{"location":"user-guide/core-concepts.html#charts","title":"Charts","text":"<p>Visual representations of data including:</p> <ul> <li>Bar charts</li> <li>Line graphs</li> <li>Pie charts</li> <li>Scatter plots</li> </ul> <p>Processing Options:</p> <ol> <li>Without VLM: Saved as images with captions</li> <li>With VLM: Data extracted + description generated</li> </ol> <p>Output: Cropped images, optional structured data</p>"},{"location":"user-guide/core-concepts.html#figures","title":"Figures","text":"<p>General images and diagrams including:</p> <ul> <li>Photographs</li> <li>Illustrations</li> <li>Diagrams</li> <li>Logos</li> </ul> <p>Processing: Cropped and saved as images with context</p>"},{"location":"user-guide/core-concepts.html#layout-detection","title":"Layout Detection","text":"<p>Layout detection is the foundation of Doctra's processing.</p>"},{"location":"user-guide/core-concepts.html#how-it-works","title":"How It Works","text":"<ol> <li>Page Rendering: PDF pages rendered to images at specified DPI</li> <li>Model Inference: PaddleOCR layout model identifies regions</li> <li>Bounding Boxes: Each element gets coordinates and confidence score</li> <li>Classification: Elements labeled as text/table/chart/figure</li> </ol>"},{"location":"user-guide/core-concepts.html#detection-parameters","title":"Detection Parameters","text":"<pre><code>parser = StructuredPDFParser(\n    layout_model_name=\"PP-DocLayout_plus-L\",  # Model choice\n    dpi=200,  # Image resolution\n    min_score=0.5  # Confidence threshold\n)\n</code></pre> layout_model_name PaddleOCR model to use. Options: <code>PP-DocLayout_plus-L</code> (best), <code>PP-DocLayout_plus-M</code> (faster) dpi Image resolution. Higher = better quality but slower. Range: 100-300 min_score Minimum confidence score (0-1). Higher = fewer false positives"},{"location":"user-guide/core-concepts.html#visualization","title":"Visualization","text":"<p>Verify layout detection quality:</p> <pre><code>parser.display_pages_with_boxes(\n    pdf_path=\"document.pdf\",\n    num_pages=3\n)\n</code></pre> <p>This shows bounding boxes with colors:</p> <ul> <li>\ud83d\udd35 Blue: Text</li> <li>\ud83d\udd34 Red: Tables</li> <li>\ud83d\udfe2 Green: Charts</li> <li>\ud83d\udfe0 Orange: Figures</li> </ul>"},{"location":"user-guide/core-concepts.html#ocr-processing","title":"OCR Processing","text":"<p>OCR (Optical Character Recognition) extracts text from images.</p>"},{"location":"user-guide/core-concepts.html#configuration","title":"Configuration","text":"<pre><code>parser = StructuredPDFParser(\n    ocr_lang=\"eng\",  # Language\n    ocr_psm=6,  # Page segmentation mode\n    ocr_oem=3  # OCR Engine mode\n)\n</code></pre> ocr_lang Tesseract language code. Examples: <code>eng</code>, <code>fra</code>, <code>spa</code>, <code>deu</code> ocr_psm <p>Page segmentation mode. Common values:</p> <ul> <li><code>3</code>: Automatic page segmentation</li> <li><code>6</code>: Uniform block of text (default)</li> <li><code>11</code>: Sparse text</li> <li><code>12</code>: Sparse text with OSD</li> </ul> ocr_oem <p>OCR Engine mode:</p> <ul> <li><code>0</code>: Legacy engine</li> <li><code>1</code>: Neural nets LSTM</li> <li><code>3</code>: Default (both)</li> </ul>"},{"location":"user-guide/core-concepts.html#improving-ocr-accuracy","title":"Improving OCR Accuracy","text":"<ol> <li> <p>Increase DPI: Higher resolution = better text recognition    <pre><code>parser = StructuredPDFParser(dpi=300)\n</code></pre></p> </li> <li> <p>Use Image Restoration: Enhance document quality first    <pre><code>from doctra import EnhancedPDFParser\nparser = EnhancedPDFParser(use_image_restoration=True)\n</code></pre></p> </li> <li> <p>Correct Language: Specify document language    <pre><code>parser = StructuredPDFParser(ocr_lang=\"fra\")  # French\n</code></pre></p> </li> </ol>"},{"location":"user-guide/core-concepts.html#image-restoration","title":"Image Restoration","text":"<p>Image restoration improves document quality before processing.</p>"},{"location":"user-guide/core-concepts.html#restoration-tasks","title":"Restoration Tasks","text":"Task Purpose When to Use <code>appearance</code> General enhancement Most documents (default) <code>dewarping</code> Fix perspective Scanned with distortion <code>deshadowing</code> Remove shadows Poor lighting <code>deblurring</code> Reduce blur Motion blur, focus issues <code>binarization</code> B&amp;W conversion Clean text extraction <code>end2end</code> Full pipeline Severely degraded"},{"location":"user-guide/core-concepts.html#usage","title":"Usage","text":"<pre><code>from doctra import EnhancedPDFParser\n\nparser = EnhancedPDFParser(\n    use_image_restoration=True,\n    restoration_task=\"appearance\",\n    restoration_device=\"cuda\"  # or \"cpu\"\n)\n</code></pre>"},{"location":"user-guide/core-concepts.html#performance-impact","title":"Performance Impact","text":"Restoration Processing Time Quality Improvement None Baseline Baseline CPU +200% +30-50% GPU +50% +30-50%"},{"location":"user-guide/core-concepts.html#vlm-integration","title":"VLM Integration","text":"<p>Vision Language Models convert visual elements to structured data.</p>"},{"location":"user-guide/core-concepts.html#supported-providers","title":"Supported Providers","text":"<ul> <li>OpenAI: GPT-4 Vision, GPT-4o</li> <li>Gemini: Google's vision models</li> <li>Anthropic: Claude with vision</li> <li>OpenRouter: Access multiple models</li> </ul>"},{"location":"user-guide/core-concepts.html#configuration_1","title":"Configuration","text":"<pre><code>parser = StructuredPDFParser(\n    use_vlm=True,\n    vlm_provider=\"openai\",\n    vlm_api_key=\"your-api-key\",\n    vlm_model=\"gpt-4o\"  # Optional, uses default if not specified\n)\n</code></pre>"},{"location":"user-guide/core-concepts.html#what-gets-processed","title":"What Gets Processed","text":"<p>With VLM enabled:</p> Tables Converted to Excel/HTML with cell-by-cell data Charts Data extracted + description generated Figures Descriptions and context generated"},{"location":"user-guide/core-concepts.html#cost-considerations","title":"Cost Considerations","text":"<p>VLM processing requires API calls:</p> <ul> <li>Per Document: 1-10 API calls depending on visual elements</li> <li>Per Element: ~1 API call per table/chart</li> <li>Cost: Varies by provider (typically \\(0.01-\\)0.10 per document)</li> </ul>"},{"location":"user-guide/core-concepts.html#output-formats","title":"Output Formats","text":"<p>Doctra generates multiple output formats simultaneously.</p>"},{"location":"user-guide/core-concepts.html#markdown-md","title":"Markdown (.md)","text":"<p>Human-readable document with:</p> <ul> <li>All text content</li> <li>Embedded images</li> <li>Table references</li> <li>Section structure</li> </ul> <p>Best for: Documentation, reading, version control</p>"},{"location":"user-guide/core-concepts.html#html-html","title":"HTML (.html)","text":"<p>Web-ready document with:</p> <ul> <li>Styled content</li> <li>Interactive tables</li> <li>Image galleries</li> <li>Responsive layout</li> </ul> <p>Best for: Web publishing, presentations</p>"},{"location":"user-guide/core-concepts.html#excel-xlsx","title":"Excel (.xlsx)","text":"<p>Spreadsheet with:</p> <ul> <li>One sheet per table</li> <li>Formatted cells</li> <li>Headers and data</li> </ul> <p>Best for: Data analysis, further processing</p>"},{"location":"user-guide/core-concepts.html#json-json","title":"JSON (.json)","text":"<p>Structured data with:</p> <ul> <li>Element metadata</li> <li>Coordinates</li> <li>Content</li> <li>Relationships</li> </ul> <p>Best for: Programmatic access, integration</p>"},{"location":"user-guide/core-concepts.html#best-practices","title":"Best Practices","text":""},{"location":"user-guide/core-concepts.html#choosing-the-right-parser","title":"Choosing the Right Parser","text":"<pre><code># General documents\nfrom doctra import StructuredPDFParser\nparser = StructuredPDFParser()\n\n# Scanned or low-quality documents\nfrom doctra import EnhancedPDFParser\nparser = EnhancedPDFParser(use_image_restoration=True)\n\n# Only need charts/tables\nfrom doctra import ChartTablePDFParser\nparser = ChartTablePDFParser(extract_charts=True, extract_tables=True)\n</code></pre>"},{"location":"user-guide/core-concepts.html#optimizing-performance","title":"Optimizing Performance","text":"<ol> <li> <p>Use appropriate DPI: Higher isn't always better    <pre><code># Good quality documents\nparser = StructuredPDFParser(dpi=150)\n\n# Low quality documents\nparser = StructuredPDFParser(dpi=250)\n</code></pre></p> </li> <li> <p>Enable GPU when available:    <pre><code>parser = EnhancedPDFParser(restoration_device=\"cuda\")\n</code></pre></p> </li> <li> <p>Batch processing: Reuse parser instances    <pre><code>parser = StructuredPDFParser()\nfor pdf in pdf_files:\n    parser.parse(pdf)\n</code></pre></p> </li> </ol>"},{"location":"user-guide/core-concepts.html#managing-costs","title":"Managing Costs","text":"<p>When using VLM:</p> <ol> <li>Test without VLM first: Ensure layout detection works</li> <li>Process selectively: Use ChartTablePDFParser for specific elements</li> <li>Use cheaper models: Consider Gemini for cost savings</li> </ol>"},{"location":"user-guide/core-concepts.html#next-steps","title":"Next Steps","text":"<ul> <li>Structured Parser - Learn about the base parser</li> <li>Enhanced Parser - Document restoration</li> <li>VLM Integration - Structured data extraction</li> <li>Examples - See it in action</li> </ul>"},{"location":"user-guide/engines/docres-engine.html","title":"DocRes Engine","text":"<p>Guide to using the DocRes image restoration engine.</p>"},{"location":"user-guide/engines/docres-engine.html#overview","title":"Overview","text":"<p>The <code>DocResEngine</code> provides direct access to document image restoration capabilities using the DocRes model. Use it for standalone image enhancement or as part of the parsing pipeline.</p>"},{"location":"user-guide/engines/docres-engine.html#key-features","title":"Key Features","text":"<ul> <li>6 Restoration Tasks: Comprehensive document enhancement</li> <li>GPU Acceleration: CUDA support for faster processing</li> <li>Flexible Input: Images or PDFs</li> <li>Detailed Metadata: Processing information returned</li> </ul>"},{"location":"user-guide/engines/docres-engine.html#basic-usage","title":"Basic Usage","text":"<pre><code>from doctra import DocResEngine\n\n# Initialize engine\nengine = DocResEngine(device=\"cuda\")\n\n# Restore image\nrestored_img, metadata = engine.restore_image(\n    image=\"document.jpg\",\n    task=\"appearance\"\n)\n\n# Save result\nrestored_img.save(\"restored.jpg\")\n</code></pre>"},{"location":"user-guide/engines/docres-engine.html#restoration-tasks","title":"Restoration Tasks","text":"Task Description Use Case <code>appearance</code> General enhancement Most documents <code>dewarping</code> Fix perspective Scanned at angle <code>deshadowing</code> Remove shadows Poor lighting <code>deblurring</code> Reduce blur Motion/focus issues <code>binarization</code> B&amp;W conversion Clean text <code>end2end</code> Full pipeline Severe degradation"},{"location":"user-guide/engines/docres-engine.html#pdf-restoration","title":"PDF Restoration","text":"<pre><code>engine = DocResEngine(device=\"cuda\")\n\nrestored_pdf = engine.restore_pdf(\n    pdf_path=\"low_quality.pdf\",\n    output_path=\"enhanced.pdf\",\n    task=\"appearance\",\n    dpi=300\n)\n</code></pre>"},{"location":"user-guide/engines/docres-engine.html#see-also","title":"See Also","text":"<ul> <li>Enhanced Parser - Integrated restoration</li> <li>API Reference - Complete API documentation</li> <li>Core Concepts - Understanding restoration</li> </ul>"},{"location":"user-guide/engines/layout-detection.html","title":"Layout Detection","text":"<p>Guide to document layout detection in Doctra.</p>"},{"location":"user-guide/engines/layout-detection.html#overview","title":"Overview","text":"<p>Layout detection is the foundation of Doctra's processing pipeline. It analyzes PDF pages to identify and classify different document elements (text, tables, charts, figures).</p>"},{"location":"user-guide/engines/layout-detection.html#how-it-works","title":"How It Works","text":"<ol> <li>Render: PDF pages converted to images at specified DPI</li> <li>Detection: PaddleOCR model identifies element regions</li> <li>Classification: Elements labeled by type</li> <li>Filtering: Low-confidence detections removed</li> </ol>"},{"location":"user-guide/engines/layout-detection.html#configuration","title":"Configuration","text":"<pre><code>from doctra import StructuredPDFParser\n\nparser = StructuredPDFParser(\n    layout_model_name=\"PP-DocLayout_plus-L\",\n    dpi=200,\n    min_score=0.5\n)\n</code></pre>"},{"location":"user-guide/engines/layout-detection.html#parameters","title":"Parameters","text":"layout_model_name PaddleOCR model to use - <code>PP-DocLayout_plus-L</code>: Best accuracy (slower) - <code>PP-DocLayout_plus-M</code>: Faster, good accuracy dpi Image resolution - 100-150: Fast, lower quality - 200: Balanced (default) - 250-300: High quality, slower min_score Confidence threshold (0-1) - 0.0: Include all detections - 0.5: Moderate filtering - 0.7+: Conservative, high confidence only"},{"location":"user-guide/engines/layout-detection.html#visualization","title":"Visualization","text":"<p>Verify detection quality:</p> <pre><code>parser.display_pages_with_boxes(\n    pdf_path=\"document.pdf\",\n    num_pages=3\n)\n</code></pre>"},{"location":"user-guide/engines/layout-detection.html#element-types","title":"Element Types","text":"<ul> <li>Text: Regular content (blue boxes)</li> <li>Tables: Tabular data (red boxes)</li> <li>Charts: Graphs and plots (green boxes)</li> <li>Figures: Images and diagrams (orange boxes)</li> </ul>"},{"location":"user-guide/engines/layout-detection.html#see-also","title":"See Also","text":"<ul> <li>Core Concepts - Understanding the pipeline</li> <li>Visualization - Layout visualization</li> <li>API Reference - Configuration options</li> </ul>"},{"location":"user-guide/engines/ocr-engine.html","title":"OCR Engine","text":"<p>Guide to text extraction using OCR in Doctra.</p>"},{"location":"user-guide/engines/ocr-engine.html#overview","title":"Overview","text":"<p>Doctra uses Tesseract OCR to extract text from document images. The OCR engine is highly configurable for different document types and languages.</p>"},{"location":"user-guide/engines/ocr-engine.html#configuration","title":"Configuration","text":"<pre><code>from doctra import StructuredPDFParser\n\nparser = StructuredPDFParser(\n    ocr_lang=\"eng\",\n    ocr_psm=6,\n    ocr_oem=3\n)\n</code></pre>"},{"location":"user-guide/engines/ocr-engine.html#parameters","title":"Parameters","text":"ocr_lang Tesseract language code - <code>eng</code>: English - <code>fra</code>: French - <code>spa</code>: Spanish - <code>deu</code>: German - Multiple: <code>eng+fra</code> ocr_psm Page segmentation mode - <code>3</code>: Automatic - <code>6</code>: Uniform block (default) - <code>11</code>: Sparse text - <code>12</code>: Sparse with OSD ocr_oem OCR engine mode - <code>0</code>: Legacy - <code>1</code>: Neural nets LSTM - <code>3</code>: Default (both)"},{"location":"user-guide/engines/ocr-engine.html#improving-accuracy","title":"Improving Accuracy","text":""},{"location":"user-guide/engines/ocr-engine.html#1-increase-dpi","title":"1. Increase DPI","text":"<pre><code>parser = StructuredPDFParser(dpi=300)\n</code></pre>"},{"location":"user-guide/engines/ocr-engine.html#2-use-image-restoration","title":"2. Use Image Restoration","text":"<pre><code>from doctra import EnhancedPDFParser\n\nparser = EnhancedPDFParser(\n    use_image_restoration=True\n)\n</code></pre>"},{"location":"user-guide/engines/ocr-engine.html#3-correct-language","title":"3. Correct Language","text":"<pre><code>parser = StructuredPDFParser(\n    ocr_lang=\"fra\"  # For French documents\n)\n</code></pre>"},{"location":"user-guide/engines/ocr-engine.html#multi-language-documents","title":"Multi-language Documents","text":"<pre><code>parser = StructuredPDFParser(\n    ocr_lang=\"eng+fra+deu\"  # Multiple languages\n)\n</code></pre>"},{"location":"user-guide/engines/ocr-engine.html#see-also","title":"See Also","text":"<ul> <li>Enhanced Parser - Improve OCR with restoration</li> <li>Core Concepts - Understanding OCR in the pipeline</li> <li>API Reference - OCR configuration options</li> </ul>"},{"location":"user-guide/engines/vlm-integration.html","title":"VLM Integration","text":"<p>Guide to using Vision Language Models with Doctra.</p>"},{"location":"user-guide/engines/vlm-integration.html#overview","title":"Overview","text":"<p>Doctra integrates with Vision Language Models (VLMs) to convert visual elements (charts, tables, figures) into structured data. This enables automatic data extraction and conversion to Excel, HTML, and JSON formats.</p>"},{"location":"user-guide/engines/vlm-integration.html#supported-providers","title":"Supported Providers","text":"<ul> <li>OpenAI: GPT-4 Vision, GPT-4o</li> <li>Gemini: Google's vision models</li> <li>Anthropic: Claude with vision</li> <li>OpenRouter: Access multiple models</li> <li>Qianfan: Baidu AI Cloud ERNIE models</li> </ul>"},{"location":"user-guide/engines/vlm-integration.html#basic-configuration","title":"Basic Configuration","text":"<pre><code>from doctra import StructuredPDFParser\n\nparser = StructuredPDFParser(\n    use_vlm=True,\n    vlm_provider=\"openai\",\n    vlm_api_key=\"your-api-key\"\n)\n\nparser.parse(\"document.pdf\")\n</code></pre>"},{"location":"user-guide/engines/vlm-integration.html#provider-setup","title":"Provider Setup","text":""},{"location":"user-guide/engines/vlm-integration.html#openai","title":"OpenAI","text":"<pre><code>parser = StructuredPDFParser(\n    use_vlm=True,\n    vlm_provider=\"openai\",\n    vlm_api_key=\"sk-xxx\",\n    vlm_model=\"gpt-4o\"  # Optional\n)\n</code></pre>"},{"location":"user-guide/engines/vlm-integration.html#gemini","title":"Gemini","text":"<pre><code>parser = StructuredPDFParser(\n    use_vlm=True,\n    vlm_provider=\"gemini\",\n    vlm_api_key=\"your-gemini-key\"\n)\n</code></pre>"},{"location":"user-guide/engines/vlm-integration.html#anthropic","title":"Anthropic","text":"<pre><code>parser = StructuredPDFParser(\n    use_vlm=True,\n    vlm_provider=\"anthropic\",\n    vlm_api_key=\"your-anthropic-key\"\n)\n</code></pre>"},{"location":"user-guide/engines/vlm-integration.html#qianfan-baidu-ai-cloud","title":"Qianfan (Baidu AI Cloud)","text":"<pre><code>parser = StructuredPDFParser(\n    use_vlm=True,\n    vlm_provider=\"qianfan\",\n    vlm_api_key=\"your-qianfan-key\",\n    vlm_model=\"ernie-4.5-turbo-vl-32k\"  # Optional, defaults to ernie-4.5-turbo-vl-32k\n)\n</code></pre> <p>Available ERNIE Models: - <code>ernie-4.5-turbo-vl-32k</code> (default) - vision model with 32k context</p>"},{"location":"user-guide/engines/vlm-integration.html#what-gets-processed","title":"What Gets Processed","text":"<p>With VLM enabled:</p> <ul> <li>Tables: Converted to Excel/HTML with cell data</li> <li>Charts: Data points extracted + descriptions</li> <li>Figures: Descriptions and context generated</li> </ul>"},{"location":"user-guide/engines/vlm-integration.html#output-files","title":"Output Files","text":"<pre><code>outputs/\n\u2514\u2500\u2500 document/\n    \u2514\u2500\u2500 full_parse/\n        \u251c\u2500\u2500 tables.xlsx      # Extracted table data\n        \u251c\u2500\u2500 tables.html      # HTML tables\n        \u251c\u2500\u2500 vlm_items.json   # Structured data\n        \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"user-guide/engines/vlm-integration.html#cost-considerations","title":"Cost Considerations","text":"<p>VLM processing requires API calls:</p> <ul> <li>~1-10 calls per document</li> <li>~\\(0.01-\\)0.10 per document</li> <li>Costs vary by provider</li> </ul>"},{"location":"user-guide/engines/vlm-integration.html#see-also","title":"See Also","text":"<ul> <li>Parsers - Using VLM with parsers</li> <li>API Reference - VLM configuration options</li> <li>Examples - VLM usage examples</li> </ul>"},{"location":"user-guide/outputs/export-formats.html","title":"Export Formats","text":"<p>Guide to Doctra's output formats.</p>"},{"location":"user-guide/outputs/export-formats.html#overview","title":"Overview","text":"<p>Doctra generates multiple output formats simultaneously, each optimized for different use cases.</p>"},{"location":"user-guide/outputs/export-formats.html#available-formats","title":"Available Formats","text":""},{"location":"user-guide/outputs/export-formats.html#markdown-md","title":"Markdown (.md)","text":"<p>Human-readable document with:</p> <ul> <li>All text content</li> <li>Embedded image references</li> <li>Table links</li> <li>Section structure</li> </ul> <p>Best for: Documentation, version control, reading</p> <p>Example: <pre><code># Document Title\n\n## Section 1\n\nText content...\n\n![Figure 1](images/figures/figure_001.jpg)\n\nSee tables in [tables.xlsx](tables.xlsx)\n</code></pre></p>"},{"location":"user-guide/outputs/export-formats.html#html-html","title":"HTML (.html)","text":"<p>Web-ready document with:</p> <ul> <li>Styled content</li> <li>Embedded images</li> <li>Interactive tables</li> <li>Responsive layout</li> </ul> <p>Best for: Web publishing, presentations</p>"},{"location":"user-guide/outputs/export-formats.html#excel-xlsx","title":"Excel (.xlsx)","text":"<p>Spreadsheet with extracted data:</p> <ul> <li>One sheet per table</li> <li>Formatted cells</li> <li>Headers preserved</li> <li>Data structured</li> </ul> <p>Best for: Data analysis, further processing</p> <p>Only generated when VLM is enabled</p>"},{"location":"user-guide/outputs/export-formats.html#json-json","title":"JSON (.json)","text":"<p>Structured data with:</p> <ul> <li>Element metadata</li> <li>Coordinates</li> <li>Content</li> <li>Relationships</li> </ul> <p>Best for: Programmatic access, integration</p> <p>Only generated when VLM is enabled</p>"},{"location":"user-guide/outputs/export-formats.html#images","title":"Images","text":"<p>Cropped visual elements:</p> <ul> <li><code>figures/</code>: Document images</li> <li><code>charts/</code>: Graphs and plots</li> <li><code>tables/</code>: Table images</li> </ul> <p>Format: JPEG or PNG Best for: Direct use, presentations</p>"},{"location":"user-guide/outputs/export-formats.html#output-structure","title":"Output Structure","text":"<pre><code>outputs/\n\u2514\u2500\u2500 document/\n    \u2514\u2500\u2500 full_parse/\n        \u251c\u2500\u2500 result.md          # Markdown\n        \u251c\u2500\u2500 result.html        # HTML\n        \u251c\u2500\u2500 tables.xlsx        # Excel (VLM)\n        \u251c\u2500\u2500 tables.html        # HTML tables (VLM)\n        \u251c\u2500\u2500 vlm_items.json     # JSON data (VLM)\n        \u2514\u2500\u2500 images/\n            \u251c\u2500\u2500 figures/\n            \u251c\u2500\u2500 charts/\n            \u2514\u2500\u2500 tables/\n</code></pre>"},{"location":"user-guide/outputs/export-formats.html#choosing-formats","title":"Choosing Formats","text":"Use Case Recommended Format Reading Markdown or HTML Data analysis Excel Web publishing HTML Integration JSON Presentations Images + HTML Version control Markdown"},{"location":"user-guide/outputs/export-formats.html#see-also","title":"See Also","text":"<ul> <li>Visualization - Visual outputs</li> <li>Examples - Usage examples</li> <li>API Reference - Exporter documentation</li> </ul>"},{"location":"user-guide/outputs/visualization.html","title":"Visualization","text":"<p>Guide to visualizing Doctra's processing results.</p>"},{"location":"user-guide/outputs/visualization.html#overview","title":"Overview","text":"<p>Doctra provides visualization tools to help you understand and verify document processing results.</p>"},{"location":"user-guide/outputs/visualization.html#layout-visualization","title":"Layout Visualization","text":"<p>Display detected document elements with bounding boxes:</p> <pre><code>from doctra import StructuredPDFParser\n\nparser = StructuredPDFParser()\n\nparser.display_pages_with_boxes(\n    pdf_path=\"document.pdf\",\n    num_pages=3\n)\n</code></pre>"},{"location":"user-guide/outputs/visualization.html#features","title":"Features","text":"<ul> <li>Color-coded Elements: Each type has a distinct color</li> <li>Confidence Scores: Shows detection confidence</li> <li>Grid Layout: Multiple pages in organized grid</li> <li>Element Counts: Summary statistics per page</li> </ul>"},{"location":"user-guide/outputs/visualization.html#color-scheme","title":"Color Scheme","text":"<ul> <li>\ud83d\udd35 Blue: Text regions</li> <li>\ud83d\udd34 Red: Tables</li> <li>\ud83d\udfe2 Green: Charts</li> <li>\ud83d\udfe0 Orange: Figures</li> </ul>"},{"location":"user-guide/outputs/visualization.html#configuration","title":"Configuration","text":"<pre><code>parser.display_pages_with_boxes(\n    pdf_path=\"document.pdf\",\n    num_pages=5,        # Pages to visualize\n    cols=3,             # Grid columns\n    page_width=700,     # Page width in pixels\n    spacing=40,         # Spacing between pages\n    save_path=\"viz.png\" # Save instead of display\n)\n</code></pre>"},{"location":"user-guide/outputs/visualization.html#use-cases","title":"Use Cases","text":"<ol> <li>Quality Assurance: Verify detection accuracy</li> <li>Debugging: Identify layout issues</li> <li>Documentation: Create visual reports</li> <li>Analysis: Understand document structure</li> </ol>"},{"location":"user-guide/outputs/visualization.html#cli-visualization","title":"CLI Visualization","text":"<pre><code>doctra visualize document.pdf --num-pages 5 --output layout.png\n</code></pre>"},{"location":"user-guide/outputs/visualization.html#see-also","title":"See Also","text":"<ul> <li>Layout Detection - Understanding detection</li> <li>Core Concepts - Processing pipeline</li> <li>CLI Reference - Command line tools</li> </ul>"},{"location":"user-guide/parsers/chart-table-extractor.html","title":"Chart &amp; Table Extractor","text":"<p>Guide to using the <code>ChartTablePDFParser</code> for targeted extraction.</p>"},{"location":"user-guide/parsers/chart-table-extractor.html#overview","title":"Overview","text":"<p>The <code>ChartTablePDFParser</code> is a specialized parser focused exclusively on extracting charts and tables from PDF documents. It's optimized for scenarios where you only need these specific elements.</p>"},{"location":"user-guide/parsers/chart-table-extractor.html#key-features","title":"Key Features","text":"<ul> <li>Focused Extraction: Extract only charts and/or tables</li> <li>Selective Processing: Choose what to extract</li> <li>VLM Integration: Convert visuals to structured data</li> <li>Faster Processing: Skips unnecessary elements</li> </ul>"},{"location":"user-guide/parsers/chart-table-extractor.html#basic-usage","title":"Basic Usage","text":"<pre><code>from doctra import ChartTablePDFParser\n\nparser = ChartTablePDFParser(\n    extract_charts=True,\n    extract_tables=True\n)\n\nparser.parse(\"data_report.pdf\")\n</code></pre>"},{"location":"user-guide/parsers/chart-table-extractor.html#selective-extraction","title":"Selective Extraction","text":"<pre><code># Extract only tables\nparser = ChartTablePDFParser(\n    extract_charts=False,\n    extract_tables=True\n)\n\n# Extract only charts\nparser = ChartTablePDFParser(\n    extract_charts=True,\n    extract_tables=False\n)\n</code></pre>"},{"location":"user-guide/parsers/chart-table-extractor.html#with-vlm-for-structured-data","title":"With VLM for Structured Data","text":"<pre><code>parser = ChartTablePDFParser(\n    extract_charts=True,\n    extract_tables=True,\n    use_vlm=True,\n    vlm_provider=\"openai\",\n    vlm_api_key=\"your-key\"\n)\n\nparser.parse(\"report.pdf\")\n# Outputs: tables.xlsx, tables.html, vlm_items.json\n</code></pre>"},{"location":"user-guide/parsers/chart-table-extractor.html#when-to-use","title":"When to Use","text":"<p>Use <code>ChartTablePDFParser</code> when:</p> <ul> <li>You only need charts and/or tables</li> <li>Faster processing is important</li> <li>Working with data-heavy documents</li> <li>Extracting data for analysis</li> </ul>"},{"location":"user-guide/parsers/chart-table-extractor.html#see-also","title":"See Also","text":"<ul> <li>VLM Integration - Structured data extraction</li> <li>Structured Parser - Full document parsing</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"user-guide/parsers/enhanced-parser.html","title":"Enhanced PDF Parser","text":"<p>Guide to using the <code>EnhancedPDFParser</code> with image restoration.</p>"},{"location":"user-guide/parsers/enhanced-parser.html#overview","title":"Overview","text":"<p>The <code>EnhancedPDFParser</code> extends <code>StructuredPDFParser</code> with DocRes image restoration capabilities. It's ideal for processing scanned documents, low-quality PDFs, or documents with visual distortions.</p>"},{"location":"user-guide/parsers/enhanced-parser.html#key-features","title":"Key Features","text":"<ul> <li>Image Restoration: DocRes integration for document enhancement</li> <li>6 Restoration Tasks: Dewarping, deshadowing, deblurring, and more</li> <li>GPU Acceleration: Optional CUDA support for faster processing</li> <li>All Base Features: Inherits all <code>StructuredPDFParser</code> capabilities</li> </ul>"},{"location":"user-guide/parsers/enhanced-parser.html#basic-usage","title":"Basic Usage","text":"<pre><code>from doctra import EnhancedPDFParser\n\nparser = EnhancedPDFParser(\n    use_image_restoration=True,\n    restoration_task=\"appearance\"\n)\n\nparser.parse(\"scanned_document.pdf\")\n</code></pre>"},{"location":"user-guide/parsers/enhanced-parser.html#restoration-tasks","title":"Restoration Tasks","text":"Task Best For <code>appearance</code> General enhancement (default) <code>dewarping</code> Perspective distortion <code>deshadowing</code> Shadow removal <code>deblurring</code> Blur reduction <code>binarization</code> Clean B&amp;W conversion <code>end2end</code> Severe degradation"},{"location":"user-guide/parsers/enhanced-parser.html#when-to-use","title":"When to Use","text":"<p>Use <code>EnhancedPDFParser</code> for:</p> <ul> <li>Scanned documents</li> <li>Low-quality PDFs</li> <li>Documents with visual distortions</li> <li>When OCR accuracy is poor with standard parser</li> </ul>"},{"location":"user-guide/parsers/enhanced-parser.html#see-also","title":"See Also","text":"<ul> <li>DocRes Engine - Image restoration details</li> <li>Structured Parser - Base parser</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"user-guide/parsers/structured-parser.html","title":"Structured PDF Parser","text":"<p>Comprehensive guide to using the <code>StructuredPDFParser</code>.</p>"},{"location":"user-guide/parsers/structured-parser.html#overview","title":"Overview","text":"<p>The <code>StructuredPDFParser</code> is the foundational parser in Doctra, designed for general-purpose PDF document processing. It combines layout detection, OCR, and optional VLM integration to extract all content from PDF documents.</p>"},{"location":"user-guide/parsers/structured-parser.html#key-features","title":"Key Features","text":"<ul> <li>Layout Detection: PaddleOCR-based document structure analysis</li> <li>OCR Processing: Text extraction from all document elements</li> <li>Visual Element Extraction: Automatic cropping of figures, charts, and tables</li> <li>VLM Integration: Optional structured data extraction</li> <li>Multiple Output Formats: Markdown, HTML, Excel, JSON</li> </ul>"},{"location":"user-guide/parsers/structured-parser.html#basic-usage","title":"Basic Usage","text":"<pre><code>from doctra import StructuredPDFParser\n\n# Initialize parser with defaults\nparser = StructuredPDFParser()\n\n# Parse document\nparser.parse(\"document.pdf\")\n</code></pre>"},{"location":"user-guide/parsers/structured-parser.html#configuration","title":"Configuration","text":"<p>See API Reference for detailed parameter documentation.</p>"},{"location":"user-guide/parsers/structured-parser.html#output-structure","title":"Output Structure","text":"<pre><code>outputs/\n\u2514\u2500\u2500 document/\n    \u2514\u2500\u2500 full_parse/\n        \u251c\u2500\u2500 result.md\n        \u251c\u2500\u2500 result.html\n        \u2514\u2500\u2500 images/\n</code></pre>"},{"location":"user-guide/parsers/structured-parser.html#when-to-use","title":"When to Use","text":"<p>Use <code>StructuredPDFParser</code> for:</p> <ul> <li>General PDF processing</li> <li>Good quality documents</li> <li>When image restoration is not needed</li> <li>Extracting all content types</li> </ul>"},{"location":"user-guide/parsers/structured-parser.html#see-also","title":"See Also","text":"<ul> <li>Enhanced Parser - With image restoration</li> <li>Chart &amp; Table Extractor - Focused extraction</li> <li>API Reference - Complete API documentation</li> </ul>"}]}
{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Welcome to Doctra","text":""},{"location":"index.html#overview","title":"Overview","text":"<p>Doctra is a powerful Python library for parsing, extracting, and analyzing document content from PDFs. It combines state-of-the-art layout detection, OCR, image restoration, and Vision Language Models (VLM) to provide comprehensive document processing capabilities.</p>"},{"location":"index.html#key-features","title":"Key Features","text":""},{"location":"index.html#comprehensive-pdf-parsing","title":"Comprehensive PDF Parsing","text":"<ul> <li>Layout Detection: Advanced document layout analysis using PaddleOCR</li> <li>OCR Processing: High-quality text extraction with Tesseract</li> <li>Visual Elements: Automatic extraction of figures, charts, and tables</li> <li>Multiple Parsers: Choose the right parser for your use case</li> </ul>"},{"location":"index.html#image-restoration","title":"Image Restoration","text":"<ul> <li>6 Restoration Tasks: Dewarping, deshadowing, appearance enhancement, deblurring, binarization, and end-to-end restoration</li> <li>DocRes Integration: State-of-the-art document image restoration</li> <li>GPU Acceleration: Automatic CUDA detection for faster processing</li> <li>Enhanced Quality: Improves document quality for better OCR results</li> </ul>"},{"location":"index.html#vlm-integration","title":"VLM Integration","text":"<ul> <li>Structured Data Extraction: Convert charts and tables to structured formats</li> <li>Multiple Providers: OpenAI, Gemini, Anthropic, and OpenRouter support</li> <li>Automatic Conversion: Transform visual elements into usable data</li> <li>Flexible Configuration: Easy API key management and model selection</li> </ul>"},{"location":"index.html#rich-output-formats","title":"Rich Output Formats","text":"<ul> <li>Markdown: Human-readable documents with embedded images</li> <li>Excel: Structured data in spreadsheet format</li> <li>JSON: Programmatically accessible data</li> <li>HTML: Interactive web-ready documents</li> <li>Images: High-quality cropped visual elements</li> </ul>"},{"location":"index.html#user-friendly-interfaces","title":"User-Friendly Interfaces","text":"<ul> <li>Web UI: Gradio-based interface with drag &amp; drop</li> <li>Command Line: Powerful CLI for automation</li> <li>Python API: Full programmatic access</li> <li>Real-time Progress: Track processing status</li> </ul>"},{"location":"index.html#quick-start","title":"Quick Start","text":""},{"location":"index.html#installation","title":"Installation","text":"<pre><code>pip install doctra\n</code></pre>"},{"location":"index.html#basic-usage","title":"Basic Usage","text":"<pre><code>from doctra import StructuredPDFParser\n\n# Initialize parser\nparser = StructuredPDFParser()\n\n# Parse a document\nparser.parse(\"document.pdf\")\n</code></pre> <p>System Dependencies</p> <p>Doctra requires Poppler for PDF processing. See the Installation Guide for detailed setup instructions.</p>"},{"location":"index.html#core-components","title":"Core Components","text":""},{"location":"index.html#parsers","title":"Parsers","text":"Parser Description Best For StructuredPDFParser Complete document processing General purpose parsing EnhancedPDFParser Parsing with image restoration Scanned or low-quality documents ChartTablePDFParser Focused extraction Only charts and tables needed"},{"location":"index.html#engines","title":"Engines","text":"Engine Description Use Case DocResEngine Image restoration Standalone image enhancement Layout Detection Document analysis Identify document structure OCR Engine Text extraction Extract text from images VLM Service AI processing Convert visuals to structured data"},{"location":"index.html#use-cases","title":"Use Cases","text":"<ul> <li> Financial Reports: Extract tables, charts, and text from financial documents</li> <li> Research Papers: Parse academic papers with figures and tables</li> <li> Document Archival: Convert scanned documents to searchable formats</li> <li> Data Extraction: Extract structured data from visual elements</li> <li> Document Enhancement: Restore and improve low-quality documents</li> </ul>"},{"location":"index.html#getting-help","title":"Getting Help","text":"<ul> <li> Documentation: You're reading it! Explore the sidebar for detailed guides</li> <li> GitHub Issues: Report bugs or request features</li> <li> PyPI: View package details</li> </ul>"},{"location":"index.html#interactive-notebooks","title":"\ud83d\udcd3 Interactive Notebooks","text":"Notebook Colab Badge Description 01_doctra_quick_start Comprehensive tutorial covering layout detection, content extraction, and multi-format outputs with visual examples"},{"location":"index.html#whats-next","title":"What's Next?","text":"<ul> <li> <p> Quick Start</p> <p>Get up and running with Doctra in minutes</p> <p> Quick Start Guide</p> </li> <li> <p> User Guide</p> <p>Learn about parsers, engines, and advanced features</p> <p> Read the Guide</p> </li> <li> <p> API Reference</p> <p>Detailed API documentation for all components</p> <p> API Docs</p> </li> <li> <p> Examples</p> <p>Real-world examples and integration patterns</p> <p> View Examples</p> </li> </ul>"},{"location":"index.html#acknowledgments","title":"Acknowledgments","text":"<p>Doctra builds upon several excellent open-source projects:</p> <ul> <li>PaddleOCR - Advanced document layout detection and OCR capabilities</li> <li>DocRes - State-of-the-art document image restoration model</li> <li>Outlines - Structured output generation for LLMs</li> </ul> <p>We thank the developers and contributors of these projects for their valuable work.</p>"},{"location":"index.html#license","title":"License","text":"<p>Doctra is released under the MIT License. See the LICENSE file for details.</p>"},{"location":"changelog.html","title":"Changelog","text":"<p>All notable changes to Doctra will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog.html#043-2024-xx-xx","title":"[0.4.3] - 2024-XX-XX","text":""},{"location":"changelog.html#current-release","title":"Current Release","text":"<p>This is the current stable release of Doctra.</p>"},{"location":"changelog.html#features","title":"Features","text":"<ul> <li> <p>Multiple PDF Parsers</p> <ul> <li><code>StructuredPDFParser</code>: Complete document processing</li> <li><code>EnhancedPDFParser</code>: Parsing with image restoration</li> <li><code>ChartTablePDFParser</code>: Specialized chart/table extraction</li> </ul> </li> <li> <p>Image Restoration</p> <ul> <li>DocRes integration for document enhancement</li> <li>6 restoration tasks: appearance, dewarping, deshadowing, deblurring, binarization, end2end</li> <li>GPU acceleration support</li> </ul> </li> <li> <p>VLM Integration</p> <ul> <li>Support for OpenAI, Gemini, Anthropic, OpenRouter, Qianfan, and Ollama</li> <li>Structured data extraction from charts and tables</li> <li>Automatic conversion to Excel/HTML/JSON</li> </ul> </li> <li> <p>Output Formats</p> <ul> <li>Markdown with embedded images</li> <li>HTML for web viewing</li> <li>Excel for data analysis</li> <li>JSON for programmatic access</li> <li>High-quality image extraction</li> </ul> </li> <li> <p>User Interfaces</p> <ul> <li>Gradio-based web UI</li> <li>Comprehensive CLI</li> <li>Full Python API</li> </ul> </li> <li> <p>Visualization</p> <ul> <li>Layout detection visualization</li> <li>Bounding box overlays</li> <li>Confidence scores</li> <li>Multi-page grid display</li> </ul> </li> </ul>"},{"location":"changelog.html#dependencies","title":"Dependencies","text":"<ul> <li>Python 3.8+</li> <li>PaddlePaddle &gt;= 2.4.0</li> <li>PaddleOCR &gt;= 2.6.0</li> <li>Pillow &gt;= 8.0.0</li> <li>OpenCV &gt;= 4.5.0</li> <li>Pandas &gt;= 1.3.0</li> <li>Tesseract &gt;= 0.1.3</li> <li>PyTesseract &gt;= 0.3.10</li> <li>pdf2image &gt;= 1.16.0</li> <li>Anthropic &gt;= 0.40.0</li> <li>Outlines &gt;= 0.0.34</li> </ul>"},{"location":"changelog.html#unreleased","title":"[Unreleased]","text":""},{"location":"changelog.html#planned-features","title":"Planned Features","text":"<ul> <li> Support for additional document formats (DOCX, PPTX)</li> <li> Improved table structure recognition</li> <li> Batch processing API</li> <li> Docker container</li> <li> Cloud deployment guides</li> <li> Additional VLM providers</li> <li> Performance optimizations</li> <li> Multilingual documentation</li> </ul>"},{"location":"changelog.html#version-history","title":"Version History","text":""},{"location":"changelog.html#043-current","title":"[0.4.3] - Current","text":"<p>Current stable release with full feature set.</p>"},{"location":"changelog.html#040-previous","title":"[0.4.0] - Previous","text":"<p>Initial public release with core features.</p>"},{"location":"changelog.html#upgrade-guide","title":"Upgrade Guide","text":""},{"location":"changelog.html#from-040-to-043","title":"From 0.4.0 to 0.4.3","text":"<p>No breaking changes. Simply upgrade:</p> <pre><code>pip install --upgrade doctra\n</code></pre>"},{"location":"changelog.html#contributing","title":"Contributing","text":"<p>See our Contributing Guide for information on:</p> <ul> <li>Reporting bugs</li> <li>Requesting features</li> <li>Submitting pull requests</li> <li>Development setup</li> </ul>"},{"location":"changelog.html#support","title":"Support","text":"<ul> <li>Documentation: https://ademboukhris457.github.io/Doctra/</li> <li>GitHub Issues: https://github.com/AdemBoukhris457/Doctra/issues</li> <li>PyPI: https://pypi.org/project/doctra/</li> </ul>"},{"location":"api/engines.html","title":"Engines API Reference","text":"<p>Complete API documentation for Doctra engines.</p>"},{"location":"api/engines.html#docresengine","title":"DocResEngine","text":"<p>Image restoration engine for document enhancement.</p>"},{"location":"api/engines.html#doctra.engines.image_restoration.DocResEngine","title":"<code>doctra.engines.image_restoration.DocResEngine</code>","text":"<p>DocRes Image Restoration Engine</p> <p>A wrapper around DocRes inference functionality for easy integration with Doctra's document processing pipeline.</p> Source code in <code>doctra/engines/image_restoration/docres_engine.py</code> <pre><code>class DocResEngine:\n    \"\"\"\n    DocRes Image Restoration Engine\n\n    A wrapper around DocRes inference functionality for easy integration\n    with Doctra's document processing pipeline.\n    \"\"\"\n\n    SUPPORTED_TASKS = [\n        'dewarping', 'deshadowing', 'appearance', \n        'deblurring', 'binarization', 'end2end'\n    ]\n\n    def __init__(\n        self, \n        device: Optional[str] = None,\n        use_half_precision: bool = True,\n        model_path: Optional[str] = None,\n        mbd_path: Optional[str] = None\n    ):\n        \"\"\"\n        Initialize DocRes Engine\n\n        Args:\n            device: Device to run on ('cuda', 'cpu', or None for auto-detect)\n            use_half_precision: Whether to use half precision for inference\n            model_path: Path to DocRes model checkpoint (optional, defaults to Hugging Face Hub)\n            mbd_path: Path to MBD model checkpoint (optional, defaults to Hugging Face Hub)\n        \"\"\"\n        if not DOCRES_AVAILABLE:\n            raise ImportError(\n                \"DocRes is not available. Please install the missing dependencies:\\n\"\n                \"pip install scikit-image&gt;=0.19.3\\n\\n\"\n                \"The DocRes module is already included in this library, but requires \"\n                \"scikit-image for image processing operations.\"\n            )\n\n        # Set device\n        if device is None:\n            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        else:\n            requested_device = torch.device(device)\n            # Check if the requested device is available\n            if requested_device.type == 'cuda' and not torch.cuda.is_available():\n                print(f\"Warning: CUDA requested but not available. Falling back to CPU.\")\n                self.device = torch.device('cpu')\n            else:\n                self.device = requested_device\n\n        self.use_half_precision = use_half_precision\n\n        # Get model paths (always from Hugging Face Hub)\n        try:\n            self.mbd_path, self.model_path = get_model_paths(\n                use_huggingface=True,\n                model_path=model_path,\n                mbd_path=mbd_path\n            )\n        except Exception as e:\n            raise RuntimeError(f\"Failed to get model paths: {e}\")\n\n        # Verify model files exist\n        if not os.path.exists(self.model_path):\n            raise FileNotFoundError(\n                f\"DocRes model not found at {self.model_path}. \"\n                f\"This may indicate a Hugging Face download failure. \"\n                f\"Please check your internet connection and try again.\"\n            )\n\n        if not os.path.exists(self.mbd_path):\n            raise FileNotFoundError(\n                f\"MBD model not found at {self.mbd_path}. \"\n                f\"This may indicate a Hugging Face download failure. \"\n                f\"Please check your internet connection and try again.\"\n            )\n\n        # Initialize model\n        self._model = None\n        self._initialize_model()\n\n    def _initialize_model(self):\n        \"\"\"Initialize the DocRes model\"\"\"\n        try:\n            # Create model architecture\n            self._model = restormer_arch.Restormer( \n                inp_channels=6, \n                out_channels=3, \n                dim=48,\n                num_blocks=[2,3,3,4], \n                num_refinement_blocks=4,\n                heads=[1,2,4,8],\n                ffn_expansion_factor=2.66,\n                bias=False,\n                LayerNorm_type='WithBias',\n                dual_pixel_task=True        \n            )\n\n            # Load model weights - always load to CPU first, then move to target device\n            state = convert_state_dict(torch.load(self.model_path, map_location='cpu')['model_state'])\n\n            self._model.load_state_dict(state)\n            self._model.eval()\n            self._model = self._model.to(self.device)\n\n        except Exception as e:\n            raise RuntimeError(f\"Failed to initialize DocRes model: {e}\")\n\n    def restore_image(\n        self, \n        image: Union[str, np.ndarray], \n        task: str = \"appearance\",\n        save_prompts: bool = False\n    ) -&gt; Tuple[np.ndarray, Dict[str, Any]]:\n        \"\"\"\n        Restore a single image using DocRes\n\n        Args:\n            image: Path to image file or numpy array\n            task: Restoration task to perform\n            save_prompts: Whether to save intermediate prompts\n\n        Returns:\n            Tuple of (restored_image, metadata)\n        \"\"\"\n        if task not in self.SUPPORTED_TASKS:\n            raise ValueError(f\"Unsupported task: {task}. Supported tasks: {self.SUPPORTED_TASKS}\")\n\n        # Load image if path provided\n        if isinstance(image, str):\n            if not os.path.exists(image):\n                raise FileNotFoundError(f\"Image not found: {image}\")\n            img_array = cv2.imread(image)\n            if img_array is None:\n                raise ValueError(f\"Could not load image: {image}\")\n        else:\n            img_array = image.copy()\n\n        original_shape = img_array.shape\n\n        try:\n            # Handle end2end pipeline\n            if task == \"end2end\":\n                return self._run_end2end_pipeline(img_array, save_prompts)\n\n            # Run single task\n            restored_img, metadata = self._run_single_task(img_array, task, save_prompts)\n\n            metadata.update({\n                'original_shape': original_shape,\n                'restored_shape': restored_img.shape,\n                'task': task,\n                'device': str(self.device)\n            })\n\n            return restored_img, metadata\n\n        except Exception as e:\n            raise RuntimeError(f\"Image restoration failed: {e}\")\n\n    def _run_single_task(self, img_array: np.ndarray, task: str, save_prompts: bool) -&gt; Tuple[np.ndarray, Dict]:\n        \"\"\"Run a single restoration task\"\"\"\n\n        # Create temporary file for inference\n        with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as tmp_file:\n            tmp_path = tmp_file.name\n            cv2.imwrite(tmp_path, img_array)\n\n        try:\n            # Change to DocRes directory for inference to work properly\n            original_cwd = os.getcwd()\n            os.chdir(str(docres_dir))\n\n            # Set global DEVICE variable that DocRes inference expects\n            import inference  # Import the inference module to set its global DEVICE\n            inference.DEVICE = self.device\n\n            try:\n                # Run inference\n                prompt1, prompt2, prompt3, restored = inference_one_im(self._model, tmp_path, task)\n            finally:\n                # Always restore original working directory\n                os.chdir(original_cwd)\n\n            metadata = {\n                'task': task,\n                'device': str(self.device)\n            }\n\n            if save_prompts:\n                metadata['prompts'] = {\n                    'prompt1': prompt1,\n                    'prompt2': prompt2, \n                    'prompt3': prompt3\n                }\n\n            return restored, metadata\n\n        finally:\n            # Clean up temporary file with retry for Windows\n            try:\n                # Wait a bit for file handles to be released\n                time.sleep(0.1)\n                os.unlink(tmp_path)\n            except PermissionError:\n                # If still locked, try again after a longer wait\n                time.sleep(1)\n                try:\n                    os.unlink(tmp_path)\n                except PermissionError:\n                    # If still failing, just leave it - it will be cleaned up by the OS\n                    pass\n\n    def _run_end2end_pipeline(self, img_array: np.ndarray, save_prompts: bool) -&gt; Tuple[np.ndarray, Dict]:\n        \"\"\"Run the end2end pipeline: dewarping \u2192 deshadowing \u2192 appearance\"\"\"\n\n        intermediate_steps = {}\n\n        # Change to DocRes directory for inference to work properly\n        original_cwd = os.getcwd()\n        os.chdir(str(docres_dir))\n\n        # Set global DEVICE variable that DocRes inference expects\n        import inference  # Import the inference module to set its global DEVICE\n        inference.DEVICE = self.device\n\n        try:\n            with tempfile.TemporaryDirectory() as tmp_dir:\n                # Step 1: Dewarping\n                step1_path = os.path.join(tmp_dir, \"step1.jpg\")\n                cv2.imwrite(step1_path, img_array)\n\n                prompt1, prompt2, prompt3, dewarped = inference_one_im(self._model, step1_path, \"dewarping\")\n                intermediate_steps['dewarped'] = dewarped\n\n                # Step 2: Deshadowing\n                step2_path = os.path.join(tmp_dir, \"step2.jpg\")\n                cv2.imwrite(step2_path, dewarped)\n\n                prompt1, prompt2, prompt3, deshadowed = inference_one_im(self._model, step2_path, \"deshadowing\")\n                intermediate_steps['deshadowed'] = deshadowed\n\n                # Step 3: Appearance\n                step3_path = os.path.join(tmp_dir, \"step3.jpg\")\n                cv2.imwrite(step3_path, deshadowed)\n\n                prompt1, prompt2, prompt3, final = inference_one_im(self._model, step3_path, \"appearance\")\n\n                metadata = {\n                    'task': 'end2end',\n                    'device': str(self.device),\n                    'intermediate_steps': intermediate_steps\n                }\n\n                if save_prompts:\n                    metadata['prompts'] = {\n                        'prompt1': prompt1,\n                        'prompt2': prompt2,\n                        'prompt3': prompt3\n                    }\n\n                return final, metadata\n        finally:\n            # Always restore original working directory\n            os.chdir(original_cwd)\n\n    def batch_restore(\n        self, \n        images: List[Union[str, np.ndarray]], \n        task: str = \"appearance\",\n        save_prompts: bool = False\n    ) -&gt; List[Tuple[Optional[np.ndarray], Dict[str, Any]]]:\n        \"\"\"\n        Restore multiple images in batch\n\n        Args:\n            images: List of image paths or numpy arrays\n            task: Restoration task to perform\n            save_prompts: Whether to save intermediate prompts\n\n        Returns:\n            List of (restored_image, metadata) tuples\n        \"\"\"\n        results = []\n\n        for i, image in enumerate(images):\n            try:\n                restored_img, metadata = self.restore_image(image, task, save_prompts)\n                results.append((restored_img, metadata))\n            except Exception as e:\n                # Return None for failed images with error metadata\n                error_metadata = {\n                    'error': str(e),\n                    'task': task,\n                    'device': str(self.device),\n                    'image_index': i\n                }\n                results.append((None, error_metadata))\n\n        return results\n\n    def get_supported_tasks(self) -&gt; List[str]:\n        \"\"\"Get list of supported restoration tasks\"\"\"\n        return self.SUPPORTED_TASKS.copy()\n\n    def is_available(self) -&gt; bool:\n        \"\"\"Check if DocRes is available and properly configured\"\"\"\n        return DOCRES_AVAILABLE and self._model is not None\n\n    def restore_pdf(\n        self, \n        pdf_path: str, \n        output_path: str | None = None,\n        task: str = \"appearance\",\n        dpi: int = 200\n    ) -&gt; str | None:\n        \"\"\"\n        Restore an entire PDF document using DocRes\n\n        Args:\n            pdf_path: Path to the input PDF file\n            output_path: Path for the enhanced PDF (if None, auto-generates)\n            task: DocRes restoration task (default: \"appearance\")\n            dpi: DPI for PDF rendering (default: 200)\n\n        Returns:\n            Path to the enhanced PDF or None if failed\n        \"\"\"\n        try:\n            from PIL import Image\n            from doctra.utils.pdf_io import render_pdf_to_images\n\n            # Generate output path if not provided\n            if output_path is None:\n                pdf_dir = os.path.dirname(pdf_path)\n                pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n                output_path = os.path.join(pdf_dir, f\"{pdf_name}_enhanced.pdf\")\n\n            print(f\"\ud83d\udd04 Processing PDF with DocRes: {os.path.basename(pdf_path)}\")\n\n            # Render all pages to images\n            pil_pages = [im for (im, _, _) in render_pdf_to_images(pdf_path, dpi=dpi)]\n\n            if not pil_pages:\n                print(\"\u274c No pages found in PDF\")\n                return None\n\n            # Process each page with DocRes\n            enhanced_pages = []\n\n            # Detect environment for progress bar\n            is_notebook = \"ipykernel\" in sys.modules or \"jupyter\" in sys.modules\n\n            # Create progress bar for page processing\n            if is_notebook:\n                progress_bar = create_notebook_friendly_bar(\n                    total=len(pil_pages), \n                    desc=\"Processing pages\"\n                )\n            else:\n                progress_bar = create_beautiful_progress_bar(\n                    total=len(pil_pages), \n                    desc=\"Processing pages\",\n                    leave=True\n                )\n\n            with progress_bar:\n                for i, page_img in enumerate(pil_pages):\n                    try:\n                        # Convert PIL to numpy array\n                        img_array = np.array(page_img)\n\n                        # Apply DocRes restoration\n                        restored_img, _ = self.restore_image(img_array, task)\n\n                        # Convert back to PIL Image\n                        enhanced_page = Image.fromarray(restored_img)\n                        enhanced_pages.append(enhanced_page)\n\n                        progress_bar.set_description(f\"\u2705 Page {i+1}/{len(pil_pages)} processed\")\n                        progress_bar.update(1)\n\n                    except Exception as e:\n                        print(f\"  \u26a0\ufe0f Page {i+1} processing failed: {e}, using original\")\n                        enhanced_pages.append(page_img)\n                        progress_bar.set_description(f\"\u26a0\ufe0f Page {i+1} failed, using original\")\n                        progress_bar.update(1)\n\n            # Create enhanced PDF\n            if enhanced_pages:\n                enhanced_pages[0].save(\n                    output_path,\n                    \"PDF\",\n                    resolution=100.0,\n                    save_all=True,\n                    append_images=enhanced_pages[1:] if len(enhanced_pages) &gt; 1 else []\n                )\n\n                print(f\"\u2705 Enhanced PDF saved: {output_path}\")\n                return output_path\n            else:\n                print(\"\u274c No pages to save\")\n                return None\n\n        except ImportError as e:\n            print(f\"\u274c Required dependencies not available: {e}\")\n            print(\"Install with: pip install PyMuPDF\")\n            return None\n        except Exception as e:\n            print(f\"\u274c Error processing PDF with DocRes: {e}\")\n            return None\n</code></pre>"},{"location":"api/engines.html#doctra.engines.image_restoration.DocResEngine.__init__","title":"<code>__init__(device=None, use_half_precision=True, model_path=None, mbd_path=None)</code>","text":"<p>Initialize DocRes Engine</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Optional[str]</code> <p>Device to run on ('cuda', 'cpu', or None for auto-detect)</p> <code>None</code> <code>use_half_precision</code> <code>bool</code> <p>Whether to use half precision for inference</p> <code>True</code> <code>model_path</code> <code>Optional[str]</code> <p>Path to DocRes model checkpoint (optional, defaults to Hugging Face Hub)</p> <code>None</code> <code>mbd_path</code> <code>Optional[str]</code> <p>Path to MBD model checkpoint (optional, defaults to Hugging Face Hub)</p> <code>None</code> Source code in <code>doctra/engines/image_restoration/docres_engine.py</code> <pre><code>def __init__(\n    self, \n    device: Optional[str] = None,\n    use_half_precision: bool = True,\n    model_path: Optional[str] = None,\n    mbd_path: Optional[str] = None\n):\n    \"\"\"\n    Initialize DocRes Engine\n\n    Args:\n        device: Device to run on ('cuda', 'cpu', or None for auto-detect)\n        use_half_precision: Whether to use half precision for inference\n        model_path: Path to DocRes model checkpoint (optional, defaults to Hugging Face Hub)\n        mbd_path: Path to MBD model checkpoint (optional, defaults to Hugging Face Hub)\n    \"\"\"\n    if not DOCRES_AVAILABLE:\n        raise ImportError(\n            \"DocRes is not available. Please install the missing dependencies:\\n\"\n            \"pip install scikit-image&gt;=0.19.3\\n\\n\"\n            \"The DocRes module is already included in this library, but requires \"\n            \"scikit-image for image processing operations.\"\n        )\n\n    # Set device\n    if device is None:\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    else:\n        requested_device = torch.device(device)\n        # Check if the requested device is available\n        if requested_device.type == 'cuda' and not torch.cuda.is_available():\n            print(f\"Warning: CUDA requested but not available. Falling back to CPU.\")\n            self.device = torch.device('cpu')\n        else:\n            self.device = requested_device\n\n    self.use_half_precision = use_half_precision\n\n    # Get model paths (always from Hugging Face Hub)\n    try:\n        self.mbd_path, self.model_path = get_model_paths(\n            use_huggingface=True,\n            model_path=model_path,\n            mbd_path=mbd_path\n        )\n    except Exception as e:\n        raise RuntimeError(f\"Failed to get model paths: {e}\")\n\n    # Verify model files exist\n    if not os.path.exists(self.model_path):\n        raise FileNotFoundError(\n            f\"DocRes model not found at {self.model_path}. \"\n            f\"This may indicate a Hugging Face download failure. \"\n            f\"Please check your internet connection and try again.\"\n        )\n\n    if not os.path.exists(self.mbd_path):\n        raise FileNotFoundError(\n            f\"MBD model not found at {self.mbd_path}. \"\n            f\"This may indicate a Hugging Face download failure. \"\n            f\"Please check your internet connection and try again.\"\n        )\n\n    # Initialize model\n    self._model = None\n    self._initialize_model()\n</code></pre>"},{"location":"api/engines.html#doctra.engines.image_restoration.DocResEngine.batch_restore","title":"<code>batch_restore(images, task='appearance', save_prompts=False)</code>","text":"<p>Restore multiple images in batch</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>List[Union[str, ndarray]]</code> <p>List of image paths or numpy arrays</p> required <code>task</code> <code>str</code> <p>Restoration task to perform</p> <code>'appearance'</code> <code>save_prompts</code> <code>bool</code> <p>Whether to save intermediate prompts</p> <code>False</code> <p>Returns:</p> Type Description <code>List[Tuple[Optional[ndarray], Dict[str, Any]]]</code> <p>List of (restored_image, metadata) tuples</p> Source code in <code>doctra/engines/image_restoration/docres_engine.py</code> <pre><code>def batch_restore(\n    self, \n    images: List[Union[str, np.ndarray]], \n    task: str = \"appearance\",\n    save_prompts: bool = False\n) -&gt; List[Tuple[Optional[np.ndarray], Dict[str, Any]]]:\n    \"\"\"\n    Restore multiple images in batch\n\n    Args:\n        images: List of image paths or numpy arrays\n        task: Restoration task to perform\n        save_prompts: Whether to save intermediate prompts\n\n    Returns:\n        List of (restored_image, metadata) tuples\n    \"\"\"\n    results = []\n\n    for i, image in enumerate(images):\n        try:\n            restored_img, metadata = self.restore_image(image, task, save_prompts)\n            results.append((restored_img, metadata))\n        except Exception as e:\n            # Return None for failed images with error metadata\n            error_metadata = {\n                'error': str(e),\n                'task': task,\n                'device': str(self.device),\n                'image_index': i\n            }\n            results.append((None, error_metadata))\n\n    return results\n</code></pre>"},{"location":"api/engines.html#doctra.engines.image_restoration.DocResEngine.get_supported_tasks","title":"<code>get_supported_tasks()</code>","text":"<p>Get list of supported restoration tasks</p> Source code in <code>doctra/engines/image_restoration/docres_engine.py</code> <pre><code>def get_supported_tasks(self) -&gt; List[str]:\n    \"\"\"Get list of supported restoration tasks\"\"\"\n    return self.SUPPORTED_TASKS.copy()\n</code></pre>"},{"location":"api/engines.html#doctra.engines.image_restoration.DocResEngine.is_available","title":"<code>is_available()</code>","text":"<p>Check if DocRes is available and properly configured</p> Source code in <code>doctra/engines/image_restoration/docres_engine.py</code> <pre><code>def is_available(self) -&gt; bool:\n    \"\"\"Check if DocRes is available and properly configured\"\"\"\n    return DOCRES_AVAILABLE and self._model is not None\n</code></pre>"},{"location":"api/engines.html#doctra.engines.image_restoration.DocResEngine.restore_image","title":"<code>restore_image(image, task='appearance', save_prompts=False)</code>","text":"<p>Restore a single image using DocRes</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, ndarray]</code> <p>Path to image file or numpy array</p> required <code>task</code> <code>str</code> <p>Restoration task to perform</p> <code>'appearance'</code> <code>save_prompts</code> <code>bool</code> <p>Whether to save intermediate prompts</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, Dict[str, Any]]</code> <p>Tuple of (restored_image, metadata)</p> Source code in <code>doctra/engines/image_restoration/docres_engine.py</code> <pre><code>def restore_image(\n    self, \n    image: Union[str, np.ndarray], \n    task: str = \"appearance\",\n    save_prompts: bool = False\n) -&gt; Tuple[np.ndarray, Dict[str, Any]]:\n    \"\"\"\n    Restore a single image using DocRes\n\n    Args:\n        image: Path to image file or numpy array\n        task: Restoration task to perform\n        save_prompts: Whether to save intermediate prompts\n\n    Returns:\n        Tuple of (restored_image, metadata)\n    \"\"\"\n    if task not in self.SUPPORTED_TASKS:\n        raise ValueError(f\"Unsupported task: {task}. Supported tasks: {self.SUPPORTED_TASKS}\")\n\n    # Load image if path provided\n    if isinstance(image, str):\n        if not os.path.exists(image):\n            raise FileNotFoundError(f\"Image not found: {image}\")\n        img_array = cv2.imread(image)\n        if img_array is None:\n            raise ValueError(f\"Could not load image: {image}\")\n    else:\n        img_array = image.copy()\n\n    original_shape = img_array.shape\n\n    try:\n        # Handle end2end pipeline\n        if task == \"end2end\":\n            return self._run_end2end_pipeline(img_array, save_prompts)\n\n        # Run single task\n        restored_img, metadata = self._run_single_task(img_array, task, save_prompts)\n\n        metadata.update({\n            'original_shape': original_shape,\n            'restored_shape': restored_img.shape,\n            'task': task,\n            'device': str(self.device)\n        })\n\n        return restored_img, metadata\n\n    except Exception as e:\n        raise RuntimeError(f\"Image restoration failed: {e}\")\n</code></pre>"},{"location":"api/engines.html#doctra.engines.image_restoration.DocResEngine.restore_pdf","title":"<code>restore_pdf(pdf_path, output_path=None, task='appearance', dpi=200)</code>","text":"<p>Restore an entire PDF document using DocRes</p> <p>Parameters:</p> Name Type Description Default <code>pdf_path</code> <code>str</code> <p>Path to the input PDF file</p> required <code>output_path</code> <code>str | None</code> <p>Path for the enhanced PDF (if None, auto-generates)</p> <code>None</code> <code>task</code> <code>str</code> <p>DocRes restoration task (default: \"appearance\")</p> <code>'appearance'</code> <code>dpi</code> <code>int</code> <p>DPI for PDF rendering (default: 200)</p> <code>200</code> <p>Returns:</p> Type Description <code>str | None</code> <p>Path to the enhanced PDF or None if failed</p> Source code in <code>doctra/engines/image_restoration/docres_engine.py</code> <pre><code>def restore_pdf(\n    self, \n    pdf_path: str, \n    output_path: str | None = None,\n    task: str = \"appearance\",\n    dpi: int = 200\n) -&gt; str | None:\n    \"\"\"\n    Restore an entire PDF document using DocRes\n\n    Args:\n        pdf_path: Path to the input PDF file\n        output_path: Path for the enhanced PDF (if None, auto-generates)\n        task: DocRes restoration task (default: \"appearance\")\n        dpi: DPI for PDF rendering (default: 200)\n\n    Returns:\n        Path to the enhanced PDF or None if failed\n    \"\"\"\n    try:\n        from PIL import Image\n        from doctra.utils.pdf_io import render_pdf_to_images\n\n        # Generate output path if not provided\n        if output_path is None:\n            pdf_dir = os.path.dirname(pdf_path)\n            pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n            output_path = os.path.join(pdf_dir, f\"{pdf_name}_enhanced.pdf\")\n\n        print(f\"\ud83d\udd04 Processing PDF with DocRes: {os.path.basename(pdf_path)}\")\n\n        # Render all pages to images\n        pil_pages = [im for (im, _, _) in render_pdf_to_images(pdf_path, dpi=dpi)]\n\n        if not pil_pages:\n            print(\"\u274c No pages found in PDF\")\n            return None\n\n        # Process each page with DocRes\n        enhanced_pages = []\n\n        # Detect environment for progress bar\n        is_notebook = \"ipykernel\" in sys.modules or \"jupyter\" in sys.modules\n\n        # Create progress bar for page processing\n        if is_notebook:\n            progress_bar = create_notebook_friendly_bar(\n                total=len(pil_pages), \n                desc=\"Processing pages\"\n            )\n        else:\n            progress_bar = create_beautiful_progress_bar(\n                total=len(pil_pages), \n                desc=\"Processing pages\",\n                leave=True\n            )\n\n        with progress_bar:\n            for i, page_img in enumerate(pil_pages):\n                try:\n                    # Convert PIL to numpy array\n                    img_array = np.array(page_img)\n\n                    # Apply DocRes restoration\n                    restored_img, _ = self.restore_image(img_array, task)\n\n                    # Convert back to PIL Image\n                    enhanced_page = Image.fromarray(restored_img)\n                    enhanced_pages.append(enhanced_page)\n\n                    progress_bar.set_description(f\"\u2705 Page {i+1}/{len(pil_pages)} processed\")\n                    progress_bar.update(1)\n\n                except Exception as e:\n                    print(f\"  \u26a0\ufe0f Page {i+1} processing failed: {e}, using original\")\n                    enhanced_pages.append(page_img)\n                    progress_bar.set_description(f\"\u26a0\ufe0f Page {i+1} failed, using original\")\n                    progress_bar.update(1)\n\n        # Create enhanced PDF\n        if enhanced_pages:\n            enhanced_pages[0].save(\n                output_path,\n                \"PDF\",\n                resolution=100.0,\n                save_all=True,\n                append_images=enhanced_pages[1:] if len(enhanced_pages) &gt; 1 else []\n            )\n\n            print(f\"\u2705 Enhanced PDF saved: {output_path}\")\n            return output_path\n        else:\n            print(\"\u274c No pages to save\")\n            return None\n\n    except ImportError as e:\n        print(f\"\u274c Required dependencies not available: {e}\")\n        print(\"Install with: pip install PyMuPDF\")\n        return None\n    except Exception as e:\n        print(f\"\u274c Error processing PDF with DocRes: {e}\")\n        return None\n</code></pre>"},{"location":"api/engines.html#quick-reference","title":"Quick Reference","text":""},{"location":"api/engines.html#docresengine_1","title":"DocResEngine","text":"<pre><code>from doctra import DocResEngine\n\n# Initialize engine\nengine = DocResEngine(\n    device: str = None,  # \"cuda\", \"cpu\", or None for auto-detect\n    use_half_precision: bool = False,\n    model_path: str = None,\n    mbd_path: str = None\n)\n\n# Restore single image\nrestored_img, metadata = engine.restore_image(\n    image: Union[str, np.ndarray, PIL.Image.Image],\n    task: str = \"appearance\"\n)\n\n# Restore PDF\noutput_path = engine.restore_pdf(\n    pdf_path: str,\n    output_path: str = None,\n    task: str = \"appearance\",\n    dpi: int = 200\n)\n</code></pre>"},{"location":"api/engines.html#parameter-reference","title":"Parameter Reference","text":""},{"location":"api/engines.html#initialization-parameters","title":"Initialization Parameters","text":"Parameter Type Default Description <code>device</code> str None Processing device: \"cuda\", \"cpu\", or None (auto-detect) <code>use_half_precision</code> bool False Use FP16 for faster GPU processing <code>model_path</code> str None Custom path to restoration model <code>mbd_path</code> str None Custom path to MBD model"},{"location":"api/engines.html#restoration-tasks","title":"Restoration Tasks","text":"Task Description Use Case <code>\"appearance\"</code> General appearance enhancement Most documents (default) <code>\"dewarping\"</code> Correct perspective distortion Scanned with perspective issues <code>\"deshadowing\"</code> Remove shadows and lighting artifacts Poor lighting conditions <code>\"deblurring\"</code> Reduce blur and improve sharpness Motion blur, focus issues <code>\"binarization\"</code> Convert to black and white Clean text extraction <code>\"end2end\"</code> Complete restoration pipeline Severely degraded documents"},{"location":"api/engines.html#methods","title":"Methods","text":""},{"location":"api/engines.html#restore_image","title":"restore_image()","text":"<p>Restore a single image.</p> <p>Parameters:</p> <ul> <li><code>image</code> (str | np.ndarray | PIL.Image.Image): Input image (path, numpy array, or PIL Image)</li> <li><code>task</code> (str): Restoration task to perform</li> </ul> <p>Returns:</p> <ul> <li><code>restored_img</code> (PIL.Image.Image): Restored image</li> <li><code>metadata</code> (dict): Processing metadata including task, device, and timing</li> </ul> <p>Example:</p> <pre><code>from doctra import DocResEngine\n\nengine = DocResEngine(device=\"cuda\")\nrestored, meta = engine.restore_image(\"blurry.jpg\", task=\"deblurring\")\n\nprint(f\"Task: {meta['task']}\")\nprint(f\"Device: {meta['device']}\")\nprint(f\"Time: {meta['processing_time']:.2f}s\")\n\n# Save restored image\nrestored.save(\"restored.jpg\")\n</code></pre>"},{"location":"api/engines.html#restore_pdf","title":"restore_pdf()","text":"<p>Restore all pages in a PDF document.</p> <p>Parameters:</p> <ul> <li><code>pdf_path</code> (str): Path to input PDF</li> <li><code>output_path</code> (str, optional): Path for output PDF (auto-generated if None)</li> <li><code>task</code> (str): Restoration task to perform</li> <li><code>dpi</code> (int): Resolution for processing</li> </ul> <p>Returns:</p> <ul> <li><code>output_path</code> (str): Path to the restored PDF</li> </ul> <p>Example:</p> <pre><code>from doctra import DocResEngine\n\nengine = DocResEngine(device=\"cuda\")\nrestored_pdf = engine.restore_pdf(\n    pdf_path=\"low_quality.pdf\",\n    output_path=\"enhanced.pdf\",\n    task=\"appearance\",\n    dpi=300\n)\n\nprint(f\"Restored PDF saved to: {restored_pdf}\")\n</code></pre>"},{"location":"api/engines.html#device-selection","title":"Device Selection","text":""},{"location":"api/engines.html#auto-detection","title":"Auto-Detection","text":"<pre><code># Automatically uses GPU if available, otherwise CPU\nengine = DocResEngine()\n</code></pre>"},{"location":"api/engines.html#explicit-gpu","title":"Explicit GPU","text":"<pre><code># Force GPU usage (will error if CUDA not available)\nengine = DocResEngine(device=\"cuda\")\n</code></pre>"},{"location":"api/engines.html#explicit-cpu","title":"Explicit CPU","text":"<pre><code># Force CPU usage (slower but always available)\nengine = DocResEngine(device=\"cpu\")\n</code></pre>"},{"location":"api/engines.html#check-device","title":"Check Device","text":"<pre><code>import torch\n\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n</code></pre>"},{"location":"api/engines.html#performance-optimization","title":"Performance Optimization","text":""},{"location":"api/engines.html#half-precision","title":"Half Precision","text":"<p>Use FP16 for ~2x speed on modern GPUs:</p> <pre><code>engine = DocResEngine(\n    device=\"cuda\",\n    use_half_precision=True  # Faster, minimal quality loss\n)\n</code></pre> <p>Requirements: - NVIDIA GPU with compute capability 7.0+ (Volta or newer) - Examples: RTX 20xx, RTX 30xx, RTX 40xx, A100, V100</p>"},{"location":"api/engines.html#batch-processing","title":"Batch Processing","text":"<p>Process multiple images efficiently:</p> <pre><code>from doctra import DocResEngine\n\nengine = DocResEngine(device=\"cuda\")\n\n# Process image list\nimages = [\"doc1.jpg\", \"doc2.jpg\", \"doc3.jpg\"]\nrestored_images = []\n\nfor img_path in images:\n    restored, _ = engine.restore_image(img_path, task=\"appearance\")\n    restored_images.append(restored)\n    restored.save(f\"restored_{img_path}\")\n</code></pre>"},{"location":"api/engines.html#dpi-considerations","title":"DPI Considerations","text":"DPI Quality Speed Memory Best For 100 Low Fast Low Quick previews 150 Medium Medium Medium General use 200 Good Slow Medium Default setting 300 High Very Slow High High-quality scans"},{"location":"api/engines.html#metadata","title":"Metadata","text":"<p>The <code>restore_image()</code> method returns metadata:</p> <pre><code>restored, metadata = engine.restore_image(\"doc.jpg\", \"appearance\")\n\nprint(metadata)\n# {\n#     'task': 'appearance',\n#     'device': 'cuda',\n#     'processing_time': 1.23,\n#     'input_size': (1920, 1080),\n#     'output_size': (1920, 1080)\n# }\n</code></pre>"},{"location":"api/engines.html#error-handling","title":"Error Handling","text":"<pre><code>from doctra import DocResEngine\n\nengine = DocResEngine(device=\"cuda\")\n\ntry:\n    restored, meta = engine.restore_image(\"document.jpg\", \"appearance\")\nexcept FileNotFoundError:\n    print(\"Image not found\")\nexcept RuntimeError as e:\n    print(f\"CUDA error: {e}\")\n    # Fall back to CPU\n    engine = DocResEngine(device=\"cpu\")\n    restored, meta = engine.restore_image(\"document.jpg\", \"appearance\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"api/engines.html#integration-with-parsers","title":"Integration with Parsers","text":"<p>DocResEngine is integrated into EnhancedPDFParser:</p> <pre><code>from doctra import EnhancedPDFParser\n\n# This internally uses DocResEngine\nparser = EnhancedPDFParser(\n    use_image_restoration=True,\n    restoration_task=\"appearance\",\n    restoration_device=\"cuda\"\n)\n\nparser.parse(\"document.pdf\")\n</code></pre> <p>For standalone restoration:</p> <pre><code>from doctra import DocResEngine\n\n# Step 1: Restore PDF\nengine = DocResEngine(device=\"cuda\")\nenhanced_pdf = engine.restore_pdf(\n    pdf_path=\"low_quality.pdf\",\n    output_path=\"enhanced.pdf\",\n    task=\"appearance\"\n)\n\n# Step 2: Parse enhanced PDF\nfrom doctra import StructuredPDFParser\n\nparser = StructuredPDFParser()\nparser.parse(enhanced_pdf)\n</code></pre>"},{"location":"api/engines.html#examples","title":"Examples","text":""},{"location":"api/engines.html#example-1-dewarp-scanned-document","title":"Example 1: Dewarp Scanned Document","text":"<pre><code>from doctra import DocResEngine\n\nengine = DocResEngine(device=\"cuda\")\n\n# Fix perspective distortion\nrestored, meta = engine.restore_image(\n    \"scanned_with_distortion.jpg\",\n    task=\"dewarping\"\n)\n\nrestored.save(\"dewarped.jpg\")\nprint(f\"Processed in {meta['processing_time']:.2f}s\")\n</code></pre>"},{"location":"api/engines.html#example-2-remove-shadows","title":"Example 2: Remove Shadows","text":"<pre><code>from doctra import DocResEngine\n\nengine = DocResEngine(device=\"cuda\")\n\n# Remove shadow artifacts\nrestored, meta = engine.restore_image(\n    \"document_with_shadows.jpg\",\n    task=\"deshadowing\"\n)\n\nrestored.save(\"no_shadows.jpg\")\n</code></pre>"},{"location":"api/engines.html#example-3-batch-pdf-restoration","title":"Example 3: Batch PDF Restoration","text":"<pre><code>import os\nfrom doctra import DocResEngine\n\nengine = DocResEngine(device=\"cuda\", use_half_precision=True)\n\npdf_dir = \"input_pdfs\"\noutput_dir = \"restored_pdfs\"\nos.makedirs(output_dir, exist_ok=True)\n\nfor filename in os.listdir(pdf_dir):\n    if filename.endswith(\".pdf\"):\n        input_path = os.path.join(pdf_dir, filename)\n        output_path = os.path.join(output_dir, f\"restored_{filename}\")\n\n        print(f\"Processing {filename}...\")\n        engine.restore_pdf(\n            pdf_path=input_path,\n            output_path=output_path,\n            task=\"appearance\",\n            dpi=200\n        )\n</code></pre>"},{"location":"api/engines.html#see-also","title":"See Also","text":"<ul> <li>Enhanced Parser - Using restoration with parsing</li> <li>Core Concepts - Understanding image restoration</li> <li>Examples - Advanced usage patterns</li> </ul>"},{"location":"api/exporters.html","title":"Exporters API Reference","text":"<p>Documentation for Doctra's export functionality.</p>"},{"location":"api/exporters.html#overview","title":"Overview","text":"<p>Exporters handle converting parsed document content into various output formats.</p>"},{"location":"api/exporters.html#available-exporters","title":"Available Exporters","text":""},{"location":"api/exporters.html#markdownwriter","title":"MarkdownWriter","text":"<p>Generates human-readable Markdown files with embedded images.</p>"},{"location":"api/exporters.html#htmlwriter","title":"HTMLWriter","text":"<p>Produces styled HTML documents for web viewing.</p>"},{"location":"api/exporters.html#excelwriter","title":"ExcelWriter","text":"<p>Creates Excel spreadsheets with structured data from tables and charts.</p>"},{"location":"api/exporters.html#imagesaver","title":"ImageSaver","text":"<p>Saves cropped images of visual elements (figures, charts, tables).</p>"},{"location":"api/exporters.html#usage","title":"Usage","text":"<p>Exporters are used automatically by parsers. Output format is determined by parser configuration.</p>"},{"location":"api/exporters.html#output-files","title":"Output Files","text":"<pre><code>outputs/\n\u2514\u2500\u2500 document/\n    \u251c\u2500\u2500 result.md          # MarkdownWriter\n    \u251c\u2500\u2500 result.html        # HTMLWriter\n    \u251c\u2500\u2500 tables.xlsx        # ExcelWriter (with VLM)\n    \u251c\u2500\u2500 tables.html        # HTMLWriter (with VLM)\n    \u2514\u2500\u2500 images/            # ImageSaver\n        \u251c\u2500\u2500 figures/\n        \u251c\u2500\u2500 charts/\n        \u2514\u2500\u2500 tables/\n</code></pre>"},{"location":"api/exporters.html#see-also","title":"See Also","text":"<ul> <li>Parsers API - Main parsing functionality</li> <li>Export Formats - Detailed format documentation</li> </ul>"},{"location":"api/parsers.html","title":"Parsers API Reference","text":"<p>Complete API documentation for all Doctra parsers.</p>"},{"location":"api/parsers.html#structuredpdfparser","title":"StructuredPDFParser","text":"<p>The base parser for comprehensive PDF document processing.</p>"},{"location":"api/parsers.html#doctra.parsers.structured_pdf_parser.StructuredPDFParser","title":"<code>doctra.parsers.structured_pdf_parser.StructuredPDFParser</code>","text":"<pre><code>Comprehensive PDF parser for extracting all types of content.\n\nProcesses PDF documents to extract text, tables, charts, and figures.\nSupports OCR for text extraction and optional VLM processing for\nconverting visual elements into structured data.\n\nFeatures automatic detection and merging of tables split across pages\nusing proximity detection and LSD-based structure analysis.\n\n:param use_vlm: Whether to use VLM for structured data extraction (default: False)\n:param vlm_provider: VLM provider to use (\"gemini\", \"openai\", \"anthropic\", or \"openrouter\", default: \"gemini\")\n:param vlm_model: Model name to use (defaults to provider-specific defaults)\n:param vlm_api_key: API key for VLM provider (required if use_vlm is True)\n:param layout_model_name: Layout detection model name (default: \"PP-DocLayout_plus-L\")\n:param dpi: DPI for PDF rendering (default: 200)\n:param min_score: Minimum confidence score for layout detection (default: 0.0)\n:param ocr_lang: OCR language code (default: \"eng\")\n:param ocr_psm: Tesseract page segmentation mode (default: 4)\n:param ocr_oem: Tesseract OCR engine mode (default: 3)\n:param ocr_extra_config: Additional Tesseract configuration (default: \"\")\n:param box_separator: Separator between text boxes in output (default: \"\n</code></pre> <p>\")     :param merge_split_tables: Whether to detect and merge split tables (default: False)     :param bottom_threshold_ratio: Ratio for \"too close to bottom\" detection (default: 0.20)     :param top_threshold_ratio: Ratio for \"too close to top\" detection (default: 0.10)     :param max_gap_ratio: Maximum allowed gap between tables (default: 0.05)     :param column_alignment_tolerance: Pixel tolerance for column alignment (default: 10.0)     :param min_merge_confidence: Minimum confidence score for merging (default: 0.7)</p> Source code in <code>doctra/parsers/structured_pdf_parser.py</code> <pre><code>class StructuredPDFParser:\n    \"\"\"\n    Comprehensive PDF parser for extracting all types of content.\n\n    Processes PDF documents to extract text, tables, charts, and figures.\n    Supports OCR for text extraction and optional VLM processing for\n    converting visual elements into structured data.\n\n    Features automatic detection and merging of tables split across pages\n    using proximity detection and LSD-based structure analysis.\n\n    :param use_vlm: Whether to use VLM for structured data extraction (default: False)\n    :param vlm_provider: VLM provider to use (\"gemini\", \"openai\", \"anthropic\", or \"openrouter\", default: \"gemini\")\n    :param vlm_model: Model name to use (defaults to provider-specific defaults)\n    :param vlm_api_key: API key for VLM provider (required if use_vlm is True)\n    :param layout_model_name: Layout detection model name (default: \"PP-DocLayout_plus-L\")\n    :param dpi: DPI for PDF rendering (default: 200)\n    :param min_score: Minimum confidence score for layout detection (default: 0.0)\n    :param ocr_lang: OCR language code (default: \"eng\")\n    :param ocr_psm: Tesseract page segmentation mode (default: 4)\n    :param ocr_oem: Tesseract OCR engine mode (default: 3)\n    :param ocr_extra_config: Additional Tesseract configuration (default: \"\")\n    :param box_separator: Separator between text boxes in output (default: \"\\n\")\n    :param merge_split_tables: Whether to detect and merge split tables (default: False)\n    :param bottom_threshold_ratio: Ratio for \"too close to bottom\" detection (default: 0.20)\n    :param top_threshold_ratio: Ratio for \"too close to top\" detection (default: 0.10)\n    :param max_gap_ratio: Maximum allowed gap between tables (default: 0.05)\n    :param column_alignment_tolerance: Pixel tolerance for column alignment (default: 10.0)\n    :param min_merge_confidence: Minimum confidence score for merging (default: 0.7)\n    \"\"\"\n\n    def __init__(\n            self,\n            *,\n            use_vlm: bool = False,\n            vlm_provider: str = \"gemini\",\n            vlm_model: str | None = None,\n            vlm_api_key: str | None = None,\n            layout_model_name: str = \"PP-DocLayout_plus-L\",\n            dpi: int = 200,\n            min_score: float = 0.0,\n            ocr_lang: str = \"eng\",\n            ocr_psm: int = 4,\n            ocr_oem: int = 3,\n            ocr_extra_config: str = \"\",\n            box_separator: str = \"\\n\",\n            merge_split_tables: bool = False,\n            bottom_threshold_ratio: float = 0.20,\n            top_threshold_ratio: float = 0.15,\n            max_gap_ratio: float = 0.25,\n            column_alignment_tolerance: float = 10.0,\n            min_merge_confidence: float = 0.65,\n    ):\n        \"\"\"\n        Initialize the StructuredPDFParser with processing configuration.\n\n        Also suppresses noisy DEBUG logs from external libraries.\n\n        :param use_vlm: Whether to use VLM for structured data extraction (default: False)\n        :param vlm_provider: VLM provider to use (\"gemini\", \"openai\", \"anthropic\", or \"openrouter\", default: \"gemini\")\n        :param vlm_model: Model name to use (defaults to provider-specific defaults)\n        :param vlm_api_key: API key for VLM provider (required if use_vlm is True)\n        :param layout_model_name: Layout detection model name (default: \"PP-DocLayout_plus-L\")\n        :param dpi: DPI for PDF rendering (default: 200)\n        :param min_score: Minimum confidence score for layout detection (default: 0.0)\n        :param ocr_lang: OCR language code (default: \"eng\")\n        :param ocr_psm: Tesseract page segmentation mode (default: 4)\n        :param ocr_oem: Tesseract OCR engine mode (default: 3)\n        :param ocr_extra_config: Additional Tesseract configuration (default: \"\")\n        :param box_separator: Separator between text boxes in output (default: \"\\n\")\n        :param merge_split_tables: Whether to detect and merge split tables (default: False)\n        :param bottom_threshold_ratio: Ratio for \"too close to bottom\" detection (default: 0.20)\n        :param top_threshold_ratio: Ratio for \"too close to top\" detection (default: 0.15)\n        :param max_gap_ratio: Maximum allowed gap between tables (default: 0.25, accounts for headers/footers)\n        :param column_alignment_tolerance: Pixel tolerance for column alignment (default: 10.0)\n        :param min_merge_confidence: Minimum confidence score for merging (default: 0.65)\n        \"\"\"\n        self.layout_engine = PaddleLayoutEngine(model_name=layout_model_name)\n        self.dpi = dpi\n        self.min_score = min_score\n        self.ocr_engine = PytesseractOCREngine(\n            lang=ocr_lang, psm=ocr_psm, oem=ocr_oem, extra_config=ocr_extra_config\n        )\n        self.box_separator = box_separator\n        self.use_vlm = use_vlm\n        self.vlm = None\n        if self.use_vlm:\n            try:\n                self.vlm = VLMStructuredExtractor(\n                    vlm_provider=vlm_provider,\n                    vlm_model=vlm_model,\n                    api_key=vlm_api_key,\n                )\n            except Exception as e:\n                self.vlm = None\n\n        # Initialize split table detector\n        self.merge_split_tables = merge_split_tables\n        if self.merge_split_tables:\n            self.split_table_detector = SplitTableDetector(\n                bottom_threshold_ratio=bottom_threshold_ratio,\n                top_threshold_ratio=top_threshold_ratio,\n                max_gap_ratio=max_gap_ratio,\n                column_alignment_tolerance=column_alignment_tolerance,\n                min_merge_confidence=min_merge_confidence,\n            )\n        else:\n            self.split_table_detector = None\n\n        # Suppress noisy DEBUG logs from external libraries\n        logging.getLogger('pytesseract').setLevel(logging.WARNING)\n        logging.getLogger('markdown_it').setLevel(logging.WARNING)\n\n    def parse(self, pdf_path: str) -&gt; None:\n        \"\"\"\n        Parse a PDF document and extract all content types.\n\n        :param pdf_path: Path to the input PDF file\n        :return: None\n        \"\"\"\n        pdf_filename = os.path.splitext(os.path.basename(pdf_path))[0]\n        out_dir = f\"outputs/{pdf_filename}/full_parse\"\n\n        os.makedirs(out_dir, exist_ok=True)\n        ensure_output_dirs(out_dir, IMAGE_SUBDIRS)\n\n        pages: List[LayoutPage] = self.layout_engine.predict_pdf(\n            pdf_path, batch_size=1, layout_nms=True, dpi=self.dpi, min_score=self.min_score\n        )\n        pil_pages = [im for (im, _, _) in render_pdf_to_images(pdf_path, dpi=self.dpi)]\n\n        # Detect split tables before processing\n        split_table_matches: List[SplitTableMatch] = []\n        merged_table_segments = []  # Track TableSegment objects that are merged\n\n        if self.merge_split_tables and self.split_table_detector:\n            try:\n                split_table_matches = self.split_table_detector.detect_split_tables(pages, pil_pages)\n                # Track which segments are part of merges\n                for match in split_table_matches:\n                    merged_table_segments.append(match.segment1)\n                    merged_table_segments.append(match.segment2)\n            except Exception as e:\n                import traceback\n                traceback.print_exc()\n                split_table_matches = []\n\n        fig_count = sum(sum(1 for b in p.boxes if b.label == \"figure\") for p in pages)\n        chart_count = sum(sum(1 for b in p.boxes if b.label == \"chart\") for p in pages)\n        table_count = sum(sum(1 for b in p.boxes if b.label == \"table\") for p in pages)\n\n        md_lines: List[str] = [\"# Extracted Content\\n\"]\n        html_lines: List[str] = [\"&lt;h1&gt;Extracted Content&lt;/h1&gt;\"]  # For direct HTML generation\n        structured_items: List[Dict[str, Any]] = []\n\n        charts_desc = \"Charts (VLM \u2192 table)\" if self.use_vlm else \"Charts (cropped)\"\n        tables_desc = \"Tables (VLM \u2192 table)\" if self.use_vlm else \"Tables (cropped)\"\n        figures_desc = \"Figures (cropped)\"\n\n        with ExitStack() as stack:\n            is_notebook = \"ipykernel\" in sys.modules or \"jupyter\" in sys.modules\n            is_terminal = hasattr(sys.stdout, 'isatty') and sys.stdout.isatty()\n            if is_notebook:\n                charts_bar = stack.enter_context(\n                    create_notebook_friendly_bar(total=chart_count, desc=charts_desc)) if chart_count else None\n                tables_bar = stack.enter_context(\n                    create_notebook_friendly_bar(total=table_count, desc=tables_desc)) if table_count else None\n                figures_bar = stack.enter_context(\n                    create_notebook_friendly_bar(total=fig_count, desc=figures_desc)) if fig_count else None\n            else:\n                charts_bar = stack.enter_context(\n                    create_beautiful_progress_bar(total=chart_count, desc=charts_desc, leave=True)) if chart_count else None\n                tables_bar = stack.enter_context(\n                    create_beautiful_progress_bar(total=table_count, desc=tables_desc, leave=True)) if table_count else None\n                figures_bar = stack.enter_context(\n                    create_beautiful_progress_bar(total=fig_count, desc=figures_desc, leave=True)) if fig_count else None\n\n            for p in pages:\n                page_num = p.page_index\n                page_img: Image.Image = pil_pages[page_num - 1]\n                md_lines.append(f\"\\n## Page {page_num}\\n\")\n                html_lines.append(f\"&lt;h2&gt;Page {page_num}&lt;/h2&gt;\")\n\n                for i, box in enumerate(sorted(p.boxes, key=reading_order_key), start=1):\n                    if box.label in EXCLUDE_LABELS:\n                        img_path = save_box_image(page_img, box, out_dir, page_num, i, IMAGE_SUBDIRS)\n                        abs_img_path = os.path.abspath(img_path)\n                        rel = os.path.relpath(abs_img_path, out_dir)\n\n                        if box.label == \"figure\":\n                            figure_md = f\"![Figure \u2014 page {page_num}]({rel})\\n\"\n                            figure_html = f'&lt;img src=\"{rel}\" alt=\"Figure \u2014 page {page_num}\" /&gt;'\n                            md_lines.append(figure_md)\n                            html_lines.append(figure_html)\n                            if figures_bar: figures_bar.update(1)\n\n                        elif box.label == \"chart\":\n                            if self.use_vlm and self.vlm:\n                                wrote_table = False\n                                try:\n                                    chart = self.vlm.extract_chart(abs_img_path)\n                                    item = to_structured_dict(chart)\n                                    if item:\n                                        # Add page and type information to structured item\n                                        item[\"page\"] = page_num\n                                        item[\"type\"] = \"Chart\"\n                                        structured_items.append(item)\n\n                                        # Generate both markdown and HTML tables\n                                        table_md = render_markdown_table(item.get(\"headers\"), item.get(\"rows\"),\n                                                                         title=item.get(\"title\"))\n                                        table_html = render_html_table(item.get(\"headers\"), item.get(\"rows\"),\n                                                                       title=item.get(\"title\"))\n\n                                        md_lines.append(table_md)\n                                        html_lines.append(table_html)\n                                        wrote_table = True\n                                except Exception as e:\n                                    pass\n                                if not wrote_table:\n                                    chart_md = f\"![Chart \u2014 page {page_num}]({rel})\\n\"\n                                    chart_html = f'&lt;img src=\"{rel}\" alt=\"Chart \u2014 page {page_num}\" /&gt;'\n                                    md_lines.append(chart_md)\n                                    html_lines.append(chart_html)\n                            else:\n                                chart_md = f\"![Chart \u2014 page {page_num}]({rel})\\n\"\n                                chart_html = f'&lt;img src=\"{rel}\" alt=\"Chart \u2014 page {page_num}\" /&gt;'\n                                md_lines.append(chart_md)\n                                html_lines.append(chart_html)\n                            if charts_bar: charts_bar.update(1)\n\n                        elif box.label == \"table\":\n                            # Skip tables that are part of merged split tables\n                            is_merged = any(seg.match_box(box, page_num) for seg in merged_table_segments)\n                            if is_merged:\n                                continue\n\n                            if self.use_vlm and self.vlm:\n                                wrote_table = False\n                                try:\n                                    table = self.vlm.extract_table(abs_img_path)\n                                    item = to_structured_dict(table)\n                                    if item:\n                                        # Add page and type information to structured item\n                                        item[\"page\"] = page_num\n                                        item[\"type\"] = \"Table\"\n                                        structured_items.append(item)\n\n                                        # Generate both markdown and HTML tables\n                                        table_md = render_markdown_table(item.get(\"headers\"), item.get(\"rows\"),\n                                                                         title=item.get(\"title\"))\n                                        table_html = render_html_table(item.get(\"headers\"), item.get(\"rows\"),\n                                                                       title=item.get(\"title\"))\n\n                                        md_lines.append(table_md)\n                                        html_lines.append(table_html)\n                                        wrote_table = True\n                                except Exception as e:\n                                    pass\n                                if not wrote_table:\n                                    table_md = f\"![Table \u2014 page {page_num}]({rel})\\n\"\n                                    table_html = f'&lt;img src=\"{rel}\" alt=\"Table \u2014 page {page_num}\" /&gt;'\n                                    md_lines.append(table_md)\n                                    html_lines.append(table_html)\n                            else:\n                                table_md = f\"![Table \u2014 page {page_num}]({rel})\\n\"\n                                table_html = f'&lt;img src=\"{rel}\" alt=\"Table \u2014 page {page_num}\" /&gt;'\n                                md_lines.append(table_md)\n                                html_lines.append(table_html)\n                            if tables_bar: tables_bar.update(1)\n                    else:\n                        text = ocr_box_text(self.ocr_engine, page_img, box)\n                        if text:\n                            md_lines.append(text)\n                            md_lines.append(self.box_separator if self.box_separator else \"\")\n                            # Convert text to HTML (basic conversion)\n                            html_text = text.replace('\\n', '&lt;br&gt;')\n                            html_lines.append(f\"&lt;p&gt;{html_text}&lt;/p&gt;\")\n                            if self.box_separator:\n                                html_lines.append(\"&lt;br&gt;\")\n\n            # Process merged split tables\n            if split_table_matches and self.split_table_detector:\n                for match_idx, match in enumerate(split_table_matches):\n                    try:\n                        # Merge the table images\n                        merged_img = self.split_table_detector.merge_table_images(match)\n\n                        # Save merged image\n                        tables_dir = os.path.join(out_dir, \"tables\")\n                        os.makedirs(tables_dir, exist_ok=True)\n                        merged_filename = f\"merged_table_{match.segment1.page_index}_{match.segment2.page_index}.png\"\n                        merged_path = os.path.join(tables_dir, merged_filename)\n                        merged_img.save(merged_path)\n\n                        abs_merged_path = os.path.abspath(merged_path)\n                        rel_merged = os.path.relpath(abs_merged_path, out_dir)\n\n                        # Add to markdown/HTML at the page where first segment appears\n                        pages_str = f\"pages {match.segment1.page_index}-{match.segment2.page_index}\"\n\n                        if self.use_vlm and self.vlm:\n                            wrote_table = False\n                            try:\n                                table = self.vlm.extract_table(abs_merged_path)\n                                item = to_structured_dict(table)\n                                if item:\n                                    # Add page and type information to structured item\n                                    item[\"page\"] = f\"{match.segment1.page_index}-{match.segment2.page_index}\"\n                                    item[\"type\"] = \"Table (Merged)\"\n                                    item[\"split_merge\"] = True\n                                    item[\"merge_confidence\"] = match.confidence\n                                    structured_items.append(item)\n\n                                    # Generate both markdown and HTML tables\n                                    table_md = render_markdown_table(\n                                        item.get(\"headers\"), \n                                        item.get(\"rows\"),\n                                        title=item.get(\"title\") or f\"Merged Table ({pages_str})\"\n                                    )\n                                    table_html = render_html_table(\n                                        item.get(\"headers\"), \n                                        item.get(\"rows\"),\n                                        title=item.get(\"title\") or f\"Merged Table ({pages_str})\"\n                                    )\n\n                                    # Insert before the next page section or at end\n                                    md_lines.append(f\"\\n### Merged Table ({pages_str})\\n\")\n                                    md_lines.append(table_md)\n                                    html_lines.append(f'&lt;h3&gt;Merged Table ({pages_str})&lt;/h3&gt;')\n                                    html_lines.append(table_html)\n                                    wrote_table = True\n                            except Exception as e:\n                                pass\n\n                            if not wrote_table:\n                                table_md = f\"![Merged Table \u2014 {pages_str}]({rel_merged})\\n\"\n                                table_html = f'&lt;img src=\"{rel_merged}\" alt=\"Merged Table \u2014 {pages_str}\" /&gt;'\n                                md_lines.append(f\"\\n### Merged Table ({pages_str})\\n\")\n                                md_lines.append(table_md)\n                                html_lines.append(f'&lt;h3&gt;Merged Table ({pages_str})&lt;/h3&gt;')\n                                html_lines.append(table_html)\n                        else:\n                            table_md = f\"![Merged Table \u2014 {pages_str}]({rel_merged})\\n\"\n                            table_html = f'&lt;img src=\"{rel_merged}\" alt=\"Merged Table \u2014 {pages_str}\" /&gt;'\n                            md_lines.append(f\"\\n### Merged Table ({pages_str})\\n\")\n                            md_lines.append(table_md)\n                            html_lines.append(f'&lt;h3&gt;Merged Table ({pages_str})&lt;/h3&gt;')\n                            html_lines.append(table_html)\n\n                        if tables_bar: tables_bar.update(1)\n\n                    except Exception as e:\n                        print(f\"\u26a0\ufe0f  Warning: Failed to merge table {match_idx + 1}: {e}\")\n\n        md_path = write_markdown(md_lines, out_dir)\n\n        # Use HTML lines if VLM is enabled for better table formatting\n        if self.use_vlm and html_lines:\n            html_path = write_html_from_lines(html_lines, out_dir)\n        else:\n            html_path = write_html(md_lines, out_dir)\n\n        excel_path = None\n        html_structured_path = None\n        if self.use_vlm and structured_items:\n            excel_path = os.path.join(out_dir, \"tables.xlsx\")\n            write_structured_excel(excel_path, structured_items)\n            html_structured_path = os.path.join(out_dir, \"tables.html\")\n            write_structured_html(html_structured_path, structured_items)\n\n        print(f\"\u2705 Parsing completed successfully!\")\n        print(f\"\ud83d\udcc1 Output directory: {out_dir}\")\n\n    def display_pages_with_boxes(self, pdf_path: str, num_pages: int = 3, cols: int = 2,\n                                 page_width: int = 800, spacing: int = 40, save_path: str = None) -&gt; None:\n        \"\"\"\n        Display the first N pages of a PDF with bounding boxes and labels overlaid in a modern grid layout.\n\n        Creates a visualization showing layout detection results with bounding boxes,\n        labels, and confidence scores overlaid on the PDF pages in a grid format.\n\n        :param pdf_path: Path to the input PDF file\n        :param num_pages: Number of pages to display (default: 3)\n        :param cols: Number of columns in the grid layout (default: 2)\n        :param page_width: Width to resize each page to in pixels (default: 800)\n        :param spacing: Spacing between pages in pixels (default: 40)\n        :param save_path: Optional path to save the visualization (if None, displays only)\n        :return: None\n        \"\"\"\n        pages: List[LayoutPage] = self.layout_engine.predict_pdf(\n            pdf_path, batch_size=1, layout_nms=True, dpi=self.dpi, min_score=self.min_score\n        )\n        pil_pages = [im for (im, _, _) in render_pdf_to_images(pdf_path, dpi=self.dpi)]\n\n        pages_to_show = min(num_pages, len(pages))\n\n        if pages_to_show == 0:\n            print(\"No pages to display\")\n            return\n\n        rows = (pages_to_show + cols - 1) // cols\n\n        used_labels = set()\n        for idx in range(pages_to_show):\n            page = pages[idx]\n            for box in page.boxes:\n                used_labels.add(box.label.lower())\n\n        base_colors = ['#3B82F6', '#EF4444', '#10B981', '#F59E0B', '#8B5CF6',\n                       '#F97316', '#EC4899', '#6B7280', '#84CC16', '#06B6D4',\n                       '#DC2626', '#059669', '#7C3AED', '#DB2777', '#0891B2']\n\n        dynamic_label_colors = {}\n        for i, label in enumerate(sorted(used_labels)):\n            dynamic_label_colors[label] = base_colors[i % len(base_colors)]\n\n        processed_pages = []\n\n        for idx in range(pages_to_show):\n            page = pages[idx]\n            page_img = pil_pages[idx].copy()\n\n            scale_factor = page_width / page_img.width\n            new_height = int(page_img.height * scale_factor)\n            page_img = page_img.resize((page_width, new_height), Image.LANCZOS)\n\n            draw = ImageDraw.Draw(page_img)\n\n            try:\n                font = ImageFont.truetype(\"arial.ttf\", 24)\n                small_font = ImageFont.truetype(\"arial.ttf\", 18)\n            except:\n                try:\n                    font = ImageFont.load_default()\n                    small_font = ImageFont.load_default()\n                except:\n                    font = None\n                    small_font = None\n\n            for box in page.boxes:\n                x1 = int(box.x1 * scale_factor)\n                y1 = int(box.y1 * scale_factor)\n                x2 = int(box.x2 * scale_factor)\n                y2 = int(box.y2 * scale_factor)\n\n                color = dynamic_label_colors.get(box.label.lower(), '#000000')\n\n                draw.rectangle([x1, y1, x2, y2], outline=color, width=3)\n\n                label_text = f\"{box.label} ({box.score:.2f})\"\n                if font:\n                    bbox = draw.textbbox((0, 0), label_text, font=small_font)\n                    text_width = bbox[2] - bbox[0]\n                    text_height = bbox[3] - bbox[1]\n                else:\n                    text_width = len(label_text) * 8\n                    text_height = 15\n\n                label_x = x1\n                label_y = max(0, y1 - text_height - 8)\n\n                padding = 4\n                draw.rectangle([\n                    label_x - padding,\n                    label_y - padding,\n                    label_x + text_width + padding,\n                    label_y + text_height + padding\n                ], fill='white', outline=color, width=2)\n\n                draw.text((label_x, label_y), label_text, fill=color, font=small_font)\n\n            title_text = f\"Page {page.page_index} ({len(page.boxes)} boxes)\"\n            if font:\n                title_bbox = draw.textbbox((0, 0), title_text, font=font)\n                title_width = title_bbox[2] - title_bbox[0]\n            else:\n                title_width = len(title_text) * 12\n\n            title_x = (page_width - title_width) // 2\n            title_y = 10\n            draw.rectangle([title_x - 10, title_y - 5, title_x + title_width + 10, title_y + 35],\n                           fill='white', outline='#1F2937', width=2)\n            draw.text((title_x, title_y), title_text, fill='#1F2937', font=font)\n\n            processed_pages.append(page_img)\n\n        legend_width = 250\n        grid_width = cols * page_width + (cols - 1) * spacing\n        total_width = grid_width + legend_width + spacing\n        grid_height = rows * (processed_pages[0].height if processed_pages else 600) + (rows - 1) * spacing\n\n        final_img = Image.new('RGB', (total_width, grid_height), '#F8FAFC')\n\n        for idx, page_img in enumerate(processed_pages):\n            row = idx // cols\n            col = idx % cols\n\n            x_pos = col * (page_width + spacing)\n            y_pos = row * (page_img.height + spacing)\n\n            final_img.paste(page_img, (x_pos, y_pos))\n\n        legend_x = grid_width + spacing\n        legend_y = 20\n\n        draw_legend = ImageDraw.Draw(final_img)\n\n        legend_title = \"Element Types\"\n        if font:\n            title_bbox = draw_legend.textbbox((0, 0), legend_title, font=font)\n            title_width = title_bbox[2] - title_bbox[0]\n            title_height = title_bbox[3] - title_bbox[1]\n        else:\n            title_width = len(legend_title) * 12\n            title_height = 20\n\n        legend_bg_height = len(used_labels) * 35 + title_height + 40\n        draw_legend.rectangle([legend_x - 10, legend_y - 10,\n                               legend_x + legend_width - 10, legend_y + legend_bg_height],\n                              fill='white', outline='#E5E7EB', width=2)\n\n        draw_legend.text((legend_x + 10, legend_y + 5), legend_title,\n                         fill='#1F2937', font=font)\n\n        current_y = legend_y + title_height + 20\n\n        for label in sorted(used_labels):\n            color = dynamic_label_colors[label]\n\n            square_size = 20\n            draw_legend.rectangle([legend_x + 10, current_y,\n                                   legend_x + 10 + square_size, current_y + square_size],\n                                  fill=color, outline='#6B7280', width=1)\n\n            draw_legend.text((legend_x + 40, current_y + 2), label.title(),\n                             fill='#374151', font=small_font)\n\n            current_y += 30\n\n        if save_path:\n            final_img.save(save_path, quality=95, optimize=True)\n            print(f\"Layout visualization saved to: {save_path}\")\n        else:\n            final_img.show()\n\n        print(f\"\\n\ud83d\udcca Layout Detection Summary for {os.path.basename(pdf_path)}:\")\n        print(f\"Pages processed: {pages_to_show}\")\n\n        total_counts = {}\n        for idx in range(pages_to_show):\n            page = pages[idx]\n            for box in page.boxes:\n                total_counts[box.label] = total_counts.get(box.label, 0) + 1\n\n        print(\"\\nTotal elements detected:\")\n        for label, count in sorted(total_counts.items()):\n            print(f\"  - {label}: {count}\")\n\n        return final_img\n</code></pre>"},{"location":"api/parsers.html#doctra.parsers.structured_pdf_parser.StructuredPDFParser.__init__","title":"<code>__init__(*, use_vlm=False, vlm_provider='gemini', vlm_model=None, vlm_api_key=None, layout_model_name='PP-DocLayout_plus-L', dpi=200, min_score=0.0, ocr_lang='eng', ocr_psm=4, ocr_oem=3, ocr_extra_config='', box_separator='\\n', merge_split_tables=False, bottom_threshold_ratio=0.2, top_threshold_ratio=0.15, max_gap_ratio=0.25, column_alignment_tolerance=10.0, min_merge_confidence=0.65)</code>","text":"<pre><code>    Initialize the StructuredPDFParser with processing configuration.\n\n    Also suppresses noisy DEBUG logs from external libraries.\n\n    :param use_vlm: Whether to use VLM for structured data extraction (default: False)\n    :param vlm_provider: VLM provider to use (\"gemini\", \"openai\", \"anthropic\", or \"openrouter\", default: \"gemini\")\n    :param vlm_model: Model name to use (defaults to provider-specific defaults)\n    :param vlm_api_key: API key for VLM provider (required if use_vlm is True)\n    :param layout_model_name: Layout detection model name (default: \"PP-DocLayout_plus-L\")\n    :param dpi: DPI for PDF rendering (default: 200)\n    :param min_score: Minimum confidence score for layout detection (default: 0.0)\n    :param ocr_lang: OCR language code (default: \"eng\")\n    :param ocr_psm: Tesseract page segmentation mode (default: 4)\n    :param ocr_oem: Tesseract OCR engine mode (default: 3)\n    :param ocr_extra_config: Additional Tesseract configuration (default: \"\")\n    :param box_separator: Separator between text boxes in output (default: \"\n</code></pre> <p>\")         :param merge_split_tables: Whether to detect and merge split tables (default: False)         :param bottom_threshold_ratio: Ratio for \"too close to bottom\" detection (default: 0.20)         :param top_threshold_ratio: Ratio for \"too close to top\" detection (default: 0.15)         :param max_gap_ratio: Maximum allowed gap between tables (default: 0.25, accounts for headers/footers)         :param column_alignment_tolerance: Pixel tolerance for column alignment (default: 10.0)         :param min_merge_confidence: Minimum confidence score for merging (default: 0.65)</p> Source code in <code>doctra/parsers/structured_pdf_parser.py</code> <pre><code>def __init__(\n        self,\n        *,\n        use_vlm: bool = False,\n        vlm_provider: str = \"gemini\",\n        vlm_model: str | None = None,\n        vlm_api_key: str | None = None,\n        layout_model_name: str = \"PP-DocLayout_plus-L\",\n        dpi: int = 200,\n        min_score: float = 0.0,\n        ocr_lang: str = \"eng\",\n        ocr_psm: int = 4,\n        ocr_oem: int = 3,\n        ocr_extra_config: str = \"\",\n        box_separator: str = \"\\n\",\n        merge_split_tables: bool = False,\n        bottom_threshold_ratio: float = 0.20,\n        top_threshold_ratio: float = 0.15,\n        max_gap_ratio: float = 0.25,\n        column_alignment_tolerance: float = 10.0,\n        min_merge_confidence: float = 0.65,\n):\n    \"\"\"\n    Initialize the StructuredPDFParser with processing configuration.\n\n    Also suppresses noisy DEBUG logs from external libraries.\n\n    :param use_vlm: Whether to use VLM for structured data extraction (default: False)\n    :param vlm_provider: VLM provider to use (\"gemini\", \"openai\", \"anthropic\", or \"openrouter\", default: \"gemini\")\n    :param vlm_model: Model name to use (defaults to provider-specific defaults)\n    :param vlm_api_key: API key for VLM provider (required if use_vlm is True)\n    :param layout_model_name: Layout detection model name (default: \"PP-DocLayout_plus-L\")\n    :param dpi: DPI for PDF rendering (default: 200)\n    :param min_score: Minimum confidence score for layout detection (default: 0.0)\n    :param ocr_lang: OCR language code (default: \"eng\")\n    :param ocr_psm: Tesseract page segmentation mode (default: 4)\n    :param ocr_oem: Tesseract OCR engine mode (default: 3)\n    :param ocr_extra_config: Additional Tesseract configuration (default: \"\")\n    :param box_separator: Separator between text boxes in output (default: \"\\n\")\n    :param merge_split_tables: Whether to detect and merge split tables (default: False)\n    :param bottom_threshold_ratio: Ratio for \"too close to bottom\" detection (default: 0.20)\n    :param top_threshold_ratio: Ratio for \"too close to top\" detection (default: 0.15)\n    :param max_gap_ratio: Maximum allowed gap between tables (default: 0.25, accounts for headers/footers)\n    :param column_alignment_tolerance: Pixel tolerance for column alignment (default: 10.0)\n    :param min_merge_confidence: Minimum confidence score for merging (default: 0.65)\n    \"\"\"\n    self.layout_engine = PaddleLayoutEngine(model_name=layout_model_name)\n    self.dpi = dpi\n    self.min_score = min_score\n    self.ocr_engine = PytesseractOCREngine(\n        lang=ocr_lang, psm=ocr_psm, oem=ocr_oem, extra_config=ocr_extra_config\n    )\n    self.box_separator = box_separator\n    self.use_vlm = use_vlm\n    self.vlm = None\n    if self.use_vlm:\n        try:\n            self.vlm = VLMStructuredExtractor(\n                vlm_provider=vlm_provider,\n                vlm_model=vlm_model,\n                api_key=vlm_api_key,\n            )\n        except Exception as e:\n            self.vlm = None\n\n    # Initialize split table detector\n    self.merge_split_tables = merge_split_tables\n    if self.merge_split_tables:\n        self.split_table_detector = SplitTableDetector(\n            bottom_threshold_ratio=bottom_threshold_ratio,\n            top_threshold_ratio=top_threshold_ratio,\n            max_gap_ratio=max_gap_ratio,\n            column_alignment_tolerance=column_alignment_tolerance,\n            min_merge_confidence=min_merge_confidence,\n        )\n    else:\n        self.split_table_detector = None\n\n    # Suppress noisy DEBUG logs from external libraries\n    logging.getLogger('pytesseract').setLevel(logging.WARNING)\n    logging.getLogger('markdown_it').setLevel(logging.WARNING)\n</code></pre>"},{"location":"api/parsers.html#doctra.parsers.structured_pdf_parser.StructuredPDFParser.display_pages_with_boxes","title":"<code>display_pages_with_boxes(pdf_path, num_pages=3, cols=2, page_width=800, spacing=40, save_path=None)</code>","text":"<p>Display the first N pages of a PDF with bounding boxes and labels overlaid in a modern grid layout.</p> <p>Creates a visualization showing layout detection results with bounding boxes, labels, and confidence scores overlaid on the PDF pages in a grid format.</p> <p>:param pdf_path: Path to the input PDF file :param num_pages: Number of pages to display (default: 3) :param cols: Number of columns in the grid layout (default: 2) :param page_width: Width to resize each page to in pixels (default: 800) :param spacing: Spacing between pages in pixels (default: 40) :param save_path: Optional path to save the visualization (if None, displays only) :return: None</p> Source code in <code>doctra/parsers/structured_pdf_parser.py</code> <pre><code>def display_pages_with_boxes(self, pdf_path: str, num_pages: int = 3, cols: int = 2,\n                             page_width: int = 800, spacing: int = 40, save_path: str = None) -&gt; None:\n    \"\"\"\n    Display the first N pages of a PDF with bounding boxes and labels overlaid in a modern grid layout.\n\n    Creates a visualization showing layout detection results with bounding boxes,\n    labels, and confidence scores overlaid on the PDF pages in a grid format.\n\n    :param pdf_path: Path to the input PDF file\n    :param num_pages: Number of pages to display (default: 3)\n    :param cols: Number of columns in the grid layout (default: 2)\n    :param page_width: Width to resize each page to in pixels (default: 800)\n    :param spacing: Spacing between pages in pixels (default: 40)\n    :param save_path: Optional path to save the visualization (if None, displays only)\n    :return: None\n    \"\"\"\n    pages: List[LayoutPage] = self.layout_engine.predict_pdf(\n        pdf_path, batch_size=1, layout_nms=True, dpi=self.dpi, min_score=self.min_score\n    )\n    pil_pages = [im for (im, _, _) in render_pdf_to_images(pdf_path, dpi=self.dpi)]\n\n    pages_to_show = min(num_pages, len(pages))\n\n    if pages_to_show == 0:\n        print(\"No pages to display\")\n        return\n\n    rows = (pages_to_show + cols - 1) // cols\n\n    used_labels = set()\n    for idx in range(pages_to_show):\n        page = pages[idx]\n        for box in page.boxes:\n            used_labels.add(box.label.lower())\n\n    base_colors = ['#3B82F6', '#EF4444', '#10B981', '#F59E0B', '#8B5CF6',\n                   '#F97316', '#EC4899', '#6B7280', '#84CC16', '#06B6D4',\n                   '#DC2626', '#059669', '#7C3AED', '#DB2777', '#0891B2']\n\n    dynamic_label_colors = {}\n    for i, label in enumerate(sorted(used_labels)):\n        dynamic_label_colors[label] = base_colors[i % len(base_colors)]\n\n    processed_pages = []\n\n    for idx in range(pages_to_show):\n        page = pages[idx]\n        page_img = pil_pages[idx].copy()\n\n        scale_factor = page_width / page_img.width\n        new_height = int(page_img.height * scale_factor)\n        page_img = page_img.resize((page_width, new_height), Image.LANCZOS)\n\n        draw = ImageDraw.Draw(page_img)\n\n        try:\n            font = ImageFont.truetype(\"arial.ttf\", 24)\n            small_font = ImageFont.truetype(\"arial.ttf\", 18)\n        except:\n            try:\n                font = ImageFont.load_default()\n                small_font = ImageFont.load_default()\n            except:\n                font = None\n                small_font = None\n\n        for box in page.boxes:\n            x1 = int(box.x1 * scale_factor)\n            y1 = int(box.y1 * scale_factor)\n            x2 = int(box.x2 * scale_factor)\n            y2 = int(box.y2 * scale_factor)\n\n            color = dynamic_label_colors.get(box.label.lower(), '#000000')\n\n            draw.rectangle([x1, y1, x2, y2], outline=color, width=3)\n\n            label_text = f\"{box.label} ({box.score:.2f})\"\n            if font:\n                bbox = draw.textbbox((0, 0), label_text, font=small_font)\n                text_width = bbox[2] - bbox[0]\n                text_height = bbox[3] - bbox[1]\n            else:\n                text_width = len(label_text) * 8\n                text_height = 15\n\n            label_x = x1\n            label_y = max(0, y1 - text_height - 8)\n\n            padding = 4\n            draw.rectangle([\n                label_x - padding,\n                label_y - padding,\n                label_x + text_width + padding,\n                label_y + text_height + padding\n            ], fill='white', outline=color, width=2)\n\n            draw.text((label_x, label_y), label_text, fill=color, font=small_font)\n\n        title_text = f\"Page {page.page_index} ({len(page.boxes)} boxes)\"\n        if font:\n            title_bbox = draw.textbbox((0, 0), title_text, font=font)\n            title_width = title_bbox[2] - title_bbox[0]\n        else:\n            title_width = len(title_text) * 12\n\n        title_x = (page_width - title_width) // 2\n        title_y = 10\n        draw.rectangle([title_x - 10, title_y - 5, title_x + title_width + 10, title_y + 35],\n                       fill='white', outline='#1F2937', width=2)\n        draw.text((title_x, title_y), title_text, fill='#1F2937', font=font)\n\n        processed_pages.append(page_img)\n\n    legend_width = 250\n    grid_width = cols * page_width + (cols - 1) * spacing\n    total_width = grid_width + legend_width + spacing\n    grid_height = rows * (processed_pages[0].height if processed_pages else 600) + (rows - 1) * spacing\n\n    final_img = Image.new('RGB', (total_width, grid_height), '#F8FAFC')\n\n    for idx, page_img in enumerate(processed_pages):\n        row = idx // cols\n        col = idx % cols\n\n        x_pos = col * (page_width + spacing)\n        y_pos = row * (page_img.height + spacing)\n\n        final_img.paste(page_img, (x_pos, y_pos))\n\n    legend_x = grid_width + spacing\n    legend_y = 20\n\n    draw_legend = ImageDraw.Draw(final_img)\n\n    legend_title = \"Element Types\"\n    if font:\n        title_bbox = draw_legend.textbbox((0, 0), legend_title, font=font)\n        title_width = title_bbox[2] - title_bbox[0]\n        title_height = title_bbox[3] - title_bbox[1]\n    else:\n        title_width = len(legend_title) * 12\n        title_height = 20\n\n    legend_bg_height = len(used_labels) * 35 + title_height + 40\n    draw_legend.rectangle([legend_x - 10, legend_y - 10,\n                           legend_x + legend_width - 10, legend_y + legend_bg_height],\n                          fill='white', outline='#E5E7EB', width=2)\n\n    draw_legend.text((legend_x + 10, legend_y + 5), legend_title,\n                     fill='#1F2937', font=font)\n\n    current_y = legend_y + title_height + 20\n\n    for label in sorted(used_labels):\n        color = dynamic_label_colors[label]\n\n        square_size = 20\n        draw_legend.rectangle([legend_x + 10, current_y,\n                               legend_x + 10 + square_size, current_y + square_size],\n                              fill=color, outline='#6B7280', width=1)\n\n        draw_legend.text((legend_x + 40, current_y + 2), label.title(),\n                         fill='#374151', font=small_font)\n\n        current_y += 30\n\n    if save_path:\n        final_img.save(save_path, quality=95, optimize=True)\n        print(f\"Layout visualization saved to: {save_path}\")\n    else:\n        final_img.show()\n\n    print(f\"\\n\ud83d\udcca Layout Detection Summary for {os.path.basename(pdf_path)}:\")\n    print(f\"Pages processed: {pages_to_show}\")\n\n    total_counts = {}\n    for idx in range(pages_to_show):\n        page = pages[idx]\n        for box in page.boxes:\n            total_counts[box.label] = total_counts.get(box.label, 0) + 1\n\n    print(\"\\nTotal elements detected:\")\n    for label, count in sorted(total_counts.items()):\n        print(f\"  - {label}: {count}\")\n\n    return final_img\n</code></pre>"},{"location":"api/parsers.html#doctra.parsers.structured_pdf_parser.StructuredPDFParser.parse","title":"<code>parse(pdf_path)</code>","text":"<p>Parse a PDF document and extract all content types.</p> <p>:param pdf_path: Path to the input PDF file :return: None</p> Source code in <code>doctra/parsers/structured_pdf_parser.py</code> <pre><code>def parse(self, pdf_path: str) -&gt; None:\n    \"\"\"\n    Parse a PDF document and extract all content types.\n\n    :param pdf_path: Path to the input PDF file\n    :return: None\n    \"\"\"\n    pdf_filename = os.path.splitext(os.path.basename(pdf_path))[0]\n    out_dir = f\"outputs/{pdf_filename}/full_parse\"\n\n    os.makedirs(out_dir, exist_ok=True)\n    ensure_output_dirs(out_dir, IMAGE_SUBDIRS)\n\n    pages: List[LayoutPage] = self.layout_engine.predict_pdf(\n        pdf_path, batch_size=1, layout_nms=True, dpi=self.dpi, min_score=self.min_score\n    )\n    pil_pages = [im for (im, _, _) in render_pdf_to_images(pdf_path, dpi=self.dpi)]\n\n    # Detect split tables before processing\n    split_table_matches: List[SplitTableMatch] = []\n    merged_table_segments = []  # Track TableSegment objects that are merged\n\n    if self.merge_split_tables and self.split_table_detector:\n        try:\n            split_table_matches = self.split_table_detector.detect_split_tables(pages, pil_pages)\n            # Track which segments are part of merges\n            for match in split_table_matches:\n                merged_table_segments.append(match.segment1)\n                merged_table_segments.append(match.segment2)\n        except Exception as e:\n            import traceback\n            traceback.print_exc()\n            split_table_matches = []\n\n    fig_count = sum(sum(1 for b in p.boxes if b.label == \"figure\") for p in pages)\n    chart_count = sum(sum(1 for b in p.boxes if b.label == \"chart\") for p in pages)\n    table_count = sum(sum(1 for b in p.boxes if b.label == \"table\") for p in pages)\n\n    md_lines: List[str] = [\"# Extracted Content\\n\"]\n    html_lines: List[str] = [\"&lt;h1&gt;Extracted Content&lt;/h1&gt;\"]  # For direct HTML generation\n    structured_items: List[Dict[str, Any]] = []\n\n    charts_desc = \"Charts (VLM \u2192 table)\" if self.use_vlm else \"Charts (cropped)\"\n    tables_desc = \"Tables (VLM \u2192 table)\" if self.use_vlm else \"Tables (cropped)\"\n    figures_desc = \"Figures (cropped)\"\n\n    with ExitStack() as stack:\n        is_notebook = \"ipykernel\" in sys.modules or \"jupyter\" in sys.modules\n        is_terminal = hasattr(sys.stdout, 'isatty') and sys.stdout.isatty()\n        if is_notebook:\n            charts_bar = stack.enter_context(\n                create_notebook_friendly_bar(total=chart_count, desc=charts_desc)) if chart_count else None\n            tables_bar = stack.enter_context(\n                create_notebook_friendly_bar(total=table_count, desc=tables_desc)) if table_count else None\n            figures_bar = stack.enter_context(\n                create_notebook_friendly_bar(total=fig_count, desc=figures_desc)) if fig_count else None\n        else:\n            charts_bar = stack.enter_context(\n                create_beautiful_progress_bar(total=chart_count, desc=charts_desc, leave=True)) if chart_count else None\n            tables_bar = stack.enter_context(\n                create_beautiful_progress_bar(total=table_count, desc=tables_desc, leave=True)) if table_count else None\n            figures_bar = stack.enter_context(\n                create_beautiful_progress_bar(total=fig_count, desc=figures_desc, leave=True)) if fig_count else None\n\n        for p in pages:\n            page_num = p.page_index\n            page_img: Image.Image = pil_pages[page_num - 1]\n            md_lines.append(f\"\\n## Page {page_num}\\n\")\n            html_lines.append(f\"&lt;h2&gt;Page {page_num}&lt;/h2&gt;\")\n\n            for i, box in enumerate(sorted(p.boxes, key=reading_order_key), start=1):\n                if box.label in EXCLUDE_LABELS:\n                    img_path = save_box_image(page_img, box, out_dir, page_num, i, IMAGE_SUBDIRS)\n                    abs_img_path = os.path.abspath(img_path)\n                    rel = os.path.relpath(abs_img_path, out_dir)\n\n                    if box.label == \"figure\":\n                        figure_md = f\"![Figure \u2014 page {page_num}]({rel})\\n\"\n                        figure_html = f'&lt;img src=\"{rel}\" alt=\"Figure \u2014 page {page_num}\" /&gt;'\n                        md_lines.append(figure_md)\n                        html_lines.append(figure_html)\n                        if figures_bar: figures_bar.update(1)\n\n                    elif box.label == \"chart\":\n                        if self.use_vlm and self.vlm:\n                            wrote_table = False\n                            try:\n                                chart = self.vlm.extract_chart(abs_img_path)\n                                item = to_structured_dict(chart)\n                                if item:\n                                    # Add page and type information to structured item\n                                    item[\"page\"] = page_num\n                                    item[\"type\"] = \"Chart\"\n                                    structured_items.append(item)\n\n                                    # Generate both markdown and HTML tables\n                                    table_md = render_markdown_table(item.get(\"headers\"), item.get(\"rows\"),\n                                                                     title=item.get(\"title\"))\n                                    table_html = render_html_table(item.get(\"headers\"), item.get(\"rows\"),\n                                                                   title=item.get(\"title\"))\n\n                                    md_lines.append(table_md)\n                                    html_lines.append(table_html)\n                                    wrote_table = True\n                            except Exception as e:\n                                pass\n                            if not wrote_table:\n                                chart_md = f\"![Chart \u2014 page {page_num}]({rel})\\n\"\n                                chart_html = f'&lt;img src=\"{rel}\" alt=\"Chart \u2014 page {page_num}\" /&gt;'\n                                md_lines.append(chart_md)\n                                html_lines.append(chart_html)\n                        else:\n                            chart_md = f\"![Chart \u2014 page {page_num}]({rel})\\n\"\n                            chart_html = f'&lt;img src=\"{rel}\" alt=\"Chart \u2014 page {page_num}\" /&gt;'\n                            md_lines.append(chart_md)\n                            html_lines.append(chart_html)\n                        if charts_bar: charts_bar.update(1)\n\n                    elif box.label == \"table\":\n                        # Skip tables that are part of merged split tables\n                        is_merged = any(seg.match_box(box, page_num) for seg in merged_table_segments)\n                        if is_merged:\n                            continue\n\n                        if self.use_vlm and self.vlm:\n                            wrote_table = False\n                            try:\n                                table = self.vlm.extract_table(abs_img_path)\n                                item = to_structured_dict(table)\n                                if item:\n                                    # Add page and type information to structured item\n                                    item[\"page\"] = page_num\n                                    item[\"type\"] = \"Table\"\n                                    structured_items.append(item)\n\n                                    # Generate both markdown and HTML tables\n                                    table_md = render_markdown_table(item.get(\"headers\"), item.get(\"rows\"),\n                                                                     title=item.get(\"title\"))\n                                    table_html = render_html_table(item.get(\"headers\"), item.get(\"rows\"),\n                                                                   title=item.get(\"title\"))\n\n                                    md_lines.append(table_md)\n                                    html_lines.append(table_html)\n                                    wrote_table = True\n                            except Exception as e:\n                                pass\n                            if not wrote_table:\n                                table_md = f\"![Table \u2014 page {page_num}]({rel})\\n\"\n                                table_html = f'&lt;img src=\"{rel}\" alt=\"Table \u2014 page {page_num}\" /&gt;'\n                                md_lines.append(table_md)\n                                html_lines.append(table_html)\n                        else:\n                            table_md = f\"![Table \u2014 page {page_num}]({rel})\\n\"\n                            table_html = f'&lt;img src=\"{rel}\" alt=\"Table \u2014 page {page_num}\" /&gt;'\n                            md_lines.append(table_md)\n                            html_lines.append(table_html)\n                        if tables_bar: tables_bar.update(1)\n                else:\n                    text = ocr_box_text(self.ocr_engine, page_img, box)\n                    if text:\n                        md_lines.append(text)\n                        md_lines.append(self.box_separator if self.box_separator else \"\")\n                        # Convert text to HTML (basic conversion)\n                        html_text = text.replace('\\n', '&lt;br&gt;')\n                        html_lines.append(f\"&lt;p&gt;{html_text}&lt;/p&gt;\")\n                        if self.box_separator:\n                            html_lines.append(\"&lt;br&gt;\")\n\n        # Process merged split tables\n        if split_table_matches and self.split_table_detector:\n            for match_idx, match in enumerate(split_table_matches):\n                try:\n                    # Merge the table images\n                    merged_img = self.split_table_detector.merge_table_images(match)\n\n                    # Save merged image\n                    tables_dir = os.path.join(out_dir, \"tables\")\n                    os.makedirs(tables_dir, exist_ok=True)\n                    merged_filename = f\"merged_table_{match.segment1.page_index}_{match.segment2.page_index}.png\"\n                    merged_path = os.path.join(tables_dir, merged_filename)\n                    merged_img.save(merged_path)\n\n                    abs_merged_path = os.path.abspath(merged_path)\n                    rel_merged = os.path.relpath(abs_merged_path, out_dir)\n\n                    # Add to markdown/HTML at the page where first segment appears\n                    pages_str = f\"pages {match.segment1.page_index}-{match.segment2.page_index}\"\n\n                    if self.use_vlm and self.vlm:\n                        wrote_table = False\n                        try:\n                            table = self.vlm.extract_table(abs_merged_path)\n                            item = to_structured_dict(table)\n                            if item:\n                                # Add page and type information to structured item\n                                item[\"page\"] = f\"{match.segment1.page_index}-{match.segment2.page_index}\"\n                                item[\"type\"] = \"Table (Merged)\"\n                                item[\"split_merge\"] = True\n                                item[\"merge_confidence\"] = match.confidence\n                                structured_items.append(item)\n\n                                # Generate both markdown and HTML tables\n                                table_md = render_markdown_table(\n                                    item.get(\"headers\"), \n                                    item.get(\"rows\"),\n                                    title=item.get(\"title\") or f\"Merged Table ({pages_str})\"\n                                )\n                                table_html = render_html_table(\n                                    item.get(\"headers\"), \n                                    item.get(\"rows\"),\n                                    title=item.get(\"title\") or f\"Merged Table ({pages_str})\"\n                                )\n\n                                # Insert before the next page section or at end\n                                md_lines.append(f\"\\n### Merged Table ({pages_str})\\n\")\n                                md_lines.append(table_md)\n                                html_lines.append(f'&lt;h3&gt;Merged Table ({pages_str})&lt;/h3&gt;')\n                                html_lines.append(table_html)\n                                wrote_table = True\n                        except Exception as e:\n                            pass\n\n                        if not wrote_table:\n                            table_md = f\"![Merged Table \u2014 {pages_str}]({rel_merged})\\n\"\n                            table_html = f'&lt;img src=\"{rel_merged}\" alt=\"Merged Table \u2014 {pages_str}\" /&gt;'\n                            md_lines.append(f\"\\n### Merged Table ({pages_str})\\n\")\n                            md_lines.append(table_md)\n                            html_lines.append(f'&lt;h3&gt;Merged Table ({pages_str})&lt;/h3&gt;')\n                            html_lines.append(table_html)\n                    else:\n                        table_md = f\"![Merged Table \u2014 {pages_str}]({rel_merged})\\n\"\n                        table_html = f'&lt;img src=\"{rel_merged}\" alt=\"Merged Table \u2014 {pages_str}\" /&gt;'\n                        md_lines.append(f\"\\n### Merged Table ({pages_str})\\n\")\n                        md_lines.append(table_md)\n                        html_lines.append(f'&lt;h3&gt;Merged Table ({pages_str})&lt;/h3&gt;')\n                        html_lines.append(table_html)\n\n                    if tables_bar: tables_bar.update(1)\n\n                except Exception as e:\n                    print(f\"\u26a0\ufe0f  Warning: Failed to merge table {match_idx + 1}: {e}\")\n\n    md_path = write_markdown(md_lines, out_dir)\n\n    # Use HTML lines if VLM is enabled for better table formatting\n    if self.use_vlm and html_lines:\n        html_path = write_html_from_lines(html_lines, out_dir)\n    else:\n        html_path = write_html(md_lines, out_dir)\n\n    excel_path = None\n    html_structured_path = None\n    if self.use_vlm and structured_items:\n        excel_path = os.path.join(out_dir, \"tables.xlsx\")\n        write_structured_excel(excel_path, structured_items)\n        html_structured_path = os.path.join(out_dir, \"tables.html\")\n        write_structured_html(html_structured_path, structured_items)\n\n    print(f\"\u2705 Parsing completed successfully!\")\n    print(f\"\ud83d\udcc1 Output directory: {out_dir}\")\n</code></pre>"},{"location":"api/parsers.html#enhancedpdfparser","title":"EnhancedPDFParser","text":"<p>Enhanced parser with image restoration capabilities.</p>"},{"location":"api/parsers.html#doctra.parsers.enhanced_pdf_parser.EnhancedPDFParser","title":"<code>doctra.parsers.enhanced_pdf_parser.EnhancedPDFParser</code>","text":"<p>               Bases: <code>StructuredPDFParser</code></p> <pre><code>Enhanced PDF Parser with Image Restoration capabilities.\n\nExtends the StructuredPDFParser with DocRes image restoration to improve\ndocument quality before processing. This is particularly useful for:\n- Scanned documents with shadows or distortion\n- Low-quality PDFs that need enhancement\n- Documents with perspective issues\n\n:param use_image_restoration: Whether to apply DocRes image restoration (default: True)\n:param restoration_task: DocRes task to use (\"dewarping\", \"deshadowing\", \"appearance\", \"deblurring\", \"binarization\", \"end2end\", default: \"appearance\")\n:param restoration_device: Device for DocRes processing (\"cuda\", \"cpu\", or None for auto-detect, default: None)\n:param restoration_dpi: DPI for restoration processing (default: 200)\n:param use_vlm: Whether to use VLM for structured data extraction (default: False)\n:param vlm_provider: VLM provider to use (\"gemini\", \"openai\", \"anthropic\", or \"openrouter\", default: \"gemini\")\n:param vlm_model: Model name to use (defaults to provider-specific defaults)\n:param vlm_api_key: API key for VLM provider (required if use_vlm is True)\n:param layout_model_name: Layout detection model name (default: \"PP-DocLayout_plus-L\")\n:param dpi: DPI for PDF rendering (default: 200)\n:param min_score: Minimum confidence score for layout detection (default: 0.0)\n:param ocr_lang: OCR language code (default: \"eng\")\n:param ocr_psm: Tesseract page segmentation mode (default: 4)\n:param ocr_oem: Tesseract OCR engine mode (default: 3)\n:param ocr_extra_config: Additional Tesseract configuration (default: \"\")\n:param box_separator: Separator between text boxes in output (default: \"\n</code></pre> <p>\")</p> Source code in <code>doctra/parsers/enhanced_pdf_parser.py</code> <pre><code>class EnhancedPDFParser(StructuredPDFParser):\n    \"\"\"\n    Enhanced PDF Parser with Image Restoration capabilities.\n\n    Extends the StructuredPDFParser with DocRes image restoration to improve\n    document quality before processing. This is particularly useful for:\n    - Scanned documents with shadows or distortion\n    - Low-quality PDFs that need enhancement\n    - Documents with perspective issues\n\n    :param use_image_restoration: Whether to apply DocRes image restoration (default: True)\n    :param restoration_task: DocRes task to use (\"dewarping\", \"deshadowing\", \"appearance\", \"deblurring\", \"binarization\", \"end2end\", default: \"appearance\")\n    :param restoration_device: Device for DocRes processing (\"cuda\", \"cpu\", or None for auto-detect, default: None)\n    :param restoration_dpi: DPI for restoration processing (default: 200)\n    :param use_vlm: Whether to use VLM for structured data extraction (default: False)\n    :param vlm_provider: VLM provider to use (\"gemini\", \"openai\", \"anthropic\", or \"openrouter\", default: \"gemini\")\n    :param vlm_model: Model name to use (defaults to provider-specific defaults)\n    :param vlm_api_key: API key for VLM provider (required if use_vlm is True)\n    :param layout_model_name: Layout detection model name (default: \"PP-DocLayout_plus-L\")\n    :param dpi: DPI for PDF rendering (default: 200)\n    :param min_score: Minimum confidence score for layout detection (default: 0.0)\n    :param ocr_lang: OCR language code (default: \"eng\")\n    :param ocr_psm: Tesseract page segmentation mode (default: 4)\n    :param ocr_oem: Tesseract OCR engine mode (default: 3)\n    :param ocr_extra_config: Additional Tesseract configuration (default: \"\")\n    :param box_separator: Separator between text boxes in output (default: \"\\n\")\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        use_image_restoration: bool = True,\n        restoration_task: str = \"appearance\",\n        restoration_device: Optional[str] = None,\n        restoration_dpi: int = 200,\n        use_vlm: bool = False,\n        vlm_provider: str = \"gemini\",\n        vlm_model: str | None = None,\n        vlm_api_key: str | None = None,\n        layout_model_name: str = \"PP-DocLayout_plus-L\",\n        dpi: int = 200,\n        min_score: float = 0.0,\n        ocr_lang: str = \"eng\",\n        ocr_psm: int = 4,\n        ocr_oem: int = 3,\n        ocr_extra_config: str = \"\",\n        box_separator: str = \"\\n\",\n    ):\n        \"\"\"\n        Initialize the Enhanced PDF Parser with image restoration capabilities.\n        \"\"\"\n        # Initialize parent class\n        super().__init__(\n            use_vlm=use_vlm,\n            vlm_provider=vlm_provider,\n            vlm_model=vlm_model,\n            vlm_api_key=vlm_api_key,\n            layout_model_name=layout_model_name,\n            dpi=dpi,\n            min_score=min_score,\n            ocr_lang=ocr_lang,\n            ocr_psm=ocr_psm,\n            ocr_oem=ocr_oem,\n            ocr_extra_config=ocr_extra_config,\n            box_separator=box_separator,\n        )\n\n        # Image restoration settings\n        self.use_image_restoration = use_image_restoration\n        self.restoration_task = restoration_task\n        self.restoration_device = restoration_device\n        self.restoration_dpi = restoration_dpi\n\n        # Initialize DocRes engine if needed\n        self.docres_engine = None\n        if self.use_image_restoration:\n            try:\n                self.docres_engine = DocResEngine(\n                    device=restoration_device,\n                    use_half_precision=True\n                )\n                print(f\"\u2705 DocRes engine initialized with task: {restoration_task}\")\n            except Exception as e:\n                print(f\"\u26a0\ufe0f DocRes initialization failed: {e}\")\n                print(\"   Continuing without image restoration...\")\n                self.use_image_restoration = False\n                self.docres_engine = None\n\n    def parse(self, pdf_path: str, enhanced_output_dir: str = None) -&gt; None:\n        \"\"\"\n        Parse a PDF document with optional image restoration.\n\n        :param pdf_path: Path to the input PDF file\n        :param enhanced_output_dir: Directory for enhanced images (if None, uses default)\n        :return: None\n        \"\"\"\n        pdf_filename = os.path.splitext(os.path.basename(pdf_path))[0]\n\n        # Set up output directories\n        if enhanced_output_dir is None:\n            out_dir = f\"outputs/{pdf_filename}/enhanced_parse\"\n        else:\n            out_dir = enhanced_output_dir\n\n        os.makedirs(out_dir, exist_ok=True)\n        ensure_output_dirs(out_dir, IMAGE_SUBDIRS)\n\n        # Process PDF pages with optional restoration\n        if self.use_image_restoration and self.docres_engine:\n            print(f\"\ud83d\udd04 Processing PDF with image restoration: {os.path.basename(pdf_path)}\")\n            enhanced_pages = self._process_pages_with_restoration(pdf_path, out_dir)\n\n            # Create enhanced PDF file using the already processed enhanced pages\n            enhanced_pdf_path = os.path.join(out_dir, f\"{pdf_filename}_enhanced.pdf\")\n            try:\n                self._create_enhanced_pdf_from_pages(enhanced_pages, enhanced_pdf_path)\n            except Exception as e:\n                print(f\"\u26a0\ufe0f Failed to create enhanced PDF: {e}\")\n        else:\n            print(f\"\ud83d\udd04 Processing PDF without image restoration: {os.path.basename(pdf_path)}\")\n            enhanced_pages = [im for (im, _, _) in render_pdf_to_images(pdf_path, dpi=self.dpi)]\n\n        # Run layout detection on enhanced pages\n        print(\"\ud83d\udd0d Running layout detection on enhanced pages...\")\n        pages = self.layout_engine.predict_pdf(\n            pdf_path, batch_size=1, layout_nms=True, dpi=self.dpi, min_score=self.min_score\n        )\n\n        # Use enhanced pages for processing\n        pil_pages = enhanced_pages\n\n        # Continue with standard parsing logic\n        self._process_parsing_logic(pages, pil_pages, out_dir, pdf_filename, pdf_path)\n\n    def _process_pages_with_restoration(self, pdf_path: str, out_dir: str) -&gt; List[Image.Image]:\n        \"\"\"\n        Process PDF pages with DocRes image restoration.\n\n        :param pdf_path: Path to the input PDF file\n        :param out_dir: Output directory for enhanced images\n        :return: List of enhanced PIL images\n        \"\"\"\n        # Render original pages\n        original_pages = [im for (im, _, _) in render_pdf_to_images(pdf_path, dpi=self.restoration_dpi)]\n\n        if not original_pages:\n            print(\"\u274c No pages found in PDF\")\n            return []\n\n        # Create progress bar\n        is_notebook = \"ipykernel\" in sys.modules or \"jupyter\" in sys.modules\n        if is_notebook:\n            progress_bar = create_notebook_friendly_bar(\n                total=len(original_pages), \n                desc=f\"DocRes {self.restoration_task}\"\n            )\n        else:\n            progress_bar = create_beautiful_progress_bar(\n                total=len(original_pages), \n                desc=f\"DocRes {self.restoration_task}\",\n                leave=True\n            )\n\n        enhanced_pages = []\n        enhanced_dir = os.path.join(out_dir, \"enhanced_pages\")\n        os.makedirs(enhanced_dir, exist_ok=True)\n\n        try:\n            with progress_bar:\n                for i, page_img in enumerate(original_pages):\n                    try:\n                        # Convert PIL to numpy array\n                        img_array = np.array(page_img)\n\n                        # Apply DocRes restoration\n                        restored_img, metadata = self.docres_engine.restore_image(\n                            img_array, \n                            task=self.restoration_task\n                        )\n\n                        # Convert back to PIL Image\n                        enhanced_page = Image.fromarray(restored_img)\n                        enhanced_pages.append(enhanced_page)\n\n                        # Save enhanced page for reference\n                        enhanced_path = os.path.join(enhanced_dir, f\"page_{i+1:03d}_enhanced.jpg\")\n                        enhanced_page.save(enhanced_path, \"JPEG\", quality=95)\n\n                        progress_bar.set_description(f\"\u2705 Page {i+1}/{len(original_pages)} enhanced\")\n                        progress_bar.update(1)\n\n                    except Exception as e:\n                        print(f\"  \u26a0\ufe0f Page {i+1} restoration failed: {e}, using original\")\n                        enhanced_pages.append(page_img)\n                        progress_bar.set_description(f\"\u26a0\ufe0f Page {i+1} failed, using original\")\n                        progress_bar.update(1)\n\n        finally:\n            if hasattr(progress_bar, 'close'):\n                progress_bar.close()\n\n        return enhanced_pages\n\n    def _process_parsing_logic(self, pages, pil_pages, out_dir, pdf_filename, pdf_path):\n        \"\"\"\n        Process the parsing logic with enhanced pages.\n        This is extracted from the parent class to allow customization.\n        \"\"\"\n\n        fig_count = sum(sum(1 for b in p.boxes if b.label == \"figure\") for p in pages)\n        chart_count = sum(sum(1 for b in p.boxes if b.label == \"chart\") for p in pages)\n        table_count = sum(sum(1 for b in p.boxes if b.label == \"table\") for p in pages)\n\n        md_lines: List[str] = [\"# Enhanced Document Content\\n\"]\n        html_lines: List[str] = [\"&lt;h1&gt;Enhanced Document Content&lt;/h1&gt;\"]  # For direct HTML generation\n        structured_items: List[Dict[str, Any]] = []\n        page_content: Dict[int, List[str]] = {}  # Store content by page\n\n        charts_desc = \"Charts (VLM \u2192 table)\" if self.use_vlm else \"Charts (cropped)\"\n        tables_desc = \"Tables (VLM \u2192 table)\" if self.use_vlm else \"Tables (cropped)\"\n        figures_desc = \"Figures (cropped)\"\n\n        with ExitStack() as stack:\n            is_notebook = \"ipykernel\" in sys.modules or \"jupyter\" in sys.modules\n            if is_notebook:\n                charts_bar = stack.enter_context(\n                    create_notebook_friendly_bar(total=chart_count, desc=charts_desc)) if chart_count else None\n                tables_bar = stack.enter_context(\n                    create_notebook_friendly_bar(total=table_count, desc=tables_desc)) if table_count else None\n                figures_bar = stack.enter_context(\n                    create_notebook_friendly_bar(total=fig_count, desc=figures_desc)) if fig_count else None\n            else:\n                charts_bar = stack.enter_context(\n                    create_beautiful_progress_bar(total=chart_count, desc=charts_desc, leave=True)) if chart_count else None\n                tables_bar = stack.enter_context(\n                    create_beautiful_progress_bar(total=table_count, desc=tables_desc, leave=True)) if table_count else None\n                figures_bar = stack.enter_context(\n                    create_beautiful_progress_bar(total=fig_count, desc=figures_desc, leave=True)) if fig_count else None\n\n            # Initialize page content for all pages first\n            for page_num in range(1, len(pil_pages) + 1):\n                page_content[page_num] = [f\"# Page {page_num} Content\\n\"]\n\n            for p in pages:\n                page_num = p.page_index\n                page_img: Image.Image = pil_pages[page_num - 1]\n                md_lines.append(f\"\\n## Page {page_num}\\n\")\n                html_lines.append(f\"&lt;h2&gt;Page {page_num}&lt;/h2&gt;\")\n\n                for i, box in enumerate(sorted(p.boxes, key=reading_order_key), start=1):\n                    if box.label in EXCLUDE_LABELS:\n                        img_path = save_box_image(page_img, box, out_dir, page_num, i, IMAGE_SUBDIRS)\n                        abs_img_path = os.path.abspath(img_path)\n                        rel = os.path.relpath(abs_img_path, out_dir)\n\n                        if box.label == \"figure\":\n                            figure_md = f\"![Figure \u2014 page {page_num}]({rel})\\n\"\n                            figure_html = f'&lt;img src=\"{rel}\" alt=\"Figure \u2014 page {page_num}\" /&gt;'\n                            md_lines.append(figure_md)\n                            html_lines.append(figure_html)\n                            page_content[page_num].append(figure_md)\n                            if figures_bar: figures_bar.update(1)\n\n                        elif box.label == \"chart\":\n                            if self.use_vlm and self.vlm:\n                                wrote_table = False\n                                try:\n                                    chart = self.vlm.extract_chart(abs_img_path)\n                                    item = to_structured_dict(chart)\n                                    if item:\n                                        # Add page and type information to structured item\n                                        item[\"page\"] = page_num\n                                        item[\"type\"] = \"Chart\"\n                                        structured_items.append(item)\n\n                                        # Generate both markdown and HTML tables\n                                        table_md = render_markdown_table(item.get(\"headers\"), item.get(\"rows\"),\n                                                                         title=item.get(\"title\"))\n                                        table_html = render_html_table(item.get(\"headers\"), item.get(\"rows\"),\n                                                                       title=item.get(\"title\"))\n\n                                        md_lines.append(table_md)\n                                        html_lines.append(table_html)\n                                        page_content[page_num].append(table_md)\n                                        wrote_table = True\n                                except Exception as e:\n                                    pass\n                                if not wrote_table:\n                                    chart_md = f\"![Chart \u2014 page {page_num}]({rel})\\n\"\n                                    chart_html = f'&lt;img src=\"{rel}\" alt=\"Chart \u2014 page {page_num}\" /&gt;'\n                                    md_lines.append(chart_md)\n                                    html_lines.append(chart_html)\n                                    page_content[page_num].append(chart_md)\n                            else:\n                                chart_md = f\"![Chart \u2014 page {page_num}]({rel})\\n\"\n                                chart_html = f'&lt;img src=\"{rel}\" alt=\"Chart \u2014 page {page_num}\" /&gt;'\n                                md_lines.append(chart_md)\n                                html_lines.append(chart_html)\n                                page_content[page_num].append(chart_md)\n                            if charts_bar: charts_bar.update(1)\n\n                        elif box.label == \"table\":\n                            if self.use_vlm and self.vlm:\n                                wrote_table = False\n                                try:\n                                    table = self.vlm.extract_table(abs_img_path)\n                                    item = to_structured_dict(table)\n                                    if item:\n                                        # Add page and type information to structured item\n                                        item[\"page\"] = page_num\n                                        item[\"type\"] = \"Table\"\n                                        structured_items.append(item)\n\n                                        # Generate both markdown and HTML tables\n                                        table_md = render_markdown_table(item.get(\"headers\"), item.get(\"rows\"),\n                                                                         title=item.get(\"title\"))\n                                        table_html = render_html_table(item.get(\"headers\"), item.get(\"rows\"),\n                                                                       title=item.get(\"title\"))\n\n                                        md_lines.append(table_md)\n                                        html_lines.append(table_html)\n                                        page_content[page_num].append(table_md)\n                                        wrote_table = True\n                                except Exception as e:\n                                    pass\n                                if not wrote_table:\n                                    table_md = f\"![Table \u2014 page {page_num}]({rel})\\n\"\n                                    table_html = f'&lt;img src=\"{rel}\" alt=\"Table \u2014 page {page_num}\" /&gt;'\n                                    md_lines.append(table_md)\n                                    html_lines.append(table_html)\n                                    page_content[page_num].append(table_md)\n                            else:\n                                table_md = f\"![Table \u2014 page {page_num}]({rel})\\n\"\n                                table_html = f'&lt;img src=\"{rel}\" alt=\"Table \u2014 page {page_num}\" /&gt;'\n                                md_lines.append(table_md)\n                                html_lines.append(table_html)\n                                page_content[page_num].append(table_md)\n                            if tables_bar: tables_bar.update(1)\n                    else:\n                        text = ocr_box_text(self.ocr_engine, page_img, box)\n                        if text:\n                            md_lines.append(text)\n                            md_lines.append(self.box_separator if self.box_separator else \"\")\n                            # Convert text to HTML (basic conversion)\n                            html_text = text.replace('\\n', '&lt;br&gt;')\n                            html_lines.append(f\"&lt;p&gt;{html_text}&lt;/p&gt;\")\n                            if self.box_separator:\n                                html_lines.append(\"&lt;br&gt;\")\n                            page_content[page_num].append(text)\n                            page_content[page_num].append(self.box_separator if self.box_separator else \"\")\n\n        md_path = write_markdown(md_lines, out_dir)\n\n        # Use HTML lines if VLM is enabled for better table formatting\n        if self.use_vlm and html_lines:\n            html_path = write_html_from_lines(html_lines, out_dir)\n        else:\n            html_path = write_html(md_lines, out_dir)\n\n        # Create pages folder and save individual page markdown files\n        pages_dir = os.path.join(out_dir, \"pages\")\n        os.makedirs(pages_dir, exist_ok=True)\n\n        for page_num, content_lines in page_content.items():\n            page_md_path = os.path.join(pages_dir, f\"page_{page_num:03d}.md\")\n            write_markdown(content_lines, os.path.dirname(page_md_path), os.path.basename(page_md_path))\n\n        excel_path = None\n        html_structured_path = None\n        if self.use_vlm and structured_items:\n            excel_path = os.path.join(out_dir, \"tables.xlsx\")\n            write_structured_excel(excel_path, structured_items)\n            html_structured_path = os.path.join(out_dir, \"tables.html\")\n            write_structured_html(html_structured_path, structured_items)\n\n        print(f\"\u2705 Enhanced parsing completed successfully!\")\n        print(f\"\ud83d\udcc1 Output directory: {out_dir}\")\n\n    def _create_enhanced_pdf_from_pages(self, enhanced_pages: List[Image.Image], output_path: str) -&gt; None:\n        \"\"\"\n        Create an enhanced PDF from already processed enhanced pages.\n\n        :param enhanced_pages: List of enhanced PIL images\n        :param output_path: Path for the enhanced PDF\n        \"\"\"\n        if not enhanced_pages:\n            raise ValueError(\"No enhanced pages provided\")\n\n        try:\n            # Create enhanced PDF from the processed pages\n            enhanced_pages[0].save(\n                output_path,\n                \"PDF\",\n                resolution=100.0,\n                save_all=True,\n                append_images=enhanced_pages[1:] if len(enhanced_pages) &gt; 1 else []\n            )\n            print(f\"\u2705 Enhanced PDF saved from processed pages: {output_path}\")\n        except Exception as e:\n            print(f\"\u274c Error creating enhanced PDF from pages: {e}\")\n            raise\n\n    def restore_pdf_only(self, pdf_path: str, output_path: str = None, task: str = None) -&gt; str:\n        \"\"\"\n        Apply DocRes restoration to a PDF without parsing.\n\n        :param pdf_path: Path to the input PDF file\n        :param output_path: Path for the enhanced PDF (if None, auto-generates)\n        :param task: DocRes restoration task (if None, uses instance default)\n        :return: Path to the enhanced PDF or None if failed\n        \"\"\"\n        if not self.use_image_restoration or not self.docres_engine:\n            raise RuntimeError(\"Image restoration is not enabled or DocRes engine is not available\")\n\n        task = task or self.restoration_task\n        return self.docres_engine.restore_pdf(pdf_path, output_path, task, self.restoration_dpi)\n\n    def get_restoration_info(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get information about the current restoration configuration.\n\n        :return: Dictionary with restoration settings and status\n        \"\"\"\n        return {\n            'enabled': self.use_image_restoration,\n            'task': self.restoration_task,\n            'device': self.restoration_device,\n            'dpi': self.restoration_dpi,\n            'engine_available': self.docres_engine is not None,\n            'supported_tasks': self.docres_engine.get_supported_tasks() if self.docres_engine else []\n        }\n</code></pre>"},{"location":"api/parsers.html#doctra.parsers.enhanced_pdf_parser.EnhancedPDFParser.__init__","title":"<code>__init__(*, use_image_restoration=True, restoration_task='appearance', restoration_device=None, restoration_dpi=200, use_vlm=False, vlm_provider='gemini', vlm_model=None, vlm_api_key=None, layout_model_name='PP-DocLayout_plus-L', dpi=200, min_score=0.0, ocr_lang='eng', ocr_psm=4, ocr_oem=3, ocr_extra_config='', box_separator='\\n')</code>","text":"<p>Initialize the Enhanced PDF Parser with image restoration capabilities.</p> Source code in <code>doctra/parsers/enhanced_pdf_parser.py</code> <pre><code>def __init__(\n    self,\n    *,\n    use_image_restoration: bool = True,\n    restoration_task: str = \"appearance\",\n    restoration_device: Optional[str] = None,\n    restoration_dpi: int = 200,\n    use_vlm: bool = False,\n    vlm_provider: str = \"gemini\",\n    vlm_model: str | None = None,\n    vlm_api_key: str | None = None,\n    layout_model_name: str = \"PP-DocLayout_plus-L\",\n    dpi: int = 200,\n    min_score: float = 0.0,\n    ocr_lang: str = \"eng\",\n    ocr_psm: int = 4,\n    ocr_oem: int = 3,\n    ocr_extra_config: str = \"\",\n    box_separator: str = \"\\n\",\n):\n    \"\"\"\n    Initialize the Enhanced PDF Parser with image restoration capabilities.\n    \"\"\"\n    # Initialize parent class\n    super().__init__(\n        use_vlm=use_vlm,\n        vlm_provider=vlm_provider,\n        vlm_model=vlm_model,\n        vlm_api_key=vlm_api_key,\n        layout_model_name=layout_model_name,\n        dpi=dpi,\n        min_score=min_score,\n        ocr_lang=ocr_lang,\n        ocr_psm=ocr_psm,\n        ocr_oem=ocr_oem,\n        ocr_extra_config=ocr_extra_config,\n        box_separator=box_separator,\n    )\n\n    # Image restoration settings\n    self.use_image_restoration = use_image_restoration\n    self.restoration_task = restoration_task\n    self.restoration_device = restoration_device\n    self.restoration_dpi = restoration_dpi\n\n    # Initialize DocRes engine if needed\n    self.docres_engine = None\n    if self.use_image_restoration:\n        try:\n            self.docres_engine = DocResEngine(\n                device=restoration_device,\n                use_half_precision=True\n            )\n            print(f\"\u2705 DocRes engine initialized with task: {restoration_task}\")\n        except Exception as e:\n            print(f\"\u26a0\ufe0f DocRes initialization failed: {e}\")\n            print(\"   Continuing without image restoration...\")\n            self.use_image_restoration = False\n            self.docres_engine = None\n</code></pre>"},{"location":"api/parsers.html#doctra.parsers.enhanced_pdf_parser.EnhancedPDFParser.get_restoration_info","title":"<code>get_restoration_info()</code>","text":"<p>Get information about the current restoration configuration.</p> <p>:return: Dictionary with restoration settings and status</p> Source code in <code>doctra/parsers/enhanced_pdf_parser.py</code> <pre><code>def get_restoration_info(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get information about the current restoration configuration.\n\n    :return: Dictionary with restoration settings and status\n    \"\"\"\n    return {\n        'enabled': self.use_image_restoration,\n        'task': self.restoration_task,\n        'device': self.restoration_device,\n        'dpi': self.restoration_dpi,\n        'engine_available': self.docres_engine is not None,\n        'supported_tasks': self.docres_engine.get_supported_tasks() if self.docres_engine else []\n    }\n</code></pre>"},{"location":"api/parsers.html#doctra.parsers.enhanced_pdf_parser.EnhancedPDFParser.parse","title":"<code>parse(pdf_path, enhanced_output_dir=None)</code>","text":"<p>Parse a PDF document with optional image restoration.</p> <p>:param pdf_path: Path to the input PDF file :param enhanced_output_dir: Directory for enhanced images (if None, uses default) :return: None</p> Source code in <code>doctra/parsers/enhanced_pdf_parser.py</code> <pre><code>def parse(self, pdf_path: str, enhanced_output_dir: str = None) -&gt; None:\n    \"\"\"\n    Parse a PDF document with optional image restoration.\n\n    :param pdf_path: Path to the input PDF file\n    :param enhanced_output_dir: Directory for enhanced images (if None, uses default)\n    :return: None\n    \"\"\"\n    pdf_filename = os.path.splitext(os.path.basename(pdf_path))[0]\n\n    # Set up output directories\n    if enhanced_output_dir is None:\n        out_dir = f\"outputs/{pdf_filename}/enhanced_parse\"\n    else:\n        out_dir = enhanced_output_dir\n\n    os.makedirs(out_dir, exist_ok=True)\n    ensure_output_dirs(out_dir, IMAGE_SUBDIRS)\n\n    # Process PDF pages with optional restoration\n    if self.use_image_restoration and self.docres_engine:\n        print(f\"\ud83d\udd04 Processing PDF with image restoration: {os.path.basename(pdf_path)}\")\n        enhanced_pages = self._process_pages_with_restoration(pdf_path, out_dir)\n\n        # Create enhanced PDF file using the already processed enhanced pages\n        enhanced_pdf_path = os.path.join(out_dir, f\"{pdf_filename}_enhanced.pdf\")\n        try:\n            self._create_enhanced_pdf_from_pages(enhanced_pages, enhanced_pdf_path)\n        except Exception as e:\n            print(f\"\u26a0\ufe0f Failed to create enhanced PDF: {e}\")\n    else:\n        print(f\"\ud83d\udd04 Processing PDF without image restoration: {os.path.basename(pdf_path)}\")\n        enhanced_pages = [im for (im, _, _) in render_pdf_to_images(pdf_path, dpi=self.dpi)]\n\n    # Run layout detection on enhanced pages\n    print(\"\ud83d\udd0d Running layout detection on enhanced pages...\")\n    pages = self.layout_engine.predict_pdf(\n        pdf_path, batch_size=1, layout_nms=True, dpi=self.dpi, min_score=self.min_score\n    )\n\n    # Use enhanced pages for processing\n    pil_pages = enhanced_pages\n\n    # Continue with standard parsing logic\n    self._process_parsing_logic(pages, pil_pages, out_dir, pdf_filename, pdf_path)\n</code></pre>"},{"location":"api/parsers.html#doctra.parsers.enhanced_pdf_parser.EnhancedPDFParser.restore_pdf_only","title":"<code>restore_pdf_only(pdf_path, output_path=None, task=None)</code>","text":"<p>Apply DocRes restoration to a PDF without parsing.</p> <p>:param pdf_path: Path to the input PDF file :param output_path: Path for the enhanced PDF (if None, auto-generates) :param task: DocRes restoration task (if None, uses instance default) :return: Path to the enhanced PDF or None if failed</p> Source code in <code>doctra/parsers/enhanced_pdf_parser.py</code> <pre><code>def restore_pdf_only(self, pdf_path: str, output_path: str = None, task: str = None) -&gt; str:\n    \"\"\"\n    Apply DocRes restoration to a PDF without parsing.\n\n    :param pdf_path: Path to the input PDF file\n    :param output_path: Path for the enhanced PDF (if None, auto-generates)\n    :param task: DocRes restoration task (if None, uses instance default)\n    :return: Path to the enhanced PDF or None if failed\n    \"\"\"\n    if not self.use_image_restoration or not self.docres_engine:\n        raise RuntimeError(\"Image restoration is not enabled or DocRes engine is not available\")\n\n    task = task or self.restoration_task\n    return self.docres_engine.restore_pdf(pdf_path, output_path, task, self.restoration_dpi)\n</code></pre>"},{"location":"api/parsers.html#charttablepdfparser","title":"ChartTablePDFParser","text":"<p>Specialized parser for extracting charts and tables.</p>"},{"location":"api/parsers.html#doctra.parsers.table_chart_extractor.ChartTablePDFParser","title":"<code>doctra.parsers.table_chart_extractor.ChartTablePDFParser</code>","text":"<p>Specialized PDF parser for extracting charts and tables.</p> <p>Focuses specifically on chart and table extraction from PDF documents, with optional VLM (Vision Language Model) processing to convert visual elements into structured data.</p> <p>:param extract_charts: Whether to extract charts from the document (default: True) :param extract_tables: Whether to extract tables from the document (default: True) :param use_vlm: Whether to use VLM for structured data extraction (default: False) :param vlm_provider: VLM provider to use (\"gemini\", \"openai\", \"anthropic\", or \"openrouter\", default: \"gemini\") :param vlm_model: Model name to use (defaults to provider-specific defaults) :param vlm_api_key: API key for VLM provider (required if use_vlm is True) :param layout_model_name: Layout detection model name (default: \"PP-DocLayout_plus-L\") :param dpi: DPI for PDF rendering (default: 200) :param min_score: Minimum confidence score for layout detection (default: 0.0)</p> Source code in <code>doctra/parsers/table_chart_extractor.py</code> <pre><code>class ChartTablePDFParser:\n    \"\"\"\n    Specialized PDF parser for extracting charts and tables.\n\n    Focuses specifically on chart and table extraction from PDF documents,\n    with optional VLM (Vision Language Model) processing to convert visual\n    elements into structured data.\n\n    :param extract_charts: Whether to extract charts from the document (default: True)\n    :param extract_tables: Whether to extract tables from the document (default: True)\n    :param use_vlm: Whether to use VLM for structured data extraction (default: False)\n    :param vlm_provider: VLM provider to use (\"gemini\", \"openai\", \"anthropic\", or \"openrouter\", default: \"gemini\")\n    :param vlm_model: Model name to use (defaults to provider-specific defaults)\n    :param vlm_api_key: API key for VLM provider (required if use_vlm is True)\n    :param layout_model_name: Layout detection model name (default: \"PP-DocLayout_plus-L\")\n    :param dpi: DPI for PDF rendering (default: 200)\n    :param min_score: Minimum confidence score for layout detection (default: 0.0)\n    \"\"\"\n\n    def __init__(\n            self,\n            *,\n            extract_charts: bool = True,\n            extract_tables: bool = True,\n            use_vlm: bool = False,\n            vlm_provider: str = \"gemini\",\n            vlm_model: str | None = None,\n            vlm_api_key: str | None = None,\n            layout_model_name: str = \"PP-DocLayout_plus-L\",\n            dpi: int = 200,\n            min_score: float = 0.0,\n    ):\n        \"\"\"\n        Initialize the ChartTablePDFParser with extraction configuration.\n\n        :param extract_charts: Whether to extract charts from the document (default: True)\n        :param extract_tables: Whether to extract tables from the document (default: True)\n        :param use_vlm: Whether to use VLM for structured data extraction (default: False)\n        :param vlm_provider: VLM provider to use (\"gemini\", \"openai\", \"anthropic\", or \"openrouter\", default: \"gemini\")\n        :param vlm_model: Model name to use (defaults to provider-specific defaults)\n        :param vlm_api_key: API key for VLM provider (required if use_vlm is True)\n        :param layout_model_name: Layout detection model name (default: \"PP-DocLayout_plus-L\")\n        :param dpi: DPI for PDF rendering (default: 200)\n        :param min_score: Minimum confidence score for layout detection (default: 0.0)\n        \"\"\"\n        if not extract_charts and not extract_tables:\n            raise ValueError(\"At least one of extract_charts or extract_tables must be True\")\n\n        self.extract_charts = extract_charts\n        self.extract_tables = extract_tables\n        self.layout_engine = PaddleLayoutEngine(model_name=layout_model_name)\n        self.dpi = dpi\n        self.min_score = min_score\n\n        self.use_vlm = use_vlm\n        self.vlm = None\n        if self.use_vlm:\n            self.vlm = VLMStructuredExtractor(\n                vlm_provider=vlm_provider,\n                vlm_model=vlm_model,\n                api_key=vlm_api_key,\n            )\n\n    def parse(self, pdf_path: str, output_base_dir: str = \"outputs\") -&gt; None:\n        \"\"\"\n        Parse a PDF document and extract charts and/or tables.\n\n        :param pdf_path: Path to the input PDF file\n        :param output_base_dir: Base directory for output files (default: \"outputs\")\n        :return: None\n        \"\"\"\n        pdf_name = Path(pdf_path).stem\n        out_dir = os.path.join(output_base_dir, pdf_name, \"structured_parsing\")\n        os.makedirs(out_dir, exist_ok=True)\n\n        charts_dir = None\n        tables_dir = None\n\n        if self.extract_charts:\n            charts_dir = os.path.join(out_dir, \"charts\")\n            os.makedirs(charts_dir, exist_ok=True)\n\n        if self.extract_tables:\n            tables_dir = os.path.join(out_dir, \"tables\")\n            os.makedirs(tables_dir, exist_ok=True)\n\n        pages: List[LayoutPage] = self.layout_engine.predict_pdf(\n            pdf_path, batch_size=1, layout_nms=True, dpi=self.dpi, min_score=self.min_score\n        )\n        pil_pages = [im for (im, _, _) in render_pdf_to_images(pdf_path, dpi=self.dpi)]\n\n        target_labels = []\n        if self.extract_charts:\n            target_labels.append(\"chart\")\n        if self.extract_tables:\n            target_labels.append(\"table\")\n\n        chart_count = sum(sum(1 for b in p.boxes if b.label == \"chart\") for p in pages) if self.extract_charts else 0\n        table_count = sum(sum(1 for b in p.boxes if b.label == \"table\") for p in pages) if self.extract_tables else 0\n\n        if self.use_vlm:\n            md_lines: List[str] = [\"# Extracted Charts and Tables\\n\"]\n            structured_items: List[Dict[str, Any]] = []\n            vlm_items: List[Dict[str, Any]] = []\n\n        charts_desc = \"Charts (VLM \u2192 table)\" if self.use_vlm else \"Charts (cropped)\"\n        tables_desc = \"Tables (VLM \u2192 table)\" if self.use_vlm else \"Tables (cropped)\"\n\n        chart_counter = 1\n        table_counter = 1\n\n        with ExitStack() as stack:\n            is_notebook = \"ipykernel\" in sys.modules or \"jupyter\" in sys.modules\n            is_terminal = hasattr(sys.stdout, 'isatty') and sys.stdout.isatty()\n\n            if is_notebook:\n                charts_bar = stack.enter_context(\n                    create_notebook_friendly_bar(total=chart_count, desc=charts_desc)) if chart_count else None\n                tables_bar = stack.enter_context(\n                    create_notebook_friendly_bar(total=table_count, desc=tables_desc)) if table_count else None\n            else:\n                charts_bar = stack.enter_context(\n                    create_beautiful_progress_bar(total=chart_count, desc=charts_desc, leave=True)) if chart_count else None\n                tables_bar = stack.enter_context(\n                    create_beautiful_progress_bar(total=table_count, desc=tables_desc, leave=True)) if table_count else None\n\n            for p in pages:\n                page_num = p.page_index\n                page_img: Image.Image = pil_pages[page_num - 1]\n\n                target_items = [box for box in p.boxes if box.label in target_labels]\n\n                if target_items and self.use_vlm:\n                    md_lines.append(f\"\\n## Page {page_num}\\n\")\n\n                for box in sorted(target_items, key=reading_order_key):\n                    if box.label == \"chart\" and self.extract_charts:\n                        chart_filename = f\"chart_{chart_counter:03d}.png\"\n                        chart_path = os.path.join(charts_dir, chart_filename)\n\n                        cropped_img = page_img.crop((box.x1, box.y1, box.x2, box.y2))\n                        cropped_img.save(chart_path)\n\n                        if self.use_vlm and self.vlm:\n                            rel_path = os.path.join(\"charts\", chart_filename)\n                            wrote_table = False\n\n                            try:\n                                extracted_chart = self.vlm.extract_chart(chart_path)\n                                structured_item = to_structured_dict(extracted_chart)\n                                if structured_item:\n                                    # Add page and type information to structured item\n                                    structured_item[\"page\"] = page_num\n                                    structured_item[\"type\"] = \"Chart\"\n                                    structured_items.append(structured_item)\n                                    vlm_items.append({\n                                        \"kind\": \"chart\",\n                                        \"page\": page_num,\n                                        \"image_rel_path\": rel_path,\n                                        \"title\": structured_item.get(\"title\"),\n                                        \"headers\": structured_item.get(\"headers\"),\n                                        \"rows\": structured_item.get(\"rows\"),\n                                    })\n                                    md_lines.append(\n                                        render_markdown_table(\n                                            structured_item.get(\"headers\"),\n                                            structured_item.get(\"rows\"),\n                                            title=structured_item.get(\n                                                \"title\") or f\"Chart {chart_counter} \u2014 page {page_num}\"\n                                        )\n                                    )\n                                    wrote_table = True\n                            except Exception:\n                                pass\n\n                            if not wrote_table:\n                                md_lines.append(f\"![Chart {chart_counter} \u2014 page {page_num}]({rel_path})\\n\")\n\n                        chart_counter += 1\n                        if charts_bar:\n                            charts_bar.update(1)\n\n                    elif box.label == \"table\" and self.extract_tables:\n                        table_filename = f\"table_{table_counter:03d}.png\"\n                        table_path = os.path.join(tables_dir, table_filename)\n\n                        cropped_img = page_img.crop((box.x1, box.y1, box.x2, box.y2))\n                        cropped_img.save(table_path)\n\n                        if self.use_vlm and self.vlm:\n                            rel_path = os.path.join(\"tables\", table_filename)\n                            wrote_table = False\n\n                            try:\n                                extracted_table = self.vlm.extract_table(table_path)\n                                structured_item = to_structured_dict(extracted_table)\n                                if structured_item:\n                                    # Add page and type information to structured item\n                                    structured_item[\"page\"] = page_num\n                                    structured_item[\"type\"] = \"Table\"\n                                    structured_items.append(structured_item)\n                                    vlm_items.append({\n                                        \"kind\": \"table\",\n                                        \"page\": page_num,\n                                        \"image_rel_path\": rel_path,\n                                        \"title\": structured_item.get(\"title\"),\n                                        \"headers\": structured_item.get(\"headers\"),\n                                        \"rows\": structured_item.get(\"rows\"),\n                                    })\n                                    md_lines.append(\n                                        render_markdown_table(\n                                            structured_item.get(\"headers\"),\n                                            structured_item.get(\"rows\"),\n                                            title=structured_item.get(\n                                                \"title\") or f\"Table {table_counter} \u2014 page {page_num}\"\n                                        )\n                                    )\n                                    wrote_table = True\n                            except Exception:\n                                pass\n\n                            if not wrote_table:\n                                md_lines.append(f\"![Table {table_counter} \u2014 page {page_num}]({rel_path})\\n\")\n\n                        table_counter += 1\n                        if tables_bar:\n                            tables_bar.update(1)\n\n        excel_path = None\n\n        if self.use_vlm:\n\n            if structured_items:\n                if self.extract_charts and self.extract_tables:\n                    excel_filename = \"parsed_tables_charts.xlsx\"\n                elif self.extract_charts:\n                    excel_filename = \"parsed_charts.xlsx\"\n                elif self.extract_tables:\n                    excel_filename = \"parsed_tables.xlsx\"\n                else:\n                    excel_filename = \"parsed_data.xlsx\"  # fallback\n\n\n                excel_path = os.path.join(out_dir, excel_filename)\n                write_structured_excel(excel_path, structured_items)\n\n                html_filename = excel_filename.replace('.xlsx', '.html')\n                html_path = os.path.join(out_dir, html_filename)\n                write_structured_html(html_path, structured_items)\n\n            if 'vlm_items' in locals() and vlm_items:\n                with open(os.path.join(out_dir, \"vlm_items.json\"), 'w', encoding='utf-8') as jf:\n                    json.dump(vlm_items, jf, ensure_ascii=False, indent=2)\n\n        extraction_types = []\n        if self.extract_charts:\n            extraction_types.append(\"charts\")\n        if self.extract_tables:\n            extraction_types.append(\"tables\")\n\n        print(f\"\u2705 Parsing completed successfully!\")\n        print(f\"\ud83d\udcc1 Output directory: {out_dir}\")\n</code></pre>"},{"location":"api/parsers.html#doctra.parsers.table_chart_extractor.ChartTablePDFParser.__init__","title":"<code>__init__(*, extract_charts=True, extract_tables=True, use_vlm=False, vlm_provider='gemini', vlm_model=None, vlm_api_key=None, layout_model_name='PP-DocLayout_plus-L', dpi=200, min_score=0.0)</code>","text":"<p>Initialize the ChartTablePDFParser with extraction configuration.</p> <p>:param extract_charts: Whether to extract charts from the document (default: True) :param extract_tables: Whether to extract tables from the document (default: True) :param use_vlm: Whether to use VLM for structured data extraction (default: False) :param vlm_provider: VLM provider to use (\"gemini\", \"openai\", \"anthropic\", or \"openrouter\", default: \"gemini\") :param vlm_model: Model name to use (defaults to provider-specific defaults) :param vlm_api_key: API key for VLM provider (required if use_vlm is True) :param layout_model_name: Layout detection model name (default: \"PP-DocLayout_plus-L\") :param dpi: DPI for PDF rendering (default: 200) :param min_score: Minimum confidence score for layout detection (default: 0.0)</p> Source code in <code>doctra/parsers/table_chart_extractor.py</code> <pre><code>def __init__(\n        self,\n        *,\n        extract_charts: bool = True,\n        extract_tables: bool = True,\n        use_vlm: bool = False,\n        vlm_provider: str = \"gemini\",\n        vlm_model: str | None = None,\n        vlm_api_key: str | None = None,\n        layout_model_name: str = \"PP-DocLayout_plus-L\",\n        dpi: int = 200,\n        min_score: float = 0.0,\n):\n    \"\"\"\n    Initialize the ChartTablePDFParser with extraction configuration.\n\n    :param extract_charts: Whether to extract charts from the document (default: True)\n    :param extract_tables: Whether to extract tables from the document (default: True)\n    :param use_vlm: Whether to use VLM for structured data extraction (default: False)\n    :param vlm_provider: VLM provider to use (\"gemini\", \"openai\", \"anthropic\", or \"openrouter\", default: \"gemini\")\n    :param vlm_model: Model name to use (defaults to provider-specific defaults)\n    :param vlm_api_key: API key for VLM provider (required if use_vlm is True)\n    :param layout_model_name: Layout detection model name (default: \"PP-DocLayout_plus-L\")\n    :param dpi: DPI for PDF rendering (default: 200)\n    :param min_score: Minimum confidence score for layout detection (default: 0.0)\n    \"\"\"\n    if not extract_charts and not extract_tables:\n        raise ValueError(\"At least one of extract_charts or extract_tables must be True\")\n\n    self.extract_charts = extract_charts\n    self.extract_tables = extract_tables\n    self.layout_engine = PaddleLayoutEngine(model_name=layout_model_name)\n    self.dpi = dpi\n    self.min_score = min_score\n\n    self.use_vlm = use_vlm\n    self.vlm = None\n    if self.use_vlm:\n        self.vlm = VLMStructuredExtractor(\n            vlm_provider=vlm_provider,\n            vlm_model=vlm_model,\n            api_key=vlm_api_key,\n        )\n</code></pre>"},{"location":"api/parsers.html#doctra.parsers.table_chart_extractor.ChartTablePDFParser.parse","title":"<code>parse(pdf_path, output_base_dir='outputs')</code>","text":"<p>Parse a PDF document and extract charts and/or tables.</p> <p>:param pdf_path: Path to the input PDF file :param output_base_dir: Base directory for output files (default: \"outputs\") :return: None</p> Source code in <code>doctra/parsers/table_chart_extractor.py</code> <pre><code>def parse(self, pdf_path: str, output_base_dir: str = \"outputs\") -&gt; None:\n    \"\"\"\n    Parse a PDF document and extract charts and/or tables.\n\n    :param pdf_path: Path to the input PDF file\n    :param output_base_dir: Base directory for output files (default: \"outputs\")\n    :return: None\n    \"\"\"\n    pdf_name = Path(pdf_path).stem\n    out_dir = os.path.join(output_base_dir, pdf_name, \"structured_parsing\")\n    os.makedirs(out_dir, exist_ok=True)\n\n    charts_dir = None\n    tables_dir = None\n\n    if self.extract_charts:\n        charts_dir = os.path.join(out_dir, \"charts\")\n        os.makedirs(charts_dir, exist_ok=True)\n\n    if self.extract_tables:\n        tables_dir = os.path.join(out_dir, \"tables\")\n        os.makedirs(tables_dir, exist_ok=True)\n\n    pages: List[LayoutPage] = self.layout_engine.predict_pdf(\n        pdf_path, batch_size=1, layout_nms=True, dpi=self.dpi, min_score=self.min_score\n    )\n    pil_pages = [im for (im, _, _) in render_pdf_to_images(pdf_path, dpi=self.dpi)]\n\n    target_labels = []\n    if self.extract_charts:\n        target_labels.append(\"chart\")\n    if self.extract_tables:\n        target_labels.append(\"table\")\n\n    chart_count = sum(sum(1 for b in p.boxes if b.label == \"chart\") for p in pages) if self.extract_charts else 0\n    table_count = sum(sum(1 for b in p.boxes if b.label == \"table\") for p in pages) if self.extract_tables else 0\n\n    if self.use_vlm:\n        md_lines: List[str] = [\"# Extracted Charts and Tables\\n\"]\n        structured_items: List[Dict[str, Any]] = []\n        vlm_items: List[Dict[str, Any]] = []\n\n    charts_desc = \"Charts (VLM \u2192 table)\" if self.use_vlm else \"Charts (cropped)\"\n    tables_desc = \"Tables (VLM \u2192 table)\" if self.use_vlm else \"Tables (cropped)\"\n\n    chart_counter = 1\n    table_counter = 1\n\n    with ExitStack() as stack:\n        is_notebook = \"ipykernel\" in sys.modules or \"jupyter\" in sys.modules\n        is_terminal = hasattr(sys.stdout, 'isatty') and sys.stdout.isatty()\n\n        if is_notebook:\n            charts_bar = stack.enter_context(\n                create_notebook_friendly_bar(total=chart_count, desc=charts_desc)) if chart_count else None\n            tables_bar = stack.enter_context(\n                create_notebook_friendly_bar(total=table_count, desc=tables_desc)) if table_count else None\n        else:\n            charts_bar = stack.enter_context(\n                create_beautiful_progress_bar(total=chart_count, desc=charts_desc, leave=True)) if chart_count else None\n            tables_bar = stack.enter_context(\n                create_beautiful_progress_bar(total=table_count, desc=tables_desc, leave=True)) if table_count else None\n\n        for p in pages:\n            page_num = p.page_index\n            page_img: Image.Image = pil_pages[page_num - 1]\n\n            target_items = [box for box in p.boxes if box.label in target_labels]\n\n            if target_items and self.use_vlm:\n                md_lines.append(f\"\\n## Page {page_num}\\n\")\n\n            for box in sorted(target_items, key=reading_order_key):\n                if box.label == \"chart\" and self.extract_charts:\n                    chart_filename = f\"chart_{chart_counter:03d}.png\"\n                    chart_path = os.path.join(charts_dir, chart_filename)\n\n                    cropped_img = page_img.crop((box.x1, box.y1, box.x2, box.y2))\n                    cropped_img.save(chart_path)\n\n                    if self.use_vlm and self.vlm:\n                        rel_path = os.path.join(\"charts\", chart_filename)\n                        wrote_table = False\n\n                        try:\n                            extracted_chart = self.vlm.extract_chart(chart_path)\n                            structured_item = to_structured_dict(extracted_chart)\n                            if structured_item:\n                                # Add page and type information to structured item\n                                structured_item[\"page\"] = page_num\n                                structured_item[\"type\"] = \"Chart\"\n                                structured_items.append(structured_item)\n                                vlm_items.append({\n                                    \"kind\": \"chart\",\n                                    \"page\": page_num,\n                                    \"image_rel_path\": rel_path,\n                                    \"title\": structured_item.get(\"title\"),\n                                    \"headers\": structured_item.get(\"headers\"),\n                                    \"rows\": structured_item.get(\"rows\"),\n                                })\n                                md_lines.append(\n                                    render_markdown_table(\n                                        structured_item.get(\"headers\"),\n                                        structured_item.get(\"rows\"),\n                                        title=structured_item.get(\n                                            \"title\") or f\"Chart {chart_counter} \u2014 page {page_num}\"\n                                    )\n                                )\n                                wrote_table = True\n                        except Exception:\n                            pass\n\n                        if not wrote_table:\n                            md_lines.append(f\"![Chart {chart_counter} \u2014 page {page_num}]({rel_path})\\n\")\n\n                    chart_counter += 1\n                    if charts_bar:\n                        charts_bar.update(1)\n\n                elif box.label == \"table\" and self.extract_tables:\n                    table_filename = f\"table_{table_counter:03d}.png\"\n                    table_path = os.path.join(tables_dir, table_filename)\n\n                    cropped_img = page_img.crop((box.x1, box.y1, box.x2, box.y2))\n                    cropped_img.save(table_path)\n\n                    if self.use_vlm and self.vlm:\n                        rel_path = os.path.join(\"tables\", table_filename)\n                        wrote_table = False\n\n                        try:\n                            extracted_table = self.vlm.extract_table(table_path)\n                            structured_item = to_structured_dict(extracted_table)\n                            if structured_item:\n                                # Add page and type information to structured item\n                                structured_item[\"page\"] = page_num\n                                structured_item[\"type\"] = \"Table\"\n                                structured_items.append(structured_item)\n                                vlm_items.append({\n                                    \"kind\": \"table\",\n                                    \"page\": page_num,\n                                    \"image_rel_path\": rel_path,\n                                    \"title\": structured_item.get(\"title\"),\n                                    \"headers\": structured_item.get(\"headers\"),\n                                    \"rows\": structured_item.get(\"rows\"),\n                                })\n                                md_lines.append(\n                                    render_markdown_table(\n                                        structured_item.get(\"headers\"),\n                                        structured_item.get(\"rows\"),\n                                        title=structured_item.get(\n                                            \"title\") or f\"Table {table_counter} \u2014 page {page_num}\"\n                                    )\n                                )\n                                wrote_table = True\n                        except Exception:\n                            pass\n\n                        if not wrote_table:\n                            md_lines.append(f\"![Table {table_counter} \u2014 page {page_num}]({rel_path})\\n\")\n\n                    table_counter += 1\n                    if tables_bar:\n                        tables_bar.update(1)\n\n    excel_path = None\n\n    if self.use_vlm:\n\n        if structured_items:\n            if self.extract_charts and self.extract_tables:\n                excel_filename = \"parsed_tables_charts.xlsx\"\n            elif self.extract_charts:\n                excel_filename = \"parsed_charts.xlsx\"\n            elif self.extract_tables:\n                excel_filename = \"parsed_tables.xlsx\"\n            else:\n                excel_filename = \"parsed_data.xlsx\"  # fallback\n\n\n            excel_path = os.path.join(out_dir, excel_filename)\n            write_structured_excel(excel_path, structured_items)\n\n            html_filename = excel_filename.replace('.xlsx', '.html')\n            html_path = os.path.join(out_dir, html_filename)\n            write_structured_html(html_path, structured_items)\n\n        if 'vlm_items' in locals() and vlm_items:\n            with open(os.path.join(out_dir, \"vlm_items.json\"), 'w', encoding='utf-8') as jf:\n                json.dump(vlm_items, jf, ensure_ascii=False, indent=2)\n\n    extraction_types = []\n    if self.extract_charts:\n        extraction_types.append(\"charts\")\n    if self.extract_tables:\n        extraction_types.append(\"tables\")\n\n    print(f\"\u2705 Parsing completed successfully!\")\n    print(f\"\ud83d\udcc1 Output directory: {out_dir}\")\n</code></pre>"},{"location":"api/parsers.html#structureddocxparser","title":"StructuredDOCXParser","text":"<p>Comprehensive parser for Microsoft Word documents (.docx files).</p>"},{"location":"api/parsers.html#doctra.parsers.structured_docx_parser.StructuredDOCXParser","title":"<code>doctra.parsers.structured_docx_parser.StructuredDOCXParser</code>","text":"<p>Comprehensive DOCX parser for extracting all types of content.</p> <p>Processes DOCX documents to extract text, tables, images, and figures. Supports structured data extraction and optional VLM processing for enhanced content analysis.</p> <p>:param use_vlm: Whether to use VLM for structured data extraction (default: False) :param vlm_provider: VLM provider to use (\"gemini\", \"openai\", \"anthropic\", or \"openrouter\", default: \"gemini\") :param vlm_model: Model name to use (defaults to provider-specific defaults) :param vlm_api_key: API key for VLM provider (required if use_vlm is True) :param extract_images: Whether to extract embedded images (default: True) :param preserve_formatting: Whether to preserve text formatting in output (default: True) :param table_detection: Whether to detect and extract tables (default: True)</p> Source code in <code>doctra/parsers/structured_docx_parser.py</code> <pre><code>class StructuredDOCXParser:\n    \"\"\"\n    Comprehensive DOCX parser for extracting all types of content.\n\n    Processes DOCX documents to extract text, tables, images, and figures.\n    Supports structured data extraction and optional VLM processing for\n    enhanced content analysis.\n\n    :param use_vlm: Whether to use VLM for structured data extraction (default: False)\n    :param vlm_provider: VLM provider to use (\"gemini\", \"openai\", \"anthropic\", or \"openrouter\", default: \"gemini\")\n    :param vlm_model: Model name to use (defaults to provider-specific defaults)\n    :param vlm_api_key: API key for VLM provider (required if use_vlm is True)\n    :param extract_images: Whether to extract embedded images (default: True)\n    :param preserve_formatting: Whether to preserve text formatting in output (default: True)\n    :param table_detection: Whether to detect and extract tables (default: True)\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        use_vlm: bool = False,\n        vlm_provider: str = \"gemini\",\n        vlm_model: str | None = None,\n        vlm_api_key: str | None = None,\n        extract_images: bool = True,\n        preserve_formatting: bool = True,\n        table_detection: bool = True,\n        export_excel: bool = True,\n    ):\n        \"\"\"\n        Initialize the StructuredDOCXParser with processing configuration.\n\n        :param use_vlm: Whether to use VLM for structured data extraction (default: False)\n        :param vlm_provider: VLM provider to use (\"gemini\", \"openai\", \"anthropic\", or \"openrouter\", default: \"gemini\")\n        :param vlm_model: Model name to use (defaults to provider-specific defaults)\n        :param vlm_api_key: API key for VLM provider (required if use_vlm is True)\n        :param extract_images: Whether to extract embedded images (default: True)\n        :param preserve_formatting: Whether to preserve text formatting in output (default: True)\n        :param table_detection: Whether to detect and extract tables (default: True)\n        :param export_excel: Whether to export tables to Excel file (default: True)\n        \"\"\"\n        if Document is None:\n            raise ImportError(\"python-docx is required for DOCX parsing. Install with: pip install python-docx\")\n\n        self.use_vlm = use_vlm\n        self.extract_images = extract_images\n        self.preserve_formatting = preserve_formatting\n        self.table_detection = table_detection\n        self.export_excel = export_excel\n        self.vlm = None\n\n        if self.use_vlm:\n            try:\n                from doctra.engines.vlm.service import VLMStructuredExtractor\n                self.vlm = VLMStructuredExtractor(\n                    vlm_provider=vlm_provider,\n                    vlm_model=vlm_model,\n                    api_key=vlm_api_key,\n                )\n            except Exception as e:\n                print(f\"Warning: VLM initialization failed: {e}\")\n                self.vlm = None\n\n    def parse(self, docx_path: str) -&gt; None:\n        \"\"\"\n        Parse a DOCX document and extract all content.\n\n        :param docx_path: Path to the DOCX file to parse\n        \"\"\"\n        if not os.path.exists(docx_path):\n            raise FileNotFoundError(f\"DOCX file not found: {docx_path}\")\n\n        docx_path = Path(docx_path)\n        output_dir = Path(f\"outputs/{docx_path.stem}\")\n        output_dir.mkdir(parents=True, exist_ok=True)\n\n        print(f\"\ud83d\udcc4 Processing DOCX: {docx_path.name}\")\n\n        try:\n            # Load the DOCX document\n            doc = Document(docx_path)\n\n            # Extract document structure\n            document_data = self._extract_document_structure(doc)\n\n            # Extract images if enabled\n            images_data = []\n            if self.extract_images:\n                images_data = self._extract_images(doc, output_dir)\n\n            # Extract tables from the document structure\n            tables_data = [elem for elem in document_data['elements'] if elem['type'] == 'table']\n\n            # Calculate total steps based on number of images\n            if self.use_vlm and self.vlm and images_data:\n                total_steps = len(images_data)  # One step per image\n            else:\n                total_steps = 1  # Just one step if no VLM processing\n\n            # Create progress bar\n            progress_bar = tqdm(total=total_steps, desc=\"Processing DOCX\", unit=\"image\")\n\n            # Process VLM data first if enabled\n            vlm_extracted_data = []\n            if self.use_vlm and self.vlm and images_data:\n                vlm_extracted_data = self._process_vlm_data(images_data, output_dir, progress_bar)\n            else:\n                # If no VLM processing, just update the progress bar\n                progress_bar.update(1)\n\n            progress_bar.close()\n\n            # Generate outputs (no progress bar for these as they're fast)\n            self._generate_markdown_output(document_data, images_data, output_dir, vlm_extracted_data)\n            self._generate_html_output(document_data, images_data, output_dir, vlm_extracted_data)\n\n            # Generate Excel output if enabled\n            if self.export_excel:\n                if vlm_extracted_data:\n                    self._generate_excel_output_with_vlm(tables_data, vlm_extracted_data, output_dir)\n                else:\n                    self._generate_excel_output(tables_data, output_dir)\n\n            print(f\"\u2705 DOCX parsing completed successfully!\")\n            print(f\"\ud83d\udcca Extracted: {len(document_data.get('paragraphs', []))} paragraphs, \"\n                  f\"{len(tables_data)} tables, {len(images_data)} images\")\n\n        except Exception as e:\n            print(f\"\u274c Error parsing DOCX: {e}\")\n            raise\n\n    def _extract_document_structure(self, doc: DocumentType) -&gt; Dict[str, Any]:\n        \"\"\"Extract the overall document structure.\"\"\"\n        document_data = {\n            'elements': [],  # Mixed list of paragraphs, tables, and other elements\n            'paragraphs': [],\n            'headings': [],\n            'lists': [],\n            'metadata': {}\n        }\n\n        # Extract metadata\n        document_data['metadata'] = {\n            'title': doc.core_properties.title or '',\n            'author': doc.core_properties.author or '',\n            'subject': doc.core_properties.subject or '',\n            'created': str(doc.core_properties.created) if doc.core_properties.created else '',\n            'modified': str(doc.core_properties.modified) if doc.core_properties.modified else '',\n        }\n\n        # Extract document elements in order (paragraphs and tables)\n        self._extract_document_elements_in_order(doc, document_data)\n\n        return document_data\n\n    def _extract_document_elements_in_order(self, doc: DocumentType, document_data: Dict):\n        \"\"\"Extract document elements (paragraphs and tables) in their original order.\"\"\"\n        elements = []\n        paragraph_index = 0\n        table_index = 0\n\n        # Get all document elements by iterating through the document body\n        for element in doc.element.body:\n            if element.tag.endswith('p'):  # Paragraph\n                # Find the corresponding paragraph object\n                for para in doc.paragraphs:\n                    if para._element == element and para.text.strip():\n                        para_data = {\n                            'type': 'paragraph',\n                            'index': paragraph_index,\n                            'text': para.text.strip(),\n                            'style': para.style.name if para.style else 'Normal',\n                            'is_heading': para.style.name.startswith('Heading') if para.style else False,\n                            'level': self._get_heading_level(para.style.name) if para.style else 0,\n                            'formatting': self._extract_formatting(para) if self.preserve_formatting else {}\n                        }\n\n                        elements.append(para_data)\n                        document_data['paragraphs'].append(para_data)\n\n                        # Categorize headings\n                        if para_data['is_heading']:\n                            document_data['headings'].append(para_data)\n\n                        paragraph_index += 1\n                        break\n\n            elif element.tag.endswith('tbl'):  # Table\n                # Find the corresponding table object\n                for table in doc.tables:\n                    if table._element == element:\n                        table_data = {\n                            'type': 'table',\n                            'index': table_index,\n                            'rows': len(table.rows),\n                            'cols': len(table.columns),\n                            'data': [],\n                            'markdown': ''\n                        }\n\n                        # Extract table data\n                        for row_idx, row in enumerate(table.rows):\n                            row_data = []\n                            for cell in row.cells:\n                                cell_text = cell.text.strip()\n                                row_data.append(cell_text)\n                            table_data['data'].append(row_data)\n\n                        # Generate markdown table\n                        if table_data['data']:\n                            headers = table_data['data'][0] if table_data['data'] else []\n                            rows = table_data['data'][1:] if len(table_data['data']) &gt; 1 else []\n                            table_data['markdown'] = render_markdown_table(headers, rows)\n\n                        elements.append(table_data)\n                        table_index += 1\n                        break\n\n        # Store elements in order\n        document_data['elements'] = elements\n\n    def _extract_tables(self, doc: DocumentType, output_dir: Path) -&gt; List[Dict[str, Any]]:\n        \"\"\"Extract all tables from the document.\"\"\"\n        tables_data = []\n\n        for table_idx, table in enumerate(doc.tables):\n            table_data = {\n                'index': table_idx,\n                'rows': len(table.rows),\n                'cols': len(table.columns),\n                'data': [],\n                'markdown': ''\n            }\n\n            # Extract table data\n            for row_idx, row in enumerate(table.rows):\n                row_data = []\n                for cell in row.cells:\n                    cell_text = cell.text.strip()\n                    row_data.append(cell_text)\n                table_data['data'].append(row_data)\n\n            # Generate markdown table\n            if table_data['data']:\n                # Extract headers (first row) and data rows\n                headers = table_data['data'][0] if table_data['data'] else []\n                rows = table_data['data'][1:] if len(table_data['data']) &gt; 1 else []\n                table_data['markdown'] = render_markdown_table(headers, rows)\n                print(f\"\ud83d\udcca Table {table_idx + 1}: {len(table_data['data'])} rows, {len(table_data['data'][0]) if table_data['data'] else 0} columns\")\n\n            tables_data.append(table_data)\n\n        return tables_data\n\n    def _extract_images(self, doc: DocumentType, output_dir: Path) -&gt; List[Dict[str, Any]]:\n        \"\"\"Extract embedded images from the document.\"\"\"\n        images_data = []\n        images_dir = output_dir / \"images\"\n        images_dir.mkdir(exist_ok=True)\n\n        # Extract images from document relationships\n        try:\n            for rel in doc.part.rels.values():\n                if hasattr(rel, 'target_ref'):\n                    # Safely get content type\n                    content_type = getattr(rel, 'target_content_type', 'unknown')\n                    # Check if this is an image relationship\n                    is_image = False\n                    if \"image\" in rel.target_ref or \"media\" in rel.target_ref:\n                        is_image = True\n                    elif content_type and \"image/\" in content_type:\n                        is_image = True\n                    elif rel.target_ref.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff', '.webp')):\n                        is_image = True\n\n                    if is_image:\n                        # Get the actual image data\n                        try:\n                            image_blob = rel.target_part.blob\n                            if image_blob:\n                                # Clean the filename and create proper path\n                                original_filename = rel.target_ref\n                                # Remove any directory structure from the filename\n                                clean_filename = Path(original_filename).name\n\n                                image_data = {\n                                    'filename': clean_filename,\n                                    'original_path': original_filename,\n                                    'type': clean_filename.split('.')[-1].lower(),\n                                    'path': str(images_dir / clean_filename)\n                                }\n\n                                # Ensure the target directory exists\n                                target_path = Path(image_data['path'])\n                                target_path.parent.mkdir(parents=True, exist_ok=True)\n\n                                # Save the image\n                                with open(target_path, 'wb') as f:\n                                    f.write(image_blob)\n\n                                images_data.append(image_data)\n                        except Exception as img_error:\n                            pass  # Silently skip problematic images\n\n        except Exception as e:\n            pass  # Silently skip if relationships can't be accessed\n\n        return images_data\n\n    def _process_vlm_data(self, images_data: List, output_dir: Path, progress_bar=None) -&gt; List[Dict]:\n        \"\"\"Process images with VLM to extract structured data.\"\"\"\n        vlm_extracted_data = []\n        if images_data:\n            for i, img_data in enumerate(images_data):\n                try:\n                    # Update progress bar for each image\n                    if progress_bar:\n                        progress_bar.set_description(f\"Processing image {i+1}/{len(images_data)}: {img_data['filename']}\")\n\n                    # Use VLM to extract structured data from image\n                    result = self.vlm.extract_table_or_chart(img_data['path'])\n\n                    # Convert to structured format\n                    if hasattr(result, 'title') and hasattr(result, 'description'):\n                        vlm_data = {\n                            'title': result.title,\n                            'description': result.description,\n                            'headers': result.headers,\n                            'rows': result.rows,\n                            'type': 'TabularArtifact',\n                            'source_image': img_data['filename'],\n                            'page': f\"Image {i+1}\"\n                        }\n                        vlm_extracted_data.append(vlm_data)\n                    elif isinstance(result, str):\n                        # Try to parse JSON string and create proper structure\n                        try:\n                            parsed_data = json.loads(result)\n                            vlm_data = {\n                                'title': parsed_data.get('title', f\"Extracted from {img_data['filename']}\"),\n                                'description': parsed_data.get('description', ''),\n                                'headers': parsed_data.get('headers', []),\n                                'rows': parsed_data.get('rows', []),\n                                'type': 'TabularArtifact',\n                                'source_image': img_data['filename'],\n                                'page': f\"Image {i+1}\"\n                            }\n                            vlm_extracted_data.append(vlm_data)\n                        except json.JSONDecodeError:\n                            # Fallback for non-JSON string\n                            vlm_data = {\n                                'title': f\"Extracted from {img_data['filename']}\",\n                                'description': result[:300] if len(result) &gt; 300 else result,\n                                'headers': [],\n                                'rows': [],\n                                'type': 'TabularArtifact',\n                                'source_image': img_data['filename'],\n                                'page': f\"Image {i+1}\",\n                                'raw_response': result\n                            }\n                            vlm_extracted_data.append(vlm_data)\n\n                    # Update progress bar after each image\n                    if progress_bar:\n                        progress_bar.update(1)\n\n                except Exception as img_error:\n                    # Still update progress bar even if image processing fails\n                    if progress_bar:\n                        progress_bar.update(1)\n                    pass  # Silently skip problematic images\n\n        return vlm_extracted_data\n\n    def _safe_sheet_name(self, raw_title: str) -&gt; str:\n        \"\"\"\n        Create a safe Excel sheet name from a raw title.\n\n        Ensures the sheet name is valid for Excel by removing invalid characters,\n        handling length limits, and avoiding duplicates.\n        \"\"\"\n        import re\n\n        # Excel invalid characters\n        invalid_chars = r'[:\\\\/*?\\[\\]]'\n        max_length = 31\n\n        name = (raw_title or \"Untitled\").strip()\n        name = re.sub(invalid_chars, \"_\", name)\n        name = re.sub(r\"\\s+\", \" \", name)\n        name = name[:max_length] if name else \"Sheet\"\n\n        return name\n\n    def _generate_markdown_output(self, document_data: Dict, images_data: List, output_dir: Path, vlm_extracted_data: List = None):\n        \"\"\"Generate markdown output.\"\"\"\n        markdown_content = []\n\n        # Add document metadata\n        if document_data['metadata']['title']:\n            markdown_content.append(f\"# {document_data['metadata']['title']}\")\n\n        # Process elements in order\n        for element in document_data['elements']:\n            if element['type'] == 'paragraph':\n                if element['is_heading']:\n                    level = element['level']\n                    markdown_content.append(f\"{'#' * level} {element['text']}\")\n                else:\n                    markdown_content.append(element['text'])\n            elif element['type'] == 'table':\n                if element['markdown']:\n                    markdown_content.append(f\"\\n## Table {element['index'] + 1}\")\n                    markdown_content.append(element['markdown'])\n\n        # Add VLM extracted tables if available, otherwise add image references\n        if vlm_extracted_data:\n            # Replace images with VLM extracted tables\n            for i, vlm_table in enumerate(vlm_extracted_data):\n                if vlm_table['rows']:\n                    markdown_content.append(f\"\\n## {vlm_table['title']}\")\n                    if vlm_table['description']:\n                        markdown_content.append(f\"*{vlm_table['description']}*\")\n\n                    # Generate markdown table from VLM data\n                    if vlm_table['headers'] and vlm_table['rows']:\n                        vlm_markdown = render_markdown_table(vlm_table['headers'], vlm_table['rows'])\n                        markdown_content.append(vlm_markdown)\n        else:\n            # Add image references only if no VLM data\n            for img in images_data:\n                # Use relative path for markdown\n                relative_path = f\"images/{img['filename']}\"\n                markdown_content.append(f\"\\n![{img['filename']}]({relative_path})\")\n\n        # Write markdown file\n        write_markdown(markdown_content, str(output_dir), \"document.md\")\n\n    def _generate_html_output(self, document_data: Dict, images_data: List, output_dir: Path, vlm_extracted_data: List = None):\n        \"\"\"Generate HTML output.\"\"\"\n        html_content = []\n\n        # Add document title\n        if document_data['metadata']['title']:\n            html_content.append(f\"&lt;h1&gt;{document_data['metadata']['title']}&lt;/h1&gt;\")\n\n        # Process elements in order\n        for element in document_data['elements']:\n            if element['type'] == 'paragraph':\n                if element['is_heading']:\n                    level = element['level']\n                    html_content.append(f\"&lt;h{level}&gt;{element['text']}&lt;/h{level}&gt;\")\n                else:\n                    html_content.append(f\"&lt;p&gt;{element['text']}&lt;/p&gt;\")\n            elif element['type'] == 'table':\n                if element['data']:\n                    html_content.append(f\"&lt;h2&gt;Table {element['index'] + 1}&lt;/h2&gt;\")\n                    html_table = self._generate_html_table(element['data'])\n                    html_content.append(html_table)\n\n        # Add VLM extracted tables if available, otherwise add image references\n        if vlm_extracted_data:\n            # Replace images with VLM extracted tables\n            for i, vlm_table in enumerate(vlm_extracted_data):\n                if vlm_table['rows']:\n                    html_content.append(f\"&lt;h2&gt;{vlm_table['title']}&lt;/h2&gt;\")\n                    if vlm_table['description']:\n                        html_content.append(f\"&lt;p&gt;&lt;em&gt;{vlm_table['description']}&lt;/em&gt;&lt;/p&gt;\")\n\n                    # Generate HTML table from VLM data\n                    if vlm_table['headers'] and vlm_table['rows']:\n                        # Create table data with headers\n                        table_data = [vlm_table['headers']] + vlm_table['rows']\n                        vlm_html_table = self._generate_html_table(table_data)\n                        html_content.append(vlm_html_table)\n        else:\n            # Add images only if no VLM data\n            for img in images_data:\n                # Use relative path for HTML\n                relative_path = f\"images/{img['filename']}\"\n                html_content.append(f'&lt;img src=\"{relative_path}\" alt=\"{img[\"filename\"]}\" /&gt;')\n\n        # Write HTML file\n        write_html(html_content, str(output_dir), \"document.html\")\n\n    def _generate_excel_output(self, tables_data: List, output_dir: Path):\n        \"\"\"Generate Excel output with all tables and Table of Contents.\"\"\"\n        if not tables_data:\n            print(\"\u26a0\ufe0f  No tables found to export to Excel\")\n            return\n\n        if not EXCEL_AVAILABLE:\n            print(\"\u26a0\ufe0f  Excel export requires pandas and openpyxl: Missing dependencies\")\n            print(\"Install with: pip install pandas openpyxl\")\n            return\n\n        try:\n            # Create a new workbook\n            wb = Workbook()\n            wb.remove(wb.active)  # Remove default sheet\n\n            # Define styling constants (matching VLM version)\n            HEADER_FILL = PatternFill(fill_type=\"solid\", start_color=\"FF2E7D32\", end_color=\"FF2E7D32\")  # Green\n            HEADER_FONT = Font(color=\"FFFFFFFF\", bold=True)\n            HEADER_ALIGN = Alignment(horizontal=\"center\", vertical=\"center\", wrap_text=True)\n\n            # Create Table of Contents data\n            toc_data = []\n            sheet_index = 1\n            sheet_mapping = {}  # For hyperlinks\n\n            for i, table in enumerate(tables_data):\n                if table['data']:\n                    # Use table title if available, otherwise fallback to generic name\n                    table_title = table.get('title', f\"Table {i+1}\")\n                    sheet_name = self._safe_sheet_name(table_title)\n                    ws = wb.create_sheet(title=sheet_name)\n\n                    # Add data to worksheet\n                    for row_idx, row_data in enumerate(table['data']):\n                        for col_idx, cell_value in enumerate(row_data):\n                            ws.cell(row=row_idx + 1, column=col_idx + 1, value=cell_value)\n\n                    # Apply styling to header row\n                    if table['data']:\n                        ncols = len(table['data'][0]) if table['data'] else 0\n                        for col_idx in range(1, ncols + 1):\n                            cell = ws.cell(row=1, column=col_idx)\n                            cell.fill = HEADER_FILL\n                            cell.font = HEADER_FONT\n                            cell.alignment = HEADER_ALIGN\n                        ws.freeze_panes = \"A2\"\n\n                    # Auto-adjust column widths\n                    for column in ws.columns:\n                        max_length = 0\n                        column_letter = column[0].column_letter\n                        for cell in column:\n                            try:\n                                if len(str(cell.value)) &gt; max_length:\n                                    max_length = len(str(cell.value))\n                            except:\n                                pass\n                        adjusted_width = min(max_length + 2, 50)\n                        ws.column_dimensions[column_letter].width = adjusted_width\n\n                    # Add to TOC and sheet mapping\n                    toc_data.append([\n                        sheet_index,\n                        table_title,\n                        f\"Original table from document\",\n                        len(table['data']),\n                        len(table['data'][0]) if table['data'] else 0,\n                        \"Document\"\n                    ])\n                    sheet_mapping[table_title] = sheet_name\n                    sheet_index += 1\n\n            # Create Table of Contents sheet\n            if toc_data:\n                toc_ws = wb.create_sheet(title=\"Table_of_Contents\", index=0)\n\n                # Add headers with proper styling\n                toc_headers = [\"Sheet #\", \"Table Name\", \"Description\", \"Rows\", \"Columns\", \"Source\"]\n                for col_idx, header in enumerate(toc_headers):\n                    cell = toc_ws.cell(row=1, column=col_idx + 1, value=header)\n                    cell.fill = HEADER_FILL\n                    cell.font = HEADER_FONT\n                    cell.alignment = HEADER_ALIGN\n\n                # Add data with hyperlinks\n                for row_idx, row_data in enumerate(toc_data):\n                    for col_idx, cell_value in enumerate(row_data):\n                        cell = toc_ws.cell(row=row_idx + 2, column=col_idx + 1, value=cell_value)\n\n                        # Add hyperlink to table name (column B)\n                        if col_idx == 1 and cell_value in sheet_mapping:  # Table Name column\n                            sheet_name = sheet_mapping[cell_value]\n\n                            # Create hyperlink to the sheet\n                            if ' ' in sheet_name or any(char in sheet_name for char in ['[', ']', '*', '?', ':', '\\\\', '/']):\n                                hyperlink_ref = f\"#'{sheet_name}'!A1\"\n                            else:\n                                hyperlink_ref = f\"#{sheet_name}!A1\"\n\n                            cell.hyperlink = Hyperlink(ref=hyperlink_ref, target=hyperlink_ref)\n                            cell.font = Font(color=\"0000FF\", underline=\"single\")\n\n                        # Wrap text for description column (column C)\n                        if col_idx == 2:  # Description column\n                            cell.alignment = Alignment(wrap_text=True, vertical=\"top\")\n\n                # Set specific column widths for TOC\n                toc_ws.column_dimensions['A'].width = 10  # Sheet #\n                toc_ws.column_dimensions['B'].width = 30  # Table Name\n                toc_ws.column_dimensions['C'].width = 60  # Description\n                toc_ws.column_dimensions['D'].width = 10  # Rows\n                toc_ws.column_dimensions['E'].width = 10  # Columns\n                toc_ws.column_dimensions['F'].width = 15  # Source\n\n                # Set row heights for better readability\n                for row_idx in range(2, len(toc_data) + 2):\n                    toc_ws.row_dimensions[row_idx].height = 30\n\n            # Save the workbook\n            excel_path = output_dir / \"tables.xlsx\"\n            wb.save(excel_path)\n\n        except Exception as e:\n            print(f\"\u274c Error creating Excel file: {e}\")\n\n    def _generate_excel_output_with_vlm(self, tables_data: List, vlm_extracted_data: List, output_dir: Path):\n        \"\"\"Generate Excel output with both original tables and VLM extracted data, including table of contents.\"\"\"\n        if not tables_data and not vlm_extracted_data:\n            print(\"\u26a0\ufe0f  No tables found to export to Excel\")\n            return\n\n        if not EXCEL_AVAILABLE:\n            print(\"\u26a0\ufe0f  Excel export requires pandas and openpyxl: Missing dependencies\")\n            print(\"Install with: pip install pandas openpyxl\")\n            return\n\n        try:\n            wb = Workbook()\n            wb.remove(wb.active)\n\n\n            # Define styling constants (matching PDF parser)\n            HEADER_FILL = PatternFill(fill_type=\"solid\", start_color=\"FF2E7D32\", end_color=\"FF2E7D32\")  # Green\n            HEADER_FONT = Font(color=\"FFFFFFFF\", bold=True)\n            HEADER_ALIGN = Alignment(horizontal=\"center\", vertical=\"center\", wrap_text=True)\n\n            # Create Table of Contents sheet\n            toc_data = []\n            sheet_index = 1\n            sheet_mapping = {}  # For hyperlinks\n\n            # Add original tables\n            for i, table in enumerate(tables_data):\n                if table['data']:\n                    # Use table title if available, otherwise fallback to generic name\n                    table_title = table.get('title', f\"Table {i+1}\")\n                    sheet_name = self._safe_sheet_name(table_title)\n                    ws = wb.create_sheet(title=sheet_name)\n\n                    # Add data to worksheet\n                    for row_idx, row_data in enumerate(table['data']):\n                        for col_idx, cell_value in enumerate(row_data):\n                            ws.cell(row=row_idx + 1, column=col_idx + 1, value=cell_value)\n\n                    # Apply styling to header row\n                    if table['data']:\n                        ncols = len(table['data'][0]) if table['data'] else 0\n                        for col_idx in range(1, ncols + 1):\n                            cell = ws.cell(row=1, column=col_idx)\n                            cell.fill = HEADER_FILL\n                            cell.font = HEADER_FONT\n                            cell.alignment = HEADER_ALIGN\n                        ws.freeze_panes = \"A2\"\n\n                    # Auto-adjust column widths\n                    for column in ws.columns:\n                        max_length = 0\n                        column_letter = column[0].column_letter\n                        for cell in column:\n                            try:\n                                if len(str(cell.value)) &gt; max_length:\n                                    max_length = len(str(cell.value))\n                            except:\n                                pass\n                        adjusted_width = min(max_length + 2, 50)\n                        ws.column_dimensions[column_letter].width = adjusted_width\n\n                    # Add to TOC and sheet mapping\n                    toc_data.append([\n                        sheet_index,\n                        table_title,\n                        f\"Original table from document\",\n                        len(table['data']),\n                        len(table['data'][0]) if table['data'] else 0,\n                        \"Document\"\n                    ])\n                    sheet_mapping[table_title] = sheet_name\n                    sheet_index += 1\n\n            # Add VLM extracted tables\n            for i, vlm_table in enumerate(vlm_extracted_data):\n                if vlm_table['rows']:\n                    # Use VLM table title as sheet name\n                    table_title = vlm_table['title']\n                    sheet_name = self._safe_sheet_name(table_title)\n                    ws = wb.create_sheet(title=sheet_name)\n\n                    # Add headers with proper styling\n                    for col_idx, header in enumerate(vlm_table['headers']):\n                        cell = ws.cell(row=1, column=col_idx + 1, value=header)\n                        cell.fill = HEADER_FILL\n                        cell.font = HEADER_FONT\n                        cell.alignment = HEADER_ALIGN\n\n                    # Add data rows\n                    for row_idx, row_data in enumerate(vlm_table['rows']):\n                        for col_idx, cell_value in enumerate(row_data):\n                            ws.cell(row=row_idx + 2, column=col_idx + 1, value=cell_value)\n\n                    # Freeze panes below header\n                    ws.freeze_panes = \"A2\"\n\n                    # Auto-adjust column widths\n                    for column in ws.columns:\n                        max_length = 0\n                        column_letter = column[0].column_letter\n                        for cell in column:\n                            try:\n                                if len(str(cell.value)) &gt; max_length:\n                                    max_length = len(str(cell.value))\n                            except:\n                                pass\n                        adjusted_width = min(max_length + 2, 50)\n                        ws.column_dimensions[column_letter].width = adjusted_width\n\n                    # Add to TOC and sheet mapping\n                    toc_data.append([\n                        sheet_index,\n                        table_title,\n                        vlm_table['description'],\n                        len(vlm_table['rows']),\n                        len(vlm_table['headers']),\n                        \"VLM Extracted\"\n                    ])\n                    sheet_mapping[table_title] = sheet_name\n                    sheet_index += 1\n\n            # Create Table of Contents sheet\n            if toc_data:\n                toc_ws = wb.create_sheet(title=\"Table_of_Contents\", index=0)\n\n                # Add headers with proper styling\n                toc_headers = [\"Sheet #\", \"Table Name\", \"Description\", \"Rows\", \"Columns\", \"Source\"]\n                for col_idx, header in enumerate(toc_headers):\n                    cell = toc_ws.cell(row=1, column=col_idx + 1, value=header)\n                    cell.fill = HEADER_FILL\n                    cell.font = HEADER_FONT\n                    cell.alignment = HEADER_ALIGN\n\n                # Add data with hyperlinks\n                for row_idx, row_data in enumerate(toc_data):\n                    for col_idx, cell_value in enumerate(row_data):\n                        cell = toc_ws.cell(row=row_idx + 2, column=col_idx + 1, value=cell_value)\n\n                        # Add hyperlink to table name (column B)\n                        if col_idx == 1 and cell_value in sheet_mapping:  # Table Name column\n                            sheet_name = sheet_mapping[cell_value]\n\n                            # Create hyperlink to the sheet\n                            if ' ' in sheet_name or any(char in sheet_name for char in ['[', ']', '*', '?', ':', '\\\\', '/']):\n                                hyperlink_ref = f\"#'{sheet_name}'!A1\"\n                            else:\n                                hyperlink_ref = f\"#{sheet_name}!A1\"\n\n                            cell.hyperlink = Hyperlink(ref=hyperlink_ref, target=hyperlink_ref)\n                            cell.font = Font(color=\"0000FF\", underline=\"single\")\n\n                        # Wrap text for description column (column C)\n                        if col_idx == 2:  # Description column\n                            cell.alignment = Alignment(wrap_text=True, vertical=\"top\")\n\n                # Set specific column widths for TOC\n                toc_ws.column_dimensions['A'].width = 10  # Sheet #\n                toc_ws.column_dimensions['B'].width = 30  # Table Name\n                toc_ws.column_dimensions['C'].width = 60  # Description\n                toc_ws.column_dimensions['D'].width = 10  # Rows\n                toc_ws.column_dimensions['E'].width = 10  # Columns\n                toc_ws.column_dimensions['F'].width = 15  # Source\n\n                # Set row heights for better readability\n                for row_idx in range(2, len(toc_data) + 2):\n                    toc_ws.row_dimensions[row_idx].height = 30\n\n            # Save the workbook\n            excel_path = output_dir / \"tables.xlsx\"\n            wb.save(excel_path)\n\n        except Exception as e:\n            print(f\"\u274c Error creating Excel file: {e}\")\n\n\n    def _get_heading_level(self, style_name: str) -&gt; int:\n        \"\"\"Extract heading level from style name.\"\"\"\n        if style_name.startswith('Heading'):\n            try:\n                return int(style_name.split()[-1])\n            except:\n                return 1\n        return 0\n\n    def _extract_formatting(self, paragraph: Paragraph) -&gt; Dict[str, Any]:\n        \"\"\"Extract formatting information from paragraph.\"\"\"\n        formatting = {\n            'bold': False,\n            'italic': False,\n            'underline': False,\n            'font_size': None,\n            'font_name': None\n        }\n\n        try:\n            for run in paragraph.runs:\n                if run.bold:\n                    formatting['bold'] = True\n                if run.italic:\n                    formatting['italic'] = True\n                if run.underline:\n                    formatting['underline'] = True\n                if run.font.size:\n                    formatting['font_size'] = run.font.size.pt\n                if run.font.name:\n                    formatting['font_name'] = run.font.name\n        except:\n            pass\n\n        return formatting\n\n    def _generate_html_table(self, table_data: List[List[str]]) -&gt; str:\n        \"\"\"Generate HTML table from table data.\"\"\"\n        if not table_data:\n            return \"\"\n\n        html = [\"&lt;table border='1'&gt;\"]\n\n        for row_idx, row in enumerate(table_data):\n            html.append(\"&lt;tr&gt;\")\n            for cell in row:\n                tag = \"th\" if row_idx == 0 else \"td\"\n                html.append(f\"&lt;{tag}&gt;{cell}&lt;/{tag}&gt;\")\n            html.append(\"&lt;/tr&gt;\")\n\n        html.append(\"&lt;/table&gt;\")\n        return '\\n'.join(html)\n</code></pre>"},{"location":"api/parsers.html#doctra.parsers.structured_docx_parser.StructuredDOCXParser.__init__","title":"<code>__init__(*, use_vlm=False, vlm_provider='gemini', vlm_model=None, vlm_api_key=None, extract_images=True, preserve_formatting=True, table_detection=True, export_excel=True)</code>","text":"<p>Initialize the StructuredDOCXParser with processing configuration.</p> <p>:param use_vlm: Whether to use VLM for structured data extraction (default: False) :param vlm_provider: VLM provider to use (\"gemini\", \"openai\", \"anthropic\", or \"openrouter\", default: \"gemini\") :param vlm_model: Model name to use (defaults to provider-specific defaults) :param vlm_api_key: API key for VLM provider (required if use_vlm is True) :param extract_images: Whether to extract embedded images (default: True) :param preserve_formatting: Whether to preserve text formatting in output (default: True) :param table_detection: Whether to detect and extract tables (default: True) :param export_excel: Whether to export tables to Excel file (default: True)</p> Source code in <code>doctra/parsers/structured_docx_parser.py</code> <pre><code>def __init__(\n    self,\n    *,\n    use_vlm: bool = False,\n    vlm_provider: str = \"gemini\",\n    vlm_model: str | None = None,\n    vlm_api_key: str | None = None,\n    extract_images: bool = True,\n    preserve_formatting: bool = True,\n    table_detection: bool = True,\n    export_excel: bool = True,\n):\n    \"\"\"\n    Initialize the StructuredDOCXParser with processing configuration.\n\n    :param use_vlm: Whether to use VLM for structured data extraction (default: False)\n    :param vlm_provider: VLM provider to use (\"gemini\", \"openai\", \"anthropic\", or \"openrouter\", default: \"gemini\")\n    :param vlm_model: Model name to use (defaults to provider-specific defaults)\n    :param vlm_api_key: API key for VLM provider (required if use_vlm is True)\n    :param extract_images: Whether to extract embedded images (default: True)\n    :param preserve_formatting: Whether to preserve text formatting in output (default: True)\n    :param table_detection: Whether to detect and extract tables (default: True)\n    :param export_excel: Whether to export tables to Excel file (default: True)\n    \"\"\"\n    if Document is None:\n        raise ImportError(\"python-docx is required for DOCX parsing. Install with: pip install python-docx\")\n\n    self.use_vlm = use_vlm\n    self.extract_images = extract_images\n    self.preserve_formatting = preserve_formatting\n    self.table_detection = table_detection\n    self.export_excel = export_excel\n    self.vlm = None\n\n    if self.use_vlm:\n        try:\n            from doctra.engines.vlm.service import VLMStructuredExtractor\n            self.vlm = VLMStructuredExtractor(\n                vlm_provider=vlm_provider,\n                vlm_model=vlm_model,\n                api_key=vlm_api_key,\n            )\n        except Exception as e:\n            print(f\"Warning: VLM initialization failed: {e}\")\n            self.vlm = None\n</code></pre>"},{"location":"api/parsers.html#doctra.parsers.structured_docx_parser.StructuredDOCXParser.parse","title":"<code>parse(docx_path)</code>","text":"<p>Parse a DOCX document and extract all content.</p> <p>:param docx_path: Path to the DOCX file to parse</p> Source code in <code>doctra/parsers/structured_docx_parser.py</code> <pre><code>def parse(self, docx_path: str) -&gt; None:\n    \"\"\"\n    Parse a DOCX document and extract all content.\n\n    :param docx_path: Path to the DOCX file to parse\n    \"\"\"\n    if not os.path.exists(docx_path):\n        raise FileNotFoundError(f\"DOCX file not found: {docx_path}\")\n\n    docx_path = Path(docx_path)\n    output_dir = Path(f\"outputs/{docx_path.stem}\")\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    print(f\"\ud83d\udcc4 Processing DOCX: {docx_path.name}\")\n\n    try:\n        # Load the DOCX document\n        doc = Document(docx_path)\n\n        # Extract document structure\n        document_data = self._extract_document_structure(doc)\n\n        # Extract images if enabled\n        images_data = []\n        if self.extract_images:\n            images_data = self._extract_images(doc, output_dir)\n\n        # Extract tables from the document structure\n        tables_data = [elem for elem in document_data['elements'] if elem['type'] == 'table']\n\n        # Calculate total steps based on number of images\n        if self.use_vlm and self.vlm and images_data:\n            total_steps = len(images_data)  # One step per image\n        else:\n            total_steps = 1  # Just one step if no VLM processing\n\n        # Create progress bar\n        progress_bar = tqdm(total=total_steps, desc=\"Processing DOCX\", unit=\"image\")\n\n        # Process VLM data first if enabled\n        vlm_extracted_data = []\n        if self.use_vlm and self.vlm and images_data:\n            vlm_extracted_data = self._process_vlm_data(images_data, output_dir, progress_bar)\n        else:\n            # If no VLM processing, just update the progress bar\n            progress_bar.update(1)\n\n        progress_bar.close()\n\n        # Generate outputs (no progress bar for these as they're fast)\n        self._generate_markdown_output(document_data, images_data, output_dir, vlm_extracted_data)\n        self._generate_html_output(document_data, images_data, output_dir, vlm_extracted_data)\n\n        # Generate Excel output if enabled\n        if self.export_excel:\n            if vlm_extracted_data:\n                self._generate_excel_output_with_vlm(tables_data, vlm_extracted_data, output_dir)\n            else:\n                self._generate_excel_output(tables_data, output_dir)\n\n        print(f\"\u2705 DOCX parsing completed successfully!\")\n        print(f\"\ud83d\udcca Extracted: {len(document_data.get('paragraphs', []))} paragraphs, \"\n              f\"{len(tables_data)} tables, {len(images_data)} images\")\n\n    except Exception as e:\n        print(f\"\u274c Error parsing DOCX: {e}\")\n        raise\n</code></pre>"},{"location":"api/parsers.html#quick-reference","title":"Quick Reference","text":""},{"location":"api/parsers.html#structuredpdfparser_1","title":"StructuredPDFParser","text":"<pre><code>from doctra import StructuredPDFParser\n\nparser = StructuredPDFParser(\n    # Layout Detection\n    layout_model_name: str = \"PP-DocLayout_plus-L\",\n    dpi: int = 200,\n    min_score: float = 0.0,\n\n    # OCR Settings\n    ocr_lang: str = \"eng\",\n    ocr_psm: int = 4,\n    ocr_oem: int = 3,\n    ocr_extra_config: str = \"\",\n\n    # VLM Settings\n    use_vlm: bool = False,\n    vlm_provider: str = None,\n    vlm_api_key: str = None,\n    vlm_model: str = None,\n\n    # Split Table Merging\n    merge_split_tables: bool = False,\n    bottom_threshold_ratio: float = 0.20,\n    top_threshold_ratio: float = 0.15,\n    max_gap_ratio: float = 0.25,\n    column_alignment_tolerance: float = 10.0,\n    min_merge_confidence: float = 0.65,\n\n    # Output Settings\n    box_separator: str = \"\\n\"\n)\n\n# Parse document\nparser.parse(\n    pdf_path: str,\n    output_base_dir: str = \"outputs\"\n)\n\n# Visualize layout\nparser.display_pages_with_boxes(\n    pdf_path: str,\n    num_pages: int = 3,\n    cols: int = 2,\n    page_width: int = 800,\n    spacing: int = 40,\n    save_path: str = None\n)\n</code></pre>"},{"location":"api/parsers.html#enhancedpdfparser_1","title":"EnhancedPDFParser","text":"<pre><code>from doctra import EnhancedPDFParser\n\nparser = EnhancedPDFParser(\n    # Image Restoration\n    use_image_restoration: bool = True,\n    restoration_task: str = \"appearance\",\n    restoration_device: str = None,\n    restoration_dpi: int = 200,\n\n    # All StructuredPDFParser parameters...\n)\n\n# Parse with enhancement\nparser.parse(\n    pdf_path: str,\n    output_base_dir: str = \"outputs\"\n)\n</code></pre>"},{"location":"api/parsers.html#charttablepdfparser_1","title":"ChartTablePDFParser","text":"<pre><code>from doctra import ChartTablePDFParser\n\nparser = ChartTablePDFParser(\n    # Extraction Settings\n    extract_charts: bool = True,\n    extract_tables: bool = True,\n\n    # VLM Settings\n    use_vlm: bool = False,\n    vlm_provider: str = None,\n    vlm_api_key: str = None,\n    vlm_model: str = None,\n\n    # Layout Detection\n    layout_model_name: str = \"PP-DocLayout_plus-L\",\n    dpi: int = 200,\n    min_score: float = 0.0\n)\n\n# Extract charts/tables\nparser.parse(\n    pdf_path: str,\n    output_base_dir: str = \"outputs\"\n)\n</code></pre>"},{"location":"api/parsers.html#structureddocxparser_1","title":"StructuredDOCXParser","text":"<pre><code>from doctra import StructuredDOCXParser\n\nparser = StructuredDOCXParser(\n    # VLM Settings\n    use_vlm: bool = False,\n    vlm_provider: str = None,\n    vlm_api_key: str = None,\n    vlm_model: str = None,\n\n    # Processing Options\n    extract_images: bool = True,\n    preserve_formatting: bool = True,\n    table_detection: bool = True,\n    export_excel: bool = True\n)\n\n# Parse DOCX document\nparser.parse(\n    docx_path: str\n)\n</code></pre>"},{"location":"api/parsers.html#parameter-reference","title":"Parameter Reference","text":""},{"location":"api/parsers.html#layout-detection-parameters","title":"Layout Detection Parameters","text":"Parameter Type Default Description <code>layout_model_name</code> str \"PP-DocLayout_plus-L\" PaddleOCR layout detection model <code>dpi</code> int 200 Image resolution for rendering PDF pages <code>min_score</code> float 0.0 Minimum confidence score for detected elements"},{"location":"api/parsers.html#ocr-parameters","title":"OCR Parameters","text":"Parameter Type Default Description <code>ocr_lang</code> str \"eng\" Tesseract language code <code>ocr_psm</code> int 4 Page segmentation mode <code>ocr_oem</code> int 3 OCR engine mode <code>ocr_extra_config</code> str \"\" Additional Tesseract configuration"},{"location":"api/parsers.html#vlm-parameters","title":"VLM Parameters","text":"Parameter Type Default Description <code>use_vlm</code> bool False Enable VLM processing <code>vlm_provider</code> str None Provider: \"openai\", \"gemini\", \"anthropic\", \"openrouter\" <code>vlm_api_key</code> str None API key for the VLM provider <code>vlm_model</code> str None Specific model to use (provider-dependent)"},{"location":"api/parsers.html#image-restoration-parameters","title":"Image Restoration Parameters","text":"Parameter Type Default Description <code>use_image_restoration</code> bool True Enable image restoration <code>restoration_task</code> str \"appearance\" Restoration task type <code>restoration_device</code> str None Device: \"cuda\", \"cpu\", or None (auto-detect) <code>restoration_dpi</code> int 200 DPI for restoration processing"},{"location":"api/parsers.html#split-table-merging-parameters","title":"Split Table Merging Parameters","text":"Parameter Type Default Description <code>merge_split_tables</code> bool False Enable automatic detection and merging of tables split across pages <code>bottom_threshold_ratio</code> float 0.20 Ratio (0-1) for detecting tables near bottom of page. Tables within this ratio from the bottom are considered candidates. <code>top_threshold_ratio</code> float 0.15 Ratio (0-1) for detecting tables near top of page. Tables within this ratio from the top are considered candidates. <code>max_gap_ratio</code> float 0.25 Maximum allowed gap between table segments as ratio of page height. Accounts for headers, footers, and page margins. <code>column_alignment_tolerance</code> float 10.0 Pixel tolerance for column alignment validation when comparing table structures. <code>min_merge_confidence</code> float 0.65 Minimum confidence score (0-1) required to merge two table segments. Higher values are more conservative."},{"location":"api/parsers.html#extraction-parameters","title":"Extraction Parameters","text":"Parameter Type Default Description <code>extract_charts</code> bool True Extract chart elements <code>extract_tables</code> bool True Extract table elements"},{"location":"api/parsers.html#docx-processing-parameters","title":"DOCX Processing Parameters","text":"Parameter Type Default Description <code>extract_images</code> bool True Extract embedded images from DOCX <code>preserve_formatting</code> bool True Preserve text formatting in output <code>table_detection</code> bool True Detect and extract tables <code>export_excel</code> bool True Export tables to Excel file"},{"location":"api/parsers.html#output-parameters","title":"Output Parameters","text":"Parameter Type Default Description <code>box_separator</code> str \"\\n\" Separator between detected elements"},{"location":"api/parsers.html#return-values","title":"Return Values","text":""},{"location":"api/parsers.html#parse-method","title":"parse() Method","text":"<p>Returns: <code>None</code></p> <p>Generates output files in the specified <code>output_base_dir</code>:</p> <pre><code>outputs/\n\u2514\u2500\u2500 &lt;document_name&gt;/\n    \u251c\u2500\u2500 full_parse/  # or 'enhanced_parse/', 'structured_parsing/'\n    \u2502   \u251c\u2500\u2500 result.md\n    \u2502   \u251c\u2500\u2500 result.html\n    \u2502   \u251c\u2500\u2500 tables.xlsx  # If VLM enabled\n    \u2502   \u251c\u2500\u2500 tables.html  # If VLM enabled\n    \u2502   \u251c\u2500\u2500 vlm_items.json  # If VLM enabled\n    \u2502   \u2514\u2500\u2500 images/\n    \u2502       \u251c\u2500\u2500 figures/\n    \u2502       \u251c\u2500\u2500 charts/\n    \u2502       \u2514\u2500\u2500 tables/\n</code></pre> <p>For DOCX parsing, generates:</p> <pre><code>outputs/\n\u2514\u2500\u2500 &lt;document_name&gt;/\n    \u251c\u2500\u2500 document.md\n    \u251c\u2500\u2500 document.html\n    \u251c\u2500\u2500 tables.xlsx  # With Table of Contents\n    \u2514\u2500\u2500 images/\n        \u251c\u2500\u2500 image1.png\n        \u251c\u2500\u2500 image2.jpg\n        \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"api/parsers.html#display_pages_with_boxes-method","title":"display_pages_with_boxes() Method","text":"<p>Returns: <code>None</code></p> <p>Displays or saves visualization of layout detection.</p>"},{"location":"api/parsers.html#error-handling","title":"Error Handling","text":"<p>All parsers may raise:</p> <ul> <li><code>FileNotFoundError</code>: PDF file not found</li> <li><code>ValueError</code>: Invalid parameter values</li> <li><code>RuntimeError</code>: Processing errors (e.g., Poppler not found)</li> <li><code>APIError</code>: VLM API errors (when VLM enabled)</li> </ul> <p>Example error handling:</p> <pre><code>from doctra import StructuredPDFParser\n\nparser = StructuredPDFParser()\n\ntry:\n    parser.parse(\"document.pdf\")\nexcept FileNotFoundError:\n    print(\"PDF file not found!\")\nexcept ValueError as e:\n    print(f\"Invalid parameter: {e}\")\nexcept RuntimeError as e:\n    print(f\"Processing error: {e}\")\n</code></pre>"},{"location":"api/parsers.html#examples","title":"Examples","text":"<p>See the Examples section for detailed usage examples.</p>"},{"location":"api/utils.html","title":"Utilities API Reference","text":"<p>Documentation for Doctra's utility functions and helpers.</p>"},{"location":"api/utils.html#overview","title":"Overview","text":"<p>Utility modules provide helper functions for common tasks.</p>"},{"location":"api/utils.html#available-utilities","title":"Available Utilities","text":""},{"location":"api/utils.html#file-operations","title":"File Operations","text":"<ul> <li>PDF I/O operations</li> <li>Image loading and saving</li> <li>Directory management</li> </ul>"},{"location":"api/utils.html#bounding-box-operations","title":"Bounding Box Operations","text":"<ul> <li>Coordinate transformations</li> <li>Box intersection and union</li> <li>Box filtering and sorting</li> </ul>"},{"location":"api/utils.html#progress-tracking","title":"Progress Tracking","text":"<ul> <li>Progress bar management</li> <li>Status reporting</li> </ul>"},{"location":"api/utils.html#ocr-utilities","title":"OCR Utilities","text":"<ul> <li>Text cleaning and normalization</li> <li>Language detection helpers</li> </ul>"},{"location":"api/utils.html#see-also","title":"See Also","text":"<ul> <li>Core Concepts - Understanding the architecture</li> <li>API Reference - Main API documentation</li> </ul>"},{"location":"contributing/code-of-conduct.html","title":"Code of Conduct","text":"<p>See our CODE_OF_CONDUCT.md in the repository root.</p>"},{"location":"contributing/development.html","title":"Development Guide","text":"<p>Thank you for your interest in contributing to Doctra! This guide will help you get started.</p>"},{"location":"contributing/development.html#getting-started","title":"Getting Started","text":""},{"location":"contributing/development.html#development-setup","title":"Development Setup","text":"<ol> <li>Fork and Clone</li> </ol> <pre><code>git clone https://github.com/YOUR_USERNAME/Doctra.git\ncd Doctra\n</code></pre> <ol> <li>Create Virtual Environment</li> </ol> <pre><code>python -m venv venv\nsource venv/bin/activate  # Linux/Mac\n# or\n.\\venv\\Scripts\\activate  # Windows\n</code></pre> <ol> <li>Install Development Dependencies</li> </ol> <pre><code>pip install -e \".[dev]\"\n</code></pre> <p>This installs Doctra in editable mode with development tools.</p> <ol> <li>Install System Dependencies</li> </ol> <p>Follow the Installation Guide for Poppler.</p>"},{"location":"contributing/development.html#project-structure","title":"Project Structure","text":"<pre><code>Doctra/\n\u251c\u2500\u2500 doctra/              # Main package\n\u2502   \u251c\u2500\u2500 parsers/         # PDF parsers\n\u2502   \u251c\u2500\u2500 engines/         # Processing engines\n\u2502   \u251c\u2500\u2500 exporters/       # Output formatters\n\u2502   \u251c\u2500\u2500 ui/              # Web interface\n\u2502   \u251c\u2500\u2500 cli/             # Command line interface\n\u2502   \u2514\u2500\u2500 utils/           # Utilities\n\u251c\u2500\u2500 tests/               # Test suite\n\u251c\u2500\u2500 docs/                # Documentation\n\u251c\u2500\u2500 examples/            # Example scripts\n\u251c\u2500\u2500 notebooks/           # Jupyter notebooks\n\u2514\u2500\u2500 setup.py             # Package configuration\n</code></pre>"},{"location":"contributing/development.html#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/development.html#1-create-a-branch","title":"1. Create a Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n</code></pre> <p>Branch naming conventions:</p> <ul> <li><code>feature/</code> - New features</li> <li><code>fix/</code> - Bug fixes</li> <li><code>docs/</code> - Documentation updates</li> <li><code>refactor/</code> - Code refactoring</li> <li><code>test/</code> - Test additions/updates</li> </ul>"},{"location":"contributing/development.html#2-make-changes","title":"2. Make Changes","text":"<p>Write clean, well-documented code following our Code Style.</p>"},{"location":"contributing/development.html#3-run-tests","title":"3. Run Tests","text":"<pre><code>pytest tests/\n</code></pre> <p>Run specific test:</p> <pre><code>pytest tests/test_structured_pdf_parser.py\n</code></pre> <p>With coverage:</p> <pre><code>pytest --cov=doctra tests/\n</code></pre>"},{"location":"contributing/development.html#4-format-code","title":"4. Format Code","text":"<pre><code># Format with Black\nblack doctra tests\n\n# Sort imports\nisort doctra tests\n\n# Lint with Flake8\nflake8 doctra tests\n</code></pre>"},{"location":"contributing/development.html#5-type-checking","title":"5. Type Checking","text":"<pre><code>mypy doctra\n</code></pre>"},{"location":"contributing/development.html#6-commit-changes","title":"6. Commit Changes","text":"<pre><code>git add .\ngit commit -m \"feat: add new feature description\"\n</code></pre> <p>Commit message format:</p> <ul> <li><code>feat:</code> - New feature</li> <li><code>fix:</code> - Bug fix</li> <li><code>docs:</code> - Documentation</li> <li><code>style:</code> - Formatting</li> <li><code>refactor:</code> - Code restructuring</li> <li><code>test:</code> - Tests</li> <li><code>chore:</code> - Maintenance</li> </ul>"},{"location":"contributing/development.html#7-push-and-create-pr","title":"7. Push and Create PR","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then create a Pull Request on GitHub.</p>"},{"location":"contributing/development.html#code-style","title":"Code Style","text":""},{"location":"contributing/development.html#python-style-guide","title":"Python Style Guide","text":"<p>We follow PEP 8 with these configurations:</p> <pre><code># .flake8\n[flake8]\nmax-line-length = 88\nextend-ignore = E203, W503\nexclude = .git,__pycache__,docs,build,dist\n</code></pre>"},{"location":"contributing/development.html#code-formatting","title":"Code Formatting","text":"<pre><code># Black configuration in pyproject.toml\n[tool.black]\nline-length = 88\ntarget-version = ['py38', 'py39', 'py310', 'py311', 'py312']\n</code></pre>"},{"location":"contributing/development.html#import-sorting","title":"Import Sorting","text":"<pre><code># isort configuration in pyproject.toml\n[tool.isort]\nprofile = \"black\"\nmulti_line_output = 3\n</code></pre>"},{"location":"contributing/development.html#example-code","title":"Example Code","text":"<pre><code>\"\"\"Module docstring explaining purpose.\"\"\"\n\nfrom typing import Optional, Union\n\nimport numpy as np\nfrom PIL import Image\n\nfrom doctra.utils import helper_function\n\n\nclass MyParser:\n    \"\"\"Class docstring explaining purpose.\n\n    Args:\n        param1: Description of param1\n        param2: Description of param2\n\n    Attributes:\n        attribute1: Description\n    \"\"\"\n\n    def __init__(self, param1: str, param2: int = 10):\n        \"\"\"Initialize the parser.\"\"\"\n        self.param1 = param1\n        self.param2 = param2\n\n    def process(self, input_data: Union[str, np.ndarray]) -&gt; Optional[Image.Image]:\n        \"\"\"Process input data.\n\n        Args:\n            input_data: Input to process\n\n        Returns:\n            Processed image or None\n\n        Raises:\n            ValueError: If input is invalid\n        \"\"\"\n        if not self._validate(input_data):\n            raise ValueError(\"Invalid input\")\n\n        return self._do_process(input_data)\n\n    def _validate(self, data) -&gt; bool:\n        \"\"\"Private helper method.\"\"\"\n        return data is not None\n</code></pre>"},{"location":"contributing/development.html#testing","title":"Testing","text":""},{"location":"contributing/development.html#writing-tests","title":"Writing Tests","text":"<p>Create tests in <code>tests/</code> directory:</p> <pre><code>import pytest\nfrom doctra.parsers import StructuredPDFParser\n\n\ndef test_parser_initialization():\n    \"\"\"Test parser can be initialized.\"\"\"\n    parser = StructuredPDFParser()\n    assert parser is not None\n\n\ndef test_parse_basic_pdf():\n    \"\"\"Test parsing a basic PDF.\"\"\"\n    parser = StructuredPDFParser()\n    result = parser.parse(\"test_data/sample.pdf\")\n    assert result is not None\n\n\n@pytest.mark.parametrize(\"dpi\", [100, 200, 300])\ndef test_different_dpi_settings(dpi):\n    \"\"\"Test parser with different DPI settings.\"\"\"\n    parser = StructuredPDFParser(dpi=dpi)\n    assert parser.dpi == dpi\n</code></pre>"},{"location":"contributing/development.html#running-tests","title":"Running Tests","text":"<pre><code># All tests\npytest\n\n# Specific file\npytest tests/test_parsers.py\n\n# Specific test\npytest tests/test_parsers.py::test_parser_initialization\n\n# With verbose output\npytest -v\n\n# With coverage\npytest --cov=doctra --cov-report=html\n\n# Stop on first failure\npytest -x\n</code></pre>"},{"location":"contributing/development.html#test-coverage","title":"Test Coverage","text":"<p>Aim for &gt;80% code coverage:</p> <pre><code>pytest --cov=doctra --cov-report=term-missing\n</code></pre>"},{"location":"contributing/development.html#documentation","title":"Documentation","text":""},{"location":"contributing/development.html#building-documentation","title":"Building Documentation","text":"<pre><code># Install documentation dependencies\npip install -r docs/requirements.txt\n\n# Build and serve locally\nmkdocs serve\n\n# Build static site\nmkdocs build\n</code></pre> <p>View at: http://127.0.0.1:8000</p>"},{"location":"contributing/development.html#writing-documentation","title":"Writing Documentation","text":"<ul> <li>Use Markdown for all documentation</li> <li>Add docstrings to all public APIs</li> <li>Include code examples</li> <li>Update relevant docs when adding features</li> </ul>"},{"location":"contributing/development.html#docstring-format","title":"Docstring Format","text":"<p>We use Google-style docstrings:</p> <pre><code>def function(param1: str, param2: int) -&gt; bool:\n    \"\"\"Short description.\n\n    Longer description if needed.\n\n    Args:\n        param1: Description of param1\n        param2: Description of param2\n\n    Returns:\n        Description of return value\n\n    Raises:\n        ValueError: When param1 is invalid\n\n    Examples:\n        &gt;&gt;&gt; function(\"test\", 5)\n        True\n    \"\"\"\n    pass\n</code></pre>"},{"location":"contributing/development.html#pull-request-guidelines","title":"Pull Request Guidelines","text":""},{"location":"contributing/development.html#before-submitting","title":"Before Submitting","text":"<ul> <li> Tests pass: <code>pytest</code></li> <li> Code formatted: <code>black doctra tests</code></li> <li> Imports sorted: <code>isort doctra tests</code></li> <li> Linting clean: <code>flake8 doctra tests</code></li> <li> Type checking: <code>mypy doctra</code></li> <li> Documentation updated</li> <li> CHANGELOG.md updated</li> </ul>"},{"location":"contributing/development.html#pr-description-template","title":"PR Description Template","text":"<pre><code>## Description\nBrief description of changes\n\n## Type of Change\n- [ ] Bug fix\n- [ ] New feature\n- [ ] Breaking change\n- [ ] Documentation update\n\n## Testing\nDescribe testing done\n\n## Checklist\n- [ ] Tests pass\n- [ ] Code formatted\n- [ ] Documentation updated\n- [ ] CHANGELOG updated\n</code></pre>"},{"location":"contributing/development.html#review-process","title":"Review Process","text":"<ol> <li>Automated checks run (tests, linting)</li> <li>Code review by maintainers</li> <li>Requested changes addressed</li> <li>Approved and merged</li> </ol>"},{"location":"contributing/development.html#common-tasks","title":"Common Tasks","text":""},{"location":"contributing/development.html#adding-a-new-parser","title":"Adding a New Parser","text":"<ol> <li>Create parser file: <code>doctra/parsers/new_parser.py</code></li> <li>Implement parser class</li> <li>Add tests: <code>tests/test_new_parser.py</code></li> <li>Update <code>doctra/__init__.py</code></li> <li>Add documentation: <code>docs/user-guide/parsers/new-parser.md</code></li> <li>Add API reference: <code>docs/api/parsers.md</code></li> </ol>"},{"location":"contributing/development.html#adding-a-new-feature","title":"Adding a New Feature","text":"<ol> <li>Create feature branch</li> <li>Implement feature with tests</li> <li>Update documentation</li> <li>Submit PR with description</li> </ol>"},{"location":"contributing/development.html#fixing-a-bug","title":"Fixing a Bug","text":"<ol> <li>Create test that reproduces bug</li> <li>Fix bug</li> <li>Verify test passes</li> <li>Submit PR referencing issue</li> </ol>"},{"location":"contributing/development.html#development-tools","title":"Development Tools","text":""},{"location":"contributing/development.html#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Install pre-commit hooks:</p> <pre><code>pre-commit install\n</code></pre> <p>This runs checks before each commit:</p> <ul> <li>Black formatting</li> <li>isort import sorting</li> <li>Flake8 linting</li> <li>Trailing whitespace removal</li> </ul>"},{"location":"contributing/development.html#ide-setup","title":"IDE Setup","text":""},{"location":"contributing/development.html#vs-code","title":"VS Code","text":"<p>Recommended <code>settings.json</code>:</p> <pre><code>{\n    \"python.formatting.provider\": \"black\",\n    \"python.linting.enabled\": true,\n    \"python.linting.flake8Enabled\": true,\n    \"python.linting.mypyEnabled\": true,\n    \"editor.formatOnSave\": true,\n    \"[python]\": {\n        \"editor.codeActionsOnSave\": {\n            \"source.organizeImports\": true\n        }\n    }\n}\n</code></pre>"},{"location":"contributing/development.html#pycharm","title":"PyCharm","text":"<ul> <li>Enable Black formatter</li> <li>Enable Flake8 linter</li> <li>Enable mypy type checker</li> </ul>"},{"location":"contributing/development.html#getting-help","title":"Getting Help","text":"<ul> <li>Questions: Open a GitHub Discussion</li> <li>Bugs: Report in GitHub Issues</li> <li>Chat: Join our community (link in README)</li> </ul>"},{"location":"contributing/development.html#code-of-conduct","title":"Code of Conduct","text":"<p>Please read and follow our Code of Conduct.</p>"},{"location":"contributing/development.html#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the MIT License.</p>"},{"location":"examples/advanced-examples.html","title":"Advanced Examples","text":"<p>Advanced usage patterns and integration examples.</p>"},{"location":"examples/advanced-examples.html#multi-stage-processing-pipeline","title":"Multi-Stage Processing Pipeline","text":"<pre><code>from doctra import DocResEngine, StructuredPDFParser\n\n# Stage 1: Restore document\nengine = DocResEngine(device=\"cuda\")\nenhanced_pdf = engine.restore_pdf(\n    pdf_path=\"low_quality.pdf\",\n    output_path=\"enhanced.pdf\",\n    task=\"appearance\"\n)\n\n# Stage 2: Parse enhanced document\nparser = StructuredPDFParser(\n    use_vlm=True,\n    vlm_provider=\"openai\",\n    vlm_api_key=\"your-key\"\n)\nparser.parse(enhanced_pdf)\n</code></pre>"},{"location":"examples/advanced-examples.html#custom-processing-with-different-vlm-providers","title":"Custom Processing with Different VLM Providers","text":"<pre><code>from doctra import ChartTablePDFParser\n\n# Using OpenAI\nparser_openai = ChartTablePDFParser(\n    use_vlm=True,\n    vlm_provider=\"openai\",\n    vlm_api_key=\"sk-xxx\"\n)\n\n# Using Gemini (cost-effective)\nparser_gemini = ChartTablePDFParser(\n    use_vlm=True,\n    vlm_provider=\"gemini\",\n    vlm_api_key=\"gemini-key\"\n)\n\n# Using Qianfan ERNIE (Baidu AI Cloud)\nparser_qianfan = ChartTablePDFParser(\n    use_vlm=True,\n    vlm_provider=\"qianfan\",\n    vlm_api_key=\"qianfan-key\",\n    vlm_model=\"ernie-4.5-turbo-vl-32k\"\n)\n\n# Parse with different providers\nparser_openai.parse(\"doc.pdf\")\nparser_gemini.parse(\"doc.pdf\")\nparser_qianfan.parse(\"doc.pdf\")\n</code></pre>"},{"location":"examples/advanced-examples.html#parallel-batch-processing","title":"Parallel Batch Processing","text":"<pre><code>from concurrent.futures import ThreadPoolExecutor\nfrom doctra import StructuredPDFParser\n\ndef process_pdf(pdf_path):\n    parser = StructuredPDFParser()\n    try:\n        parser.parse(pdf_path)\n        return f\"Success: {pdf_path}\"\n    except Exception as e:\n        return f\"Error {pdf_path}: {e}\"\n\n# Process multiple PDFs in parallel\npdf_files = [\"doc1.pdf\", \"doc2.pdf\", \"doc3.pdf\"]\n\nwith ThreadPoolExecutor(max_workers=3) as executor:\n    results = executor.map(process_pdf, pdf_files)\n\nfor result in results:\n    print(result)\n</code></pre>"},{"location":"examples/advanced-examples.html#dynamic-dpi-selection","title":"Dynamic DPI Selection","text":"<pre><code>import os\nfrom doctra import StructuredPDFParser\n\ndef smart_parse(pdf_path):\n    # Choose DPI based on file size\n    file_size = os.path.getsize(pdf_path) / (1024 * 1024)  # MB\n\n    if file_size &lt; 5:\n        dpi = 200  # Standard quality\n    elif file_size &lt; 20:\n        dpi = 150  # Lower for large files\n    else:\n        dpi = 100  # Very low for huge files\n\n    parser = StructuredPDFParser(dpi=dpi)\n    print(f\"Processing {pdf_path} at {dpi} DPI\")\n    parser.parse(pdf_path)\n\nsmart_parse(\"document.pdf\")\n</code></pre>"},{"location":"examples/advanced-examples.html#integration-with-data-analysis","title":"Integration with Data Analysis","text":"<pre><code>from doctra import ChartTablePDFParser\nimport pandas as pd\n\n# Extract tables\nparser = ChartTablePDFParser(\n    extract_tables=True,\n    use_vlm=True,\n    vlm_provider=\"openai\",\n    vlm_api_key=\"your-key\"\n)\n\nparser.parse(\"financial_report.pdf\")\n\n# Load and analyze extracted data\nexcel_path = \"outputs/financial_report/structured_parsing/parsed_tables_charts.xlsx\"\nxls = pd.ExcelFile(excel_path)\n\nfor sheet_name in xls.sheet_names:\n    df = pd.read_excel(xls, sheet_name=sheet_name)\n    print(f\"\\nTable: {sheet_name}\")\n    print(df.describe())\n</code></pre>"},{"location":"examples/advanced-examples.html#see-also","title":"See Also","text":"<ul> <li>Basic Examples - Simpler examples</li> <li>Integration - Integration patterns</li> <li>API Reference - API documentation</li> </ul>"},{"location":"examples/basic-usage.html","title":"Basic Usage Examples","text":"<p>Practical examples for common Doctra use cases.</p>"},{"location":"examples/basic-usage.html#example-1-parse-a-simple-pdf","title":"Example 1: Parse a Simple PDF","text":"<pre><code>from doctra import StructuredPDFParser\n\n# Initialize parser\nparser = StructuredPDFParser()\n\n# Parse document\nparser.parse(\"document.pdf\")\n\n# Output saved to: outputs/document/full_parse/\n</code></pre>"},{"location":"examples/basic-usage.html#example-2-parse-with-custom-settings","title":"Example 2: Parse with Custom Settings","text":"<pre><code>from doctra import StructuredPDFParser\n\nparser = StructuredPDFParser(\n    dpi=250,  # Higher quality\n    min_score=0.7,  # More confident detections\n    ocr_lang=\"eng\"  # English language\n)\n\nparser.parse(\"document.pdf\", output_base_dir=\"my_results\")\n</code></pre>"},{"location":"examples/basic-usage.html#example-3-enhanced-parsing-for-scanned-documents","title":"Example 3: Enhanced Parsing for Scanned Documents","text":"<pre><code>from doctra import EnhancedPDFParser\n\nparser = EnhancedPDFParser(\n    use_image_restoration=True,\n    restoration_task=\"appearance\",\n    restoration_device=\"cuda\"  # Use GPU\n)\n\nparser.parse(\"scanned_document.pdf\")\n</code></pre>"},{"location":"examples/basic-usage.html#example-4-extract-structured-data-with-vlm","title":"Example 4: Extract Structured Data with VLM","text":"<pre><code>from doctra import StructuredPDFParser\n\nparser = StructuredPDFParser(\n    use_vlm=True,\n    vlm_provider=\"openai\",\n    vlm_api_key=\"your-api-key-here\"\n)\n\nparser.parse(\"data_report.pdf\")\n\n# Output includes:\n# - tables.xlsx with extracted data\n# - tables.html with formatted tables\n# - vlm_items.json with structured data\n</code></pre>"},{"location":"examples/basic-usage.html#example-5-extract-only-charts","title":"Example 5: Extract Only Charts","text":"<pre><code>from doctra import ChartTablePDFParser\n\nparser = ChartTablePDFParser(\n    extract_charts=True,\n    extract_tables=False\n)\n\nparser.parse(\"presentation.pdf\")\n</code></pre>"},{"location":"examples/basic-usage.html#example-6-visualize-layout-detection","title":"Example 6: Visualize Layout Detection","text":"<pre><code>from doctra import StructuredPDFParser\n\nparser = StructuredPDFParser()\n\n# Display layout detection\nparser.display_pages_with_boxes(\n    pdf_path=\"document.pdf\",\n    num_pages=3,\n    save_path=\"layout_visualization.png\"\n)\n</code></pre>"},{"location":"examples/basic-usage.html#example-7-standalone-image-restoration","title":"Example 7: Standalone Image Restoration","text":"<pre><code>from doctra import DocResEngine\n\n# Initialize restoration engine\nengine = DocResEngine(device=\"cuda\")\n\n# Restore a single image\nrestored_img, metadata = engine.restore_image(\n    image=\"blurry_document.jpg\",\n    task=\"deblurring\"\n)\n\n# Save result\nrestored_img.save(\"restored.jpg\")\nprint(f\"Processed in {metadata['processing_time']:.2f}s\")\n</code></pre>"},{"location":"examples/basic-usage.html#example-8-batch-processing","title":"Example 8: Batch Processing","text":"<pre><code>import os\nfrom doctra import StructuredPDFParser\n\nparser = StructuredPDFParser()\n\n# Process all PDFs in directory\npdf_directory = \"documents\"\nfor filename in os.listdir(pdf_directory):\n    if filename.endswith(\".pdf\"):\n        pdf_path = os.path.join(pdf_directory, filename)\n        print(f\"Processing {filename}...\")\n        parser.parse(pdf_path)\n        print(f\"Completed {filename}\")\n</code></pre>"},{"location":"examples/basic-usage.html#example-9-error-handling","title":"Example 9: Error Handling","text":"<pre><code>from doctra import StructuredPDFParser\n\nparser = StructuredPDFParser()\n\ntry:\n    parser.parse(\"document.pdf\")\n    print(\"Processing successful!\")\nexcept FileNotFoundError:\n    print(\"Error: PDF file not found\")\nexcept Exception as e:\n    print(f\"Error during processing: {e}\")\n</code></pre>"},{"location":"examples/basic-usage.html#example-10-using-the-web-ui","title":"Example 10: Using the Web UI","text":"<pre><code>from doctra import launch_ui\n\n# Launch web interface\nlaunch_ui()\n\n# Opens browser at http://127.0.0.1:7860\n</code></pre>"},{"location":"examples/basic-usage.html#next-steps","title":"Next Steps","text":"<ul> <li>Advanced Examples - Complex use cases</li> <li>Integration Examples - Integrate with other tools</li> <li>API Reference - Detailed API documentation</li> </ul>"},{"location":"examples/integration.html","title":"Integration Examples","text":"<p>Examples of integrating Doctra with other tools and frameworks.</p>"},{"location":"examples/integration.html#flask-web-application","title":"Flask Web Application","text":"<pre><code>from flask import Flask, request, jsonify, send_file\nfrom doctra import StructuredPDFParser\nimport os\n\napp = Flask(__name__)\nparser = StructuredPDFParser()\n\n@app.route('/parse', methods=['POST'])\ndef parse_document():\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n\n    file = request.files['file']\n\n    # Save uploaded file\n    pdf_path = f\"uploads/{file.filename}\"\n    file.save(pdf_path)\n\n    # Parse document\n    try:\n        parser.parse(pdf_path)\n        return jsonify({\n            'status': 'success',\n            'output_dir': f\"outputs/{file.filename.replace('.pdf', '')}\"\n        })\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\nif __name__ == '__main__':\n    os.makedirs('uploads', exist_ok=True)\n    app.run(debug=True)\n</code></pre>"},{"location":"examples/integration.html#fastapi-service","title":"FastAPI Service","text":"<pre><code>from fastapi import FastAPI, File, UploadFile, BackgroundTasks\nfrom doctra import StructuredPDFParser\nimport shutil\n\napp = FastAPI()\nparser = StructuredPDFParser()\n\n@app.post(\"/parse\")\nasync def parse_pdf(\n    background_tasks: BackgroundTasks,\n    file: UploadFile = File(...)\n):\n    # Save file\n    file_path = f\"temp/{file.filename}\"\n    with open(file_path, \"wb\") as buffer:\n        shutil.copyfileobj(file.file, buffer)\n\n    # Queue processing\n    background_tasks.add_task(parser.parse, file_path)\n\n    return {\"status\": \"processing\", \"filename\": file.filename}\n</code></pre>"},{"location":"examples/integration.html#database-integration","title":"Database Integration","text":"<pre><code>from doctra import StructuredPDFParser\nimport sqlite3\nimport json\n\ndef store_results_in_db(pdf_path, db_path=\"documents.db\"):\n    # Parse document\n    parser = StructuredPDFParser(use_vlm=True)\n    parser.parse(pdf_path)\n\n    # Connect to database\n    conn = sqlite3.connect(db_path)\n    cursor = conn.cursor()\n\n    # Create table\n    cursor.execute('''\n        CREATE TABLE IF NOT EXISTS documents (\n            id INTEGER PRIMARY KEY,\n            filename TEXT,\n            num_pages INTEGER,\n            content TEXT,\n            metadata TEXT\n        )\n    ''')\n\n    # Load results\n    result_path = f\"outputs/{os.path.basename(pdf_path).replace('.pdf', '')}/full_parse/result.md\"\n    with open(result_path) as f:\n        content = f.read()\n\n    # Store in database\n    cursor.execute(\n        \"INSERT INTO documents (filename, content) VALUES (?, ?)\",\n        (pdf_path, content)\n    )\n\n    conn.commit()\n    conn.close()\n</code></pre>"},{"location":"examples/integration.html#aws-lambda-function","title":"AWS Lambda Function","text":"<pre><code>import json\nimport boto3\nfrom doctra import StructuredPDFParser\n\ns3 = boto3.client('s3')\nparser = StructuredPDFParser()\n\ndef lambda_handler(event, context):\n    # Get PDF from S3\n    bucket = event['bucket']\n    key = event['key']\n\n    # Download file\n    local_path = f\"/tmp/{key}\"\n    s3.download_file(bucket, key, local_path)\n\n    # Parse document\n    parser.parse(local_path, output_base_dir=\"/tmp/outputs\")\n\n    # Upload results back to S3\n    output_dir = f\"/tmp/outputs/{key.replace('.pdf', '')}\"\n    # ... upload logic ...\n\n    return {\n        'statusCode': 200,\n        'body': json.dumps('Processing complete')\n    }\n</code></pre>"},{"location":"examples/integration.html#see-also","title":"See Also","text":"<ul> <li>Basic Examples - Getting started</li> <li>Advanced Examples - Complex patterns</li> <li>API Reference - API documentation</li> </ul>"},{"location":"getting-started/installation.html","title":"Installation","text":"<p>This guide will help you install Doctra and its dependencies on your system.</p>"},{"location":"getting-started/installation.html#requirements","title":"Requirements","text":"<ul> <li>Python 3.8 or higher</li> <li>pip package manager</li> <li>Poppler (for PDF processing)</li> <li>Tesseract OCR (automatically handled by dependencies)</li> </ul>"},{"location":"getting-started/installation.html#installing-doctra","title":"Installing Doctra","text":""},{"location":"getting-started/installation.html#from-pypi-recommended","title":"From PyPI (Recommended)","text":"<p>The easiest way to install Doctra is from PyPI using pip:</p> <pre><code>pip install doctra\n</code></pre> <p>This will install Doctra and all Python dependencies automatically.</p>"},{"location":"getting-started/installation.html#from-source","title":"From Source","text":"<p>To install the latest development version from source:</p> <pre><code>git clone https://github.com/AdemBoukhris457/Doctra.git\ncd Doctra\npip install -e .\n</code></pre> <p>The <code>-e</code> flag installs in editable mode, which is useful for development.</p>"},{"location":"getting-started/installation.html#system-dependencies","title":"System Dependencies","text":"<p>Doctra requires Poppler for PDF processing. Follow the instructions for your operating system:</p>"},{"location":"getting-started/installation.html#ubuntudebian","title":"Ubuntu/Debian","text":"<pre><code>sudo apt-get update\nsudo apt-get install poppler-utils\n</code></pre>"},{"location":"getting-started/installation.html#macos","title":"macOS","text":"<p>Using Homebrew:</p> <pre><code>brew install poppler\n</code></pre> <p>If you don't have Homebrew, install it from brew.sh.</p>"},{"location":"getting-started/installation.html#simple-windows-windows","title":":simple-windows: Windows","text":""},{"location":"getting-started/installation.html#option-1-using-conda","title":"Option 1: Using Conda","text":"<pre><code>conda install -c conda-forge poppler\n</code></pre>"},{"location":"getting-started/installation.html#option-2-manual-installation","title":"Option 2: Manual Installation","text":"<ol> <li>Download Poppler for Windows from this link</li> <li>Extract the archive</li> <li>Add the <code>bin</code> directory to your system PATH</li> </ol>"},{"location":"getting-started/installation.html#google-colab","title":"Google Colab","text":"<pre><code>!apt-get install poppler-utils\n</code></pre>"},{"location":"getting-started/installation.html#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"getting-started/installation.html#vlm-providers","title":"VLM Providers","text":"<p>To use Vision Language Models for structured data extraction, install the appropriate provider:</p>"},{"location":"getting-started/installation.html#openai","title":"OpenAI","text":"<pre><code>pip install doctra[openai]\n</code></pre>"},{"location":"getting-started/installation.html#google-gemini","title":"Google Gemini","text":"<pre><code>pip install doctra[gemini]\n</code></pre>"},{"location":"getting-started/installation.html#all-vlm-providers","title":"All VLM Providers","text":"<pre><code>pip install doctra[openai,gemini]\n</code></pre>"},{"location":"getting-started/installation.html#development-dependencies","title":"Development Dependencies","text":"<p>For contributing to Doctra:</p> <pre><code>pip install doctra[dev]\n</code></pre> <p>This installs testing, linting, and formatting tools.</p>"},{"location":"getting-started/installation.html#verifying-installation","title":"Verifying Installation","text":"<p>After installation, verify that Doctra is installed correctly:</p> <pre><code>import doctra\nprint(doctra.__version__)\n</code></pre> <p>You should see the version number printed (e.g., <code>0.4.3</code>).</p>"},{"location":"getting-started/installation.html#check-system-dependencies","title":"Check System Dependencies","text":"<p>To check if Poppler is installed correctly:</p> <pre><code>pdftoppm -v\n</code></pre> <p>You should see the Poppler version information.</p>"},{"location":"getting-started/installation.html#gpu-support","title":"GPU Support","text":""},{"location":"getting-started/installation.html#cuda-for-faster-processing","title":"CUDA for Faster Processing","text":"<p>Doctra can leverage GPU acceleration for image restoration tasks. To enable GPU support:</p> <ol> <li>Install CUDA-compatible PyTorch:</li> </ol> <pre><code>pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n</code></pre> <ol> <li>Verify CUDA is available:</li> </ol> <pre><code>import torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n</code></pre>"},{"location":"getting-started/installation.html#paddlepaddle-gpu-support","title":"PaddlePaddle GPU Support","text":"<p>For GPU-accelerated layout detection:</p> <pre><code>pip uninstall paddlepaddle\npip install paddlepaddle-gpu\n</code></pre> <p>GPU Requirements</p> <p>GPU support requires:</p> <ul> <li>NVIDIA GPU with CUDA Compute Capability 3.5+</li> <li>CUDA 11.8 or higher</li> <li>cuDNN 8.6 or higher</li> </ul>"},{"location":"getting-started/installation.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation.html#importerror-no-module-named-doctra","title":"ImportError: No module named 'doctra'","text":"<p>Solution: Ensure Doctra is installed in your active Python environment:</p> <pre><code>pip list | grep doctra\n</code></pre> <p>If not listed, reinstall with <code>pip install doctra</code>.</p>"},{"location":"getting-started/installation.html#poppler-not-found","title":"Poppler not found","text":"<p>Symptoms: Error message mentioning \"pdftoppm\" or \"Poppler\"</p> <p>Solution: </p> <ol> <li>Verify Poppler installation: <code>pdftoppm -v</code></li> <li>If not installed, follow the System Dependencies section</li> <li>On Windows, ensure Poppler's <code>bin</code> directory is in your PATH</li> </ol>"},{"location":"getting-started/installation.html#cuda-out-of-memory","title":"CUDA out of memory","text":"<p>Solution: Use CPU processing or reduce DPI settings:</p> <pre><code>parser = StructuredPDFParser(\n    dpi=150,  # Reduce from default 200\n    restoration_device=\"cpu\"  # Force CPU usage\n)\n</code></pre>"},{"location":"getting-started/installation.html#paddleocr-model-download-fails","title":"PaddleOCR model download fails","text":"<p>Solution: Manually download models or check your network connection:</p> <pre><code>from doctra.parsers import StructuredPDFParser\n\n# This will trigger model download\nparser = StructuredPDFParser()\n</code></pre> <p>Models are downloaded to <code>~/.paddleocr/</code> on first use.</p>"},{"location":"getting-started/installation.html#next-steps","title":"Next Steps","text":"<p>Now that you have Doctra installed, check out:</p> <ul> <li>Quick Start - Your first Doctra program</li> <li>System Requirements - Detailed hardware requirements</li> <li>User Guide - Learn about core concepts</li> </ul>"},{"location":"getting-started/installation.html#getting-help","title":"Getting Help","text":"<p>If you encounter issues during installation:</p> <ol> <li>Check the GitHub Issues for similar problems</li> <li>Create a new issue with:<ul> <li>Your operating system and version</li> <li>Python version (<code>python --version</code>)</li> <li>Full error message</li> <li>Installation method used</li> </ul> </li> </ol>"},{"location":"getting-started/quick-start.html","title":"Quick Start","text":"<p>This guide will get you started with Doctra in just a few minutes.</p>"},{"location":"getting-started/quick-start.html#your-first-document-parse","title":"Your First Document Parse","text":"<p>Let's parse a PDF document and extract its content:</p> <pre><code>from doctra import StructuredPDFParser\n\n# Initialize the parser\nparser = StructuredPDFParser()\n\n# Parse a document\nparser.parse(\"document.pdf\")\n</code></pre> <p>That's it! Doctra will:</p> <ol> <li>Detect the document layout</li> <li>Extract text using OCR</li> <li>Save images of figures, charts, and tables</li> <li>Generate a Markdown file with all content</li> </ol>"},{"location":"getting-started/quick-start.html#understanding-the-output","title":"Understanding the Output","text":"<p>After parsing, you'll find the following structure:</p> <pre><code>outputs/\n\u2514\u2500\u2500 document/\n    \u251c\u2500\u2500 full_parse/\n    \u2502   \u251c\u2500\u2500 result.md          # Markdown with all content\n    \u2502   \u251c\u2500\u2500 result.html        # HTML version\n    \u2502   \u2514\u2500\u2500 images/            # Extracted visual elements\n    \u2502       \u251c\u2500\u2500 figures/       # Document figures\n    \u2502       \u251c\u2500\u2500 charts/        # Charts and graphs\n    \u2502       \u2514\u2500\u2500 tables/        # Table images\n</code></pre>"},{"location":"getting-started/quick-start.html#basic-examples","title":"Basic Examples","text":""},{"location":"getting-started/quick-start.html#parse-with-custom-output-directory","title":"Parse with Custom Output Directory","text":"<pre><code>from doctra import StructuredPDFParser\n\nparser = StructuredPDFParser()\nparser.parse(\"document.pdf\", output_base_dir=\"my_outputs\")\n</code></pre>"},{"location":"getting-started/quick-start.html#parse-scanned-documents","title":"Parse Scanned Documents","text":"<p>For scanned or low-quality documents, use the enhanced parser:</p> <pre><code>from doctra import EnhancedPDFParser\n\nparser = EnhancedPDFParser(\n    use_image_restoration=True,\n    restoration_task=\"appearance\"  # Improve overall appearance\n)\n\nparser.parse(\"scanned_document.pdf\")\n</code></pre>"},{"location":"getting-started/quick-start.html#extract-only-charts-and-tables","title":"Extract Only Charts and Tables","text":"<p>If you only need charts and tables:</p> <pre><code>from doctra import ChartTablePDFParser\n\nparser = ChartTablePDFParser(\n    extract_charts=True,\n    extract_tables=True\n)\n\nparser.parse(\"data_report.pdf\")\n</code></pre>"},{"location":"getting-started/quick-start.html#using-vision-language-models","title":"Using Vision Language Models","text":"<p>To convert charts and tables to structured data, add VLM support:</p> <pre><code>from doctra import StructuredPDFParser\n\nparser = StructuredPDFParser(\n    use_vlm=True,\n    vlm_provider=\"openai\",\n    vlm_api_key=\"your-api-key-here\"\n)\n\nparser.parse(\"document.pdf\")\n</code></pre> <p>This will generate:</p> <ul> <li><code>tables.xlsx</code> - Excel file with extracted table data</li> <li><code>tables.html</code> - HTML tables for web viewing</li> <li><code>vlm_items.json</code> - JSON with structured data</li> </ul> <p>VLM Providers</p> <p>Doctra supports multiple VLM providers:</p> <ul> <li><code>\"openai\"</code> - GPT-4 Vision and GPT-4o</li> <li><code>\"gemini\"</code> - Google's Gemini models</li> <li><code>\"anthropic\"</code> - Claude with vision</li> <li><code>\"openrouter\"</code> - Access multiple models</li> <li><code>\"qianfan\"</code> - Baidu AI Cloud ERNIE models</li> <li><code>\"ollama\"</code> - Local models (no API key required)</li> </ul>"},{"location":"getting-started/quick-start.html#document-restoration","title":"Document Restoration","text":"<p>Enhance document quality before parsing:</p> <pre><code>from doctra import DocResEngine\n\n# Initialize restoration engine\ndocres = DocResEngine(device=\"cuda\")  # Use GPU for speed\n\n# Restore a single image\nrestored_img, metadata = docres.restore_image(\n    image=\"blurry_doc.jpg\",\n    task=\"deblurring\"\n)\n\n# Or enhance an entire PDF\ndocres.restore_pdf(\n    pdf_path=\"low_quality.pdf\",\n    output_path=\"enhanced.pdf\",\n    task=\"appearance\"\n)\n</code></pre> <p>Available restoration tasks:</p> Task Description <code>appearance</code> General appearance enhancement <code>dewarping</code> Correct perspective distortion <code>deshadowing</code> Remove shadows <code>deblurring</code> Reduce blur <code>binarization</code> Convert to black and white <code>end2end</code> Complete restoration pipeline"},{"location":"getting-started/quick-start.html#using-the-web-ui","title":"Using the Web UI","text":"<p>Launch the graphical interface for easy document processing:</p> <pre><code>from doctra import launch_ui\n\n# Launch web interface\nlaunch_ui()\n</code></pre> <p>Or from the command line:</p> <pre><code>python -m doctra.ui.app\n</code></pre> <p>Then open your browser to the displayed URL (typically <code>http://127.0.0.1:7860</code>).</p>"},{"location":"getting-started/quick-start.html#command-line-interface","title":"Command Line Interface","text":"<p>Doctra provides a powerful CLI:</p> <pre><code># Parse a document\ndoctra parse document.pdf\n\n# Enhanced parsing\ndoctra enhance document.pdf --restoration-task appearance\n\n# Extract charts and tables\ndoctra extract both document.pdf --use-vlm\n\n# Visualize layout\ndoctra visualize document.pdf\n</code></pre> <p>See the CLI Reference for all available commands.</p>"},{"location":"getting-started/quick-start.html#layout-visualization","title":"Layout Visualization","text":"<p>Visualize how Doctra detects document elements:</p> <pre><code>from doctra import StructuredPDFParser\n\nparser = StructuredPDFParser()\n\n# Display layout detection results\nparser.display_pages_with_boxes(\n    pdf_path=\"document.pdf\",\n    num_pages=3,  # First 3 pages\n    save_path=\"layout_viz.png\"\n)\n</code></pre> <p>This creates a visual representation showing:</p> <ul> <li>Detected text regions (blue boxes)</li> <li>Tables (red boxes)</li> <li>Charts (green boxes)</li> <li>Figures (orange boxes)</li> <li>Confidence scores for each element</li> </ul>"},{"location":"getting-started/quick-start.html#configuration-options","title":"Configuration Options","text":""},{"location":"getting-started/quick-start.html#parser-configuration","title":"Parser Configuration","text":"<pre><code>parser = StructuredPDFParser(\n    # Layout Detection\n    layout_model_name=\"PP-DocLayout_plus-L\",  # Model choice\n    dpi=200,  # Image resolution\n    min_score=0.5,  # Confidence threshold\n\n    # OCR Settings\n    ocr_lang=\"eng\",  # Language code\n    ocr_psm=6,  # Page segmentation mode\n\n    # Output\n    box_separator=\"\\n\"  # Separator between elements\n)\n</code></pre>"},{"location":"getting-started/quick-start.html#enhanced-parser-configuration","title":"Enhanced Parser Configuration","text":"<pre><code>from doctra import EnhancedPDFParser\n\nparser = EnhancedPDFParser(\n    # Image Restoration\n    use_image_restoration=True,\n    restoration_task=\"dewarping\",\n    restoration_device=\"cuda\",  # or \"cpu\"\n    restoration_dpi=300,\n\n    # All StructuredPDFParser options also available\n    use_vlm=True,\n    vlm_provider=\"openai\",\n    vlm_api_key=\"your-key\"\n)\n</code></pre>"},{"location":"getting-started/quick-start.html#common-patterns","title":"Common Patterns","text":""},{"location":"getting-started/quick-start.html#batch-processing","title":"Batch Processing","text":"<pre><code>import os\nfrom doctra import StructuredPDFParser\n\nparser = StructuredPDFParser()\n\n# Process all PDFs in a directory\npdf_dir = \"documents\"\nfor filename in os.listdir(pdf_dir):\n    if filename.endswith(\".pdf\"):\n        pdf_path = os.path.join(pdf_dir, filename)\n        print(f\"Processing {filename}...\")\n        parser.parse(pdf_path)\n</code></pre>"},{"location":"getting-started/quick-start.html#error-handling","title":"Error Handling","text":"<pre><code>from doctra import StructuredPDFParser\n\nparser = StructuredPDFParser()\n\ntry:\n    parser.parse(\"document.pdf\")\nexcept FileNotFoundError:\n    print(\"Document not found!\")\nexcept Exception as e:\n    print(f\"Error parsing document: {e}\")\n</code></pre>"},{"location":"getting-started/quick-start.html#progress-tracking","title":"Progress Tracking","text":"<pre><code>from doctra import StructuredPDFParser\n\nparser = StructuredPDFParser()\n\n# Progress bars are shown automatically\nparser.parse(\"large_document.pdf\")\n</code></pre>"},{"location":"getting-started/quick-start.html#next-steps","title":"Next Steps","text":"<p>Now that you've learned the basics:</p> <ol> <li>Dive Deeper: Read the User Guide for detailed explanations</li> <li>Explore Parsers: Learn about each parser's capabilities</li> <li>Advanced Examples: Check out Advanced Examples</li> <li>API Reference: Browse the API Documentation</li> </ol>"},{"location":"getting-started/quick-start.html#getting-help","title":"Getting Help","text":"<ul> <li> Read the full documentation</li> <li> Check GitHub issues</li> <li> Ask questions in discussions</li> </ul>"},{"location":"getting-started/quick-start.html#common-issues","title":"Common Issues","text":""},{"location":"getting-started/quick-start.html#poppler-not-found-error","title":"\"Poppler not found\" Error","text":"<p>Install Poppler (see Installation).</p>"},{"location":"getting-started/quick-start.html#low-ocr-accuracy","title":"Low OCR Accuracy","text":"<p>Try the enhanced parser with image restoration:</p> <pre><code>from doctra import EnhancedPDFParser\n\nparser = EnhancedPDFParser(\n    use_image_restoration=True,\n    restoration_task=\"appearance\"\n)\n</code></pre>"},{"location":"getting-started/quick-start.html#slow-processing","title":"Slow Processing","text":"<p>Use GPU acceleration:</p> <pre><code>parser = EnhancedPDFParser(\n    restoration_device=\"cuda\"  # Use GPU\n)\n</code></pre> <p>Or reduce DPI:</p> <pre><code>parser = StructuredPDFParser(\n    dpi=150  # Lower resolution\n)\n</code></pre>"},{"location":"getting-started/system-requirements.html","title":"System Requirements","text":"<p>This page outlines the hardware and software requirements for running Doctra effectively.</p>"},{"location":"getting-started/system-requirements.html#python-requirements","title":"Python Requirements","text":"<ul> <li>Python Version: 3.8 or higher</li> <li>Operating Systems: <ul> <li>Linux (Ubuntu, Debian, CentOS, etc.)</li> <li>macOS (10.13 or higher)</li> <li>Windows (10 or higher)</li> </ul> </li> </ul>"},{"location":"getting-started/system-requirements.html#hardware-requirements","title":"Hardware Requirements","text":""},{"location":"getting-started/system-requirements.html#minimum-requirements","title":"Minimum Requirements","text":"Component Specification CPU Dual-core processor, 2.0 GHz RAM 4 GB Disk Space 2 GB for installation + space for outputs GPU Not required (CPU processing available)"},{"location":"getting-started/system-requirements.html#recommended-requirements","title":"Recommended Requirements","text":"Component Specification CPU Quad-core processor, 3.0 GHz or higher RAM 8 GB or more Disk Space 10 GB for installation + models + outputs GPU NVIDIA GPU with 4+ GB VRAM (for acceleration)"},{"location":"getting-started/system-requirements.html#performance-considerations","title":"Performance Considerations","text":""},{"location":"getting-started/system-requirements.html#processing-speed","title":"Processing Speed","text":"<p>Typical processing times for a 10-page PDF:</p> Configuration Time CPU only (4 cores) ~2-3 minutes GPU (NVIDIA GTX 1060) ~1-2 minutes GPU (NVIDIA RTX 3080) ~30-60 seconds <p>Factors Affecting Performance</p> <ul> <li>Document complexity (number of images, tables, charts)</li> <li>Image resolution (DPI setting)</li> <li>Image restoration enabled/disabled</li> <li>VLM processing (requires network calls)</li> </ul>"},{"location":"getting-started/system-requirements.html#memory-usage","title":"Memory Usage","text":"<p>Expected RAM usage:</p> <ul> <li>Basic parsing: 500 MB - 2 GB</li> <li>Enhanced parsing: 1 GB - 4 GB</li> <li>VLM processing: Additional 500 MB - 1 GB</li> <li>High DPI (300+): Additional 2-4 GB</li> </ul>"},{"location":"getting-started/system-requirements.html#software-dependencies","title":"Software Dependencies","text":""},{"location":"getting-started/system-requirements.html#required","title":"Required","text":"<ol> <li> <p>Poppler - PDF rendering and processing</p> <ul> <li>Version: Latest stable release</li> <li>Installation: See Installation Guide</li> </ul> </li> <li> <p>Tesseract OCR - Text extraction</p> <ul> <li>Automatically installed via Python dependencies</li> <li>No manual installation required</li> </ul> </li> </ol>"},{"location":"getting-started/system-requirements.html#optional","title":"Optional","text":"<ol> <li> <p>CUDA Toolkit - For GPU acceleration</p> <ul> <li>Version: 11.8 or higher</li> <li>Required only for GPU processing</li> <li>Download: NVIDIA CUDA Downloads</li> </ul> </li> <li> <p>cuDNN - Deep learning GPU acceleration</p> <ul> <li>Version: 8.6 or higher</li> <li>Required only for GPU processing</li> <li>Download: NVIDIA cuDNN Downloads</li> </ul> </li> </ol>"},{"location":"getting-started/system-requirements.html#gpu-support","title":"GPU Support","text":""},{"location":"getting-started/system-requirements.html#cuda-requirements","title":"CUDA Requirements","text":"<p>For GPU-accelerated processing:</p> <ul> <li>GPU: NVIDIA GPU with Compute Capability 3.5 or higher</li> <li>CUDA: Version 11.8 or higher</li> <li>cuDNN: Version 8.6 or higher</li> <li>Driver: Compatible NVIDIA driver</li> </ul>"},{"location":"getting-started/system-requirements.html#supported-gpus","title":"Supported GPUs","text":"<p>Doctra's image restoration works with CUDA-capable NVIDIA GPUs:</p> GPU Series Support Level GeForce GTX 10xx and newer \u2705 Full support GeForce RTX series \u2705 Full support Tesla series \u2705 Full support Quadro series \u2705 Full support AMD GPUs \u274c Not supported Intel GPUs \u274c Not supported"},{"location":"getting-started/system-requirements.html#checking-gpu-compatibility","title":"Checking GPU Compatibility","text":"<p>Verify CUDA availability:</p> <pre><code>import torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"CUDA version: {torch.version.cuda}\")\nprint(f\"GPU: {torch.cuda.get_device_name(0)}\")\n</code></pre>"},{"location":"getting-started/system-requirements.html#network-requirements","title":"Network Requirements","text":""},{"location":"getting-started/system-requirements.html#model-downloads","title":"Model Downloads","text":"<p>On first use, Doctra downloads AI models:</p> <ul> <li>PaddleOCR models: ~300 MB</li> <li>DocRes models: ~200 MB</li> <li>Total: ~500 MB initial download</li> </ul> <p>Models are cached locally after first download.</p>"},{"location":"getting-started/system-requirements.html#vlm-api-access","title":"VLM API Access","text":"<p>If using Vision Language Models:</p> <ul> <li>Stable internet connection required</li> <li>API rate limits apply (provider-dependent)</li> <li>Bandwidth: Minimal (images are compressed before sending)</li> </ul>"},{"location":"getting-started/system-requirements.html#storage-requirements","title":"Storage Requirements","text":""},{"location":"getting-started/system-requirements.html#installation","title":"Installation","text":"Component Size Doctra package ~50 MB Python dependencies ~500 MB AI models (downloaded on first use) ~500 MB Total ~1 GB"},{"location":"getting-started/system-requirements.html#processing-outputs","title":"Processing Outputs","text":"<p>Expected output sizes per document:</p> Document Size Output Size (approx.) 10-page report 5-20 MB 50-page document 25-100 MB 100-page book 50-200 MB <p>Storage Planning</p> <p>Plan for 2-10x the original PDF size for outputs, depending on:</p> <ul> <li>Number of images in the document</li> <li>DPI settings used</li> <li>Whether image restoration is enabled</li> </ul>"},{"location":"getting-started/system-requirements.html#browser-requirements-web-ui","title":"Browser Requirements (Web UI)","text":"<p>For the Gradio-based web interface:</p> <ul> <li>Modern Browser: Chrome 90+, Firefox 88+, Safari 14+, Edge 90+</li> <li>JavaScript: Must be enabled</li> <li>Local Network: Access to localhost required</li> </ul>"},{"location":"getting-started/system-requirements.html#cloud-deployment","title":"Cloud Deployment","text":"<p>Doctra can run on cloud platforms:</p>"},{"location":"getting-started/system-requirements.html#recommended-cloud-specs","title":"Recommended Cloud Specs","text":"Provider Instance Type vCPUs RAM GPU AWS t3.xlarge 4 16 GB Optional GCP n1-standard-4 4 15 GB Optional Azure Standard_D4s_v3 4 16 GB Optional <p>For GPU processing:</p> Provider Instance Type GPU VRAM AWS g4dn.xlarge T4 16 GB GCP n1-standard-4 + T4 T4 16 GB Azure NC6 K80 12 GB"},{"location":"getting-started/system-requirements.html#google-colab","title":"Google Colab","text":"<p>Doctra works perfectly in Google Colab:</p> <ul> <li>Free Tier: Sufficient for most use cases</li> <li>GPU: Available in free tier</li> <li>RAM: 12-13 GB in free tier</li> <li>Disk: 100+ GB temporary storage</li> </ul>"},{"location":"getting-started/system-requirements.html#operating-system-specific-notes","title":"Operating System Specific Notes","text":""},{"location":"getting-started/system-requirements.html#linux","title":"Linux","text":"<ul> <li>Best Performance: Generally fastest due to better CUDA support</li> <li>Easy Setup: Package managers make dependency installation simple</li> <li>Docker: Easy containerization for deployment</li> </ul>"},{"location":"getting-started/system-requirements.html#macos","title":"macOS","text":"<ul> <li>No GPU Support: CUDA not available on macOS</li> <li>Good CPU Performance: Efficient on Apple Silicon (M1/M2)</li> <li>Poppler: Easy installation via Homebrew</li> </ul>"},{"location":"getting-started/system-requirements.html#windows","title":"Windows","text":"<ul> <li>GPU Support: Full CUDA support available</li> <li>Poppler Setup: Requires manual installation or conda</li> <li>Path Configuration: May need to add Poppler to PATH</li> </ul>"},{"location":"getting-started/system-requirements.html#performance-optimization","title":"Performance Optimization","text":""},{"location":"getting-started/system-requirements.html#for-cpu-only-systems","title":"For CPU-Only Systems","text":"<pre><code>parser = StructuredPDFParser(\n    dpi=150,  # Lower resolution\n    min_score=0.7  # Higher threshold = fewer elements\n)\n</code></pre>"},{"location":"getting-started/system-requirements.html#for-gpu-systems","title":"For GPU Systems","text":"<pre><code>from doctra import EnhancedPDFParser\n\nparser = EnhancedPDFParser(\n    use_image_restoration=True,\n    restoration_device=\"cuda\",  # Use GPU\n    restoration_dpi=300  # Higher quality\n)\n</code></pre>"},{"location":"getting-started/system-requirements.html#memory-optimization","title":"Memory Optimization","text":"<pre><code># Process documents in batches\nimport os\nfrom doctra import StructuredPDFParser\n\nparser = StructuredPDFParser()\n\n# Process one at a time to manage memory\nfor pdf_file in pdf_files:\n    parser.parse(pdf_file)\n    # Parser is reused, memory is cleaned between documents\n</code></pre>"},{"location":"getting-started/system-requirements.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/system-requirements.html#out-of-memory-errors","title":"Out of Memory Errors","text":"<p>Solutions:</p> <ol> <li>Reduce DPI: <code>dpi=100</code></li> <li>Disable image restoration</li> <li>Close other applications</li> <li>Process fewer pages at once</li> </ol>"},{"location":"getting-started/system-requirements.html#slow-processing","title":"Slow Processing","text":"<p>Solutions:</p> <ol> <li>Enable GPU: <code>restoration_device=\"cuda\"</code></li> <li>Reduce DPI: <code>dpi=150</code></li> <li>Upgrade hardware</li> <li>Process during off-peak hours</li> </ol>"},{"location":"getting-started/system-requirements.html#model-download-failures","title":"Model Download Failures","text":"<p>Solutions:</p> <ol> <li>Check internet connection</li> <li>Verify firewall settings</li> <li>Use VPN if behind restrictive network</li> <li>Manual model download (see troubleshooting guide)</li> </ol>"},{"location":"getting-started/system-requirements.html#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide - Install Doctra</li> <li>Quick Start - Start using Doctra</li> <li>Performance Tips - Optimize your setup</li> </ul>"},{"location":"interfaces/cli.html","title":"Command Line Interface","text":"<p>Doctra provides a powerful CLI for document processing automation.</p>"},{"location":"interfaces/cli.html#installation","title":"Installation","text":"<p>The CLI is automatically installed with Doctra:</p> <pre><code>pip install doctra\n</code></pre> <p>Verify installation:</p> <pre><code>doctra --version\n</code></pre>"},{"location":"interfaces/cli.html#basic-usage","title":"Basic Usage","text":"<pre><code>doctra [COMMAND] [OPTIONS] [ARGUMENTS]\n</code></pre>"},{"location":"interfaces/cli.html#commands","title":"Commands","text":""},{"location":"interfaces/cli.html#parse","title":"parse","text":"<p>Parse a PDF document with full processing.</p> <pre><code>doctra parse &lt;pdf_file&gt; [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--output-dir PATH</code>: Output directory (default: <code>outputs</code>)</li> <li><code>--dpi INTEGER</code>: Image resolution (default: 200)</li> <li><code>--min-score FLOAT</code>: Minimum confidence score (default: 0.0)</li> <li><code>--ocr-lang TEXT</code>: OCR language code (default: <code>eng</code>)</li> <li><code>--use-vlm</code>: Enable VLM processing</li> <li><code>--vlm-provider TEXT</code>: VLM provider (<code>openai</code>, <code>gemini</code>, <code>anthropic</code>, <code>openrouter</code>)</li> <li><code>--vlm-api-key TEXT</code>: VLM API key</li> <li><code>--vlm-model TEXT</code>: Specific VLM model</li> </ul> <p>Example:</p> <pre><code># Basic parsing\ndoctra parse document.pdf\n\n# With custom settings\ndoctra parse document.pdf --dpi 300 --output-dir my_outputs\n\n# With VLM\ndoctra parse document.pdf --use-vlm --vlm-provider openai --vlm-api-key sk-xxx\n</code></pre>"},{"location":"interfaces/cli.html#parse-docx","title":"parse-docx","text":"<p>Parse a Microsoft Word document (.docx file).</p> <pre><code>doctra parse-docx &lt;docx_file&gt; [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--output-dir PATH</code>: Output directory (default: <code>outputs</code>)</li> <li><code>--use-vlm</code>: Enable VLM processing</li> <li><code>--vlm-provider TEXT</code>: VLM provider (<code>openai</code>, <code>gemini</code>, <code>anthropic</code>, <code>openrouter</code>)</li> <li><code>--vlm-api-key TEXT</code>: VLM API key</li> <li><code>--vlm-model TEXT</code>: Specific VLM model</li> <li><code>--extract-images</code>: Extract embedded images (default: True)</li> <li><code>--preserve-formatting</code>: Preserve text formatting (default: True)</li> <li><code>--table-detection</code>: Detect and extract tables (default: True)</li> <li><code>--export-excel</code>: Export tables to Excel file (default: True)</li> <li><code>--verbose</code>: Enable verbose output</li> </ul> <p>Examples:</p> <pre><code># Basic DOCX parsing\ndoctra parse-docx document.docx\n\n# With VLM enhancement\ndoctra parse-docx document.docx --use-vlm --vlm-provider openai --vlm-api-key sk-xxx\n\n# Custom options\ndoctra parse-docx document.docx \\\n  --extract-images \\\n  --preserve-formatting \\\n  --table-detection \\\n  --export-excel \\\n  --output-dir my_outputs\n</code></pre>"},{"location":"interfaces/cli.html#enhance","title":"enhance","text":"<p>Parse with image restoration for low-quality documents.</p> <pre><code>doctra enhance &lt;pdf_file&gt; [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li>All <code>parse</code> options, plus:</li> <li><code>--restoration-task TEXT</code>: Restoration task (default: <code>appearance</code>)<ul> <li>Choices: <code>appearance</code>, <code>dewarping</code>, <code>deshadowing</code>, <code>deblurring</code>, <code>binarization</code>, <code>end2end</code></li> </ul> </li> <li><code>--restoration-device TEXT</code>: Device (<code>cuda</code>, <code>cpu</code>, or auto)</li> <li><code>--restoration-dpi INTEGER</code>: DPI for restoration (default: 200)</li> </ul> <p>Example:</p> <pre><code># Basic enhancement\ndoctra enhance scanned.pdf\n\n# Dewarp with GPU\ndoctra enhance scanned.pdf --restoration-task dewarping --restoration-device cuda\n\n# Full enhancement with VLM\ndoctra enhance scanned.pdf \\\n  --restoration-task appearance \\\n  --restoration-device cuda \\\n  --use-vlm \\\n  --vlm-provider openai \\\n  --vlm-api-key sk-xxx\n</code></pre>"},{"location":"interfaces/cli.html#extract","title":"extract","text":"<p>Extract only charts and/or tables from a document.</p> <pre><code>doctra extract &lt;type&gt; &lt;pdf_file&gt; [OPTIONS]\n</code></pre> <p>Type:</p> <ul> <li><code>charts</code>: Extract only charts</li> <li><code>tables</code>: Extract only tables</li> <li><code>both</code>: Extract both charts and tables</li> </ul> <p>Options:</p> <ul> <li><code>--output-dir PATH</code>: Output directory (default: <code>outputs</code>)</li> <li><code>--dpi INTEGER</code>: Image resolution (default: 200)</li> <li><code>--use-vlm</code>: Enable VLM for structured data</li> <li><code>--vlm-provider TEXT</code>: VLM provider</li> <li><code>--vlm-api-key TEXT</code>: VLM API key</li> <li><code>--vlm-model TEXT</code>: Specific VLM model</li> </ul> <p>Examples:</p> <pre><code># Extract charts only\ndoctra extract charts report.pdf\n\n# Extract tables with VLM\ndoctra extract tables report.pdf --use-vlm --vlm-provider gemini --vlm-api-key xxx\n\n# Extract both\ndoctra extract both report.pdf --output-dir data_extracts\n</code></pre>"},{"location":"interfaces/cli.html#visualize","title":"visualize","text":"<p>Visualize layout detection results.</p> <pre><code>doctra visualize &lt;pdf_file&gt; [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--num-pages INTEGER</code>: Number of pages to visualize (default: 3)</li> <li><code>--cols INTEGER</code>: Number of columns in grid (default: 2)</li> <li><code>--page-width INTEGER</code>: Width of each page (default: 800)</li> <li><code>--spacing INTEGER</code>: Spacing between pages (default: 40)</li> <li><code>--output PATH</code>: Save to file instead of displaying</li> <li><code>--dpi INTEGER</code>: Image resolution (default: 200)</li> </ul> <p>Examples:</p> <pre><code># Display first 3 pages\ndoctra visualize document.pdf\n\n# Save visualization of 6 pages\ndoctra visualize document.pdf --num-pages 6 --output layout.png\n\n# Custom grid layout\ndoctra visualize document.pdf --num-pages 9 --cols 3 --page-width 600\n</code></pre>"},{"location":"interfaces/cli.html#analyze","title":"analyze","text":"<p>Quick document analysis showing structure.</p> <pre><code>doctra analyze &lt;pdf_file&gt; [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--dpi INTEGER</code>: Image resolution (default: 200)</li> </ul> <p>Example:</p> <pre><code>doctra analyze document.pdf\n</code></pre> <p>Output shows:</p> <pre><code>Document Analysis: document.pdf\n=====================================\nTotal pages: 10\n\nPage 1:\n  - Text regions: 5\n  - Tables: 1\n  - Charts: 0\n  - Figures: 2\n\nPage 2:\n  ...\n</code></pre>"},{"location":"interfaces/cli.html#info","title":"info","text":"<p>Display system and configuration information.</p> <pre><code>doctra info\n</code></pre> <p>Shows:</p> <ul> <li>Doctra version</li> <li>Python version</li> <li>Installed dependencies</li> <li>GPU availability</li> <li>System information</li> </ul> <p>Example output:</p> <pre><code>Doctra Information\n==================\nVersion: 0.4.3\nPython: 3.10.11\n\nDependencies:\n  - PaddlePaddle: 2.5.0\n  - PaddleOCR: 2.7.0\n  - PyTesseract: 0.3.10\n  - Pillow: 10.0.0\n\nSystem:\n  - OS: Windows 10\n  - CUDA Available: Yes\n  - GPU: NVIDIA GeForce RTX 3080\n</code></pre>"},{"location":"interfaces/cli.html#batch-processing","title":"Batch Processing","text":""},{"location":"interfaces/cli.html#process-multiple-files","title":"Process Multiple Files","text":"<pre><code># Using shell globbing\ndoctra parse *.pdf --output-dir batch_results\n\n# Using find (Linux/Mac)\nfind ./documents -name \"*.pdf\" -exec doctra parse {} \\;\n\n# Using PowerShell (Windows)\nGet-ChildItem *.pdf | ForEach-Object { doctra parse $_.FullName }\n</code></pre>"},{"location":"interfaces/cli.html#process-directory","title":"Process Directory","text":"<pre><code># Parse all PDFs in directory\nfor pdf in directory/*.pdf; do\n    doctra parse \"$pdf\" --output-dir results/\ndone\n</code></pre>"},{"location":"interfaces/cli.html#environment-variables","title":"Environment Variables","text":"<p>Set default values using environment variables:</p> <pre><code># VLM Configuration\nexport DOCTRA_VLM_PROVIDER=openai\nexport DOCTRA_VLM_API_KEY=sk-xxx\nexport DOCTRA_VLM_MODEL=gpt-4o\n\n# Processing Settings\nexport DOCTRA_DPI=200\nexport DOCTRA_OCR_LANG=eng\nexport DOCTRA_DEVICE=cuda\n\n# Then use without flags\ndoctra parse document.pdf --use-vlm\n</code></pre>"},{"location":"interfaces/cli.html#configuration-file","title":"Configuration File","text":"<p>Create <code>.doctra.yml</code> in your project directory:</p> <pre><code># .doctra.yml\nvlm:\n  provider: openai\n  api_key: sk-xxx\n  model: gpt-4o\n\nprocessing:\n  dpi: 200\n  ocr_lang: eng\n  device: cuda\n\noutput:\n  base_dir: outputs\n</code></pre> <p>Then run commands without options:</p> <pre><code>doctra parse document.pdf\n</code></pre>"},{"location":"interfaces/cli.html#output-structure","title":"Output Structure","text":""},{"location":"interfaces/cli.html#standard-parse","title":"Standard Parse","text":"<pre><code>outputs/\n\u2514\u2500\u2500 document/\n    \u2514\u2500\u2500 full_parse/\n        \u251c\u2500\u2500 result.md\n        \u251c\u2500\u2500 result.html\n        \u2514\u2500\u2500 images/\n            \u251c\u2500\u2500 figures/\n            \u251c\u2500\u2500 charts/\n            \u2514\u2500\u2500 tables/\n</code></pre>"},{"location":"interfaces/cli.html#enhanced-parse","title":"Enhanced Parse","text":"<pre><code>outputs/\n\u2514\u2500\u2500 document/\n    \u2514\u2500\u2500 enhanced_parse/\n        \u251c\u2500\u2500 result.md\n        \u251c\u2500\u2500 result.html\n        \u251c\u2500\u2500 document_enhanced.pdf  # Restored PDF\n        \u251c\u2500\u2500 enhanced_pages/  # Restored page images\n        \u2514\u2500\u2500 images/\n</code></pre>"},{"location":"interfaces/cli.html#extract_1","title":"Extract","text":"<pre><code>outputs/\n\u2514\u2500\u2500 document/\n    \u2514\u2500\u2500 structured_parsing/\n        \u251c\u2500\u2500 charts/  # Chart images\n        \u251c\u2500\u2500 tables/  # Table images\n        \u251c\u2500\u2500 parsed_tables_charts.xlsx  # If VLM enabled\n        \u251c\u2500\u2500 parsed_tables_charts.html  # If VLM enabled\n        \u2514\u2500\u2500 vlm_items.json  # If VLM enabled\n</code></pre>"},{"location":"interfaces/cli.html#docx-parse","title":"DOCX Parse","text":"<pre><code>outputs/\n\u2514\u2500\u2500 document/\n    \u251c\u2500\u2500 document.md\n    \u251c\u2500\u2500 document.html\n    \u251c\u2500\u2500 tables.xlsx  # With Table of Contents\n    \u2514\u2500\u2500 images/\n        \u251c\u2500\u2500 image1.png\n        \u251c\u2500\u2500 image2.jpg\n        \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"interfaces/cli.html#examples","title":"Examples","text":""},{"location":"interfaces/cli.html#example-1-basic-document-processing","title":"Example 1: Basic Document Processing","text":"<pre><code># Parse a financial report\ndoctra parse financial_report.pdf\n\n# Output: outputs/financial_report/full_parse/\n</code></pre>"},{"location":"interfaces/cli.html#example-2-enhanced-processing-with-vlm","title":"Example 2: Enhanced Processing with VLM","text":"<pre><code># Process scanned document with enhancement and VLM\ndoctra enhance scanned_document.pdf \\\n  --restoration-task appearance \\\n  --restoration-device cuda \\\n  --use-vlm \\\n  --vlm-provider openai \\\n  --vlm-api-key $OPENAI_API_KEY \\\n  --output-dir enhanced_results\n</code></pre>"},{"location":"interfaces/cli.html#example-3-docx-document-processing","title":"Example 3: DOCX Document Processing","text":"<pre><code># Basic DOCX parsing\ndoctra parse-docx report.docx\n\n# With VLM enhancement for structured data\ndoctra parse-docx financial_report.docx \\\n  --use-vlm \\\n  --vlm-provider openai \\\n  --vlm-api-key $OPENAI_API_KEY \\\n  --export-excel\n\n# Result: outputs/financial_report/document.md, document.html, tables.xlsx\n</code></pre>"},{"location":"interfaces/cli.html#example-4-extract-data-for-analysis","title":"Example 4: Extract Data for Analysis","text":"<pre><code># Extract all tables with VLM to get structured data\ndoctra extract tables data_report.pdf \\\n  --use-vlm \\\n  --vlm-provider gemini \\\n  --vlm-api-key $GEMINI_API_KEY\n\n# Result: outputs/data_report/structured_parsing/parsed_tables_charts.xlsx\n</code></pre>"},{"location":"interfaces/cli.html#example-5-batch-processing-pipeline","title":"Example 5: Batch Processing Pipeline","text":"<pre><code>#!/bin/bash\n# process_documents.sh\n\nINPUT_DIR=\"./input_pdfs\"\nOUTPUT_DIR=\"./processed\"\n\nfor pdf in \"$INPUT_DIR\"/*.pdf; do\n    echo \"Processing: $pdf\"\n\n    # First enhance the document\n    doctra enhance \"$pdf\" \\\n      --restoration-task appearance \\\n      --restoration-device cuda \\\n      --output-dir \"$OUTPUT_DIR\"\n\n    echo \"Completed: $pdf\"\ndone\n\necho \"All documents processed!\"\n</code></pre>"},{"location":"interfaces/cli.html#example-6-quality-check-with-visualization","title":"Example 6: Quality Check with Visualization","text":"<pre><code># Visualize layout detection before full processing\ndoctra visualize document.pdf --num-pages 5 --output viz_check.png\n\n# Review viz_check.png to ensure good detection\n\n# Then proceed with full processing\ndoctra parse document.pdf --use-vlm\n</code></pre>"},{"location":"interfaces/cli.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"interfaces/cli.html#command-not-found","title":"Command Not Found","text":"<p>Problem: <code>doctra: command not found</code></p> <p>Solution:</p> <pre><code># Ensure Doctra is installed\npip install doctra\n\n# Or use module syntax\npython -m doctra.cli.main parse document.pdf\n</code></pre>"},{"location":"interfaces/cli.html#api-key-errors","title":"API Key Errors","text":"<p>Problem: VLM API key not recognized</p> <p>Solution:</p> <pre><code># Set environment variable\nexport OPENAI_API_KEY=sk-xxx\n\n# Or pass directly\ndoctra parse document.pdf --use-vlm --vlm-api-key sk-xxx\n</code></pre>"},{"location":"interfaces/cli.html#poppler-errors","title":"Poppler Errors","text":"<p>Problem: <code>pdftoppm not found</code></p> <p>Solution: Install Poppler (see Installation Guide)</p>"},{"location":"interfaces/cli.html#memory-errors","title":"Memory Errors","text":"<p>Problem: Out of memory during processing</p> <p>Solution:</p> <pre><code># Reduce DPI\ndoctra parse large.pdf --dpi 150\n\n# Or process pages individually\ndoctra parse large.pdf --max-pages 10\n</code></pre>"},{"location":"interfaces/cli.html#advanced-usage","title":"Advanced Usage","text":""},{"location":"interfaces/cli.html#custom-scripts","title":"Custom Scripts","text":"<p>Combine CLI with shell scripts:</p> <pre><code>#!/bin/bash\n# Smart processing script\n\nPDF=$1\n\n# Check file size\nSIZE=$(du -k \"$PDF\" | cut -f1)\n\nif [ $SIZE -gt 10000 ]; then\n    echo \"Large file, using lower DPI...\"\n    doctra parse \"$PDF\" --dpi 150\nelse\n    echo \"Standard processing...\"\n    doctra parse \"$PDF\" --dpi 200 --use-vlm\nfi\n</code></pre>"},{"location":"interfaces/cli.html#integration-with-other-tools","title":"Integration with Other Tools","text":"<pre><code># OCR + Search Pipeline\ndoctra parse document.pdf\ngrep \"keyword\" outputs/document/full_parse/result.md\n\n# Extract data and analyze\ndoctra extract tables report.pdf --use-vlm\npython analyze_tables.py outputs/report/structured_parsing/parsed_tables_charts.xlsx\n</code></pre>"},{"location":"interfaces/cli.html#see-also","title":"See Also","text":"<ul> <li>Python API - Programmatic usage</li> <li>Web UI - Graphical interface</li> <li>Examples - Usage examples</li> </ul>"},{"location":"interfaces/web-ui.html","title":"Web UI","text":"<p>Guide to using Doctra's Gradio-based web interface.</p>"},{"location":"interfaces/web-ui.html#overview","title":"Overview","text":"<p>Doctra provides a user-friendly web interface for document processing without writing code.</p>"},{"location":"interfaces/web-ui.html#launching-the-ui","title":"Launching the UI","text":""},{"location":"interfaces/web-ui.html#python","title":"Python","text":"<pre><code>from doctra import launch_ui\n\n# Launch web interface\nlaunch_ui()\n</code></pre>"},{"location":"interfaces/web-ui.html#command-line","title":"Command Line","text":"<pre><code>python -m doctra.ui.app\n</code></pre>"},{"location":"interfaces/web-ui.html#module-script","title":"Module Script","text":"<pre><code>python gradio_app.py\n</code></pre> <p>The UI opens at: <code>http://127.0.0.1:7860</code></p>"},{"location":"interfaces/web-ui.html#interface-tabs","title":"Interface Tabs","text":""},{"location":"interfaces/web-ui.html#1-full-parse","title":"1. Full Parse","text":"<p>Complete document processing:</p> <ul> <li>Upload PDF</li> <li>Configure settings</li> <li>View results</li> <li>Download outputs</li> </ul>"},{"location":"interfaces/web-ui.html#2-docx-parser","title":"2. DOCX Parser","text":"<p>Microsoft Word document processing:</p> <ul> <li>Upload DOCX file</li> <li>Configure VLM settings</li> <li>Choose processing options</li> <li>View extracted content</li> <li>Download structured outputs</li> </ul>"},{"location":"interfaces/web-ui.html#3-tables-charts","title":"3. Tables &amp; Charts","text":"<p>Specialized extraction:</p> <ul> <li>Extract charts and/or tables</li> <li>Enable VLM processing</li> <li>Configure API keys</li> <li>Download structured data</li> </ul>"},{"location":"interfaces/web-ui.html#4-docres","title":"4. DocRes","text":"<p>Image restoration:</p> <ul> <li>Upload images or PDFs</li> <li>Select restoration task</li> <li>Compare before/after</li> <li>Download enhanced files</li> </ul>"},{"location":"interfaces/web-ui.html#5-enhanced-parser","title":"5. Enhanced Parser","text":"<p>Combined restoration and parsing:</p> <ul> <li>Upload PDF</li> <li>Configure restoration</li> <li>Enable VLM</li> <li>Get comprehensive results</li> </ul>"},{"location":"interfaces/web-ui.html#features","title":"Features","text":"<ul> <li>Drag &amp; Drop: Easy file upload</li> <li>Real-time Progress: See processing status</li> <li>Preview Results: View output in browser</li> <li>Download ZIP: Get all results packaged</li> <li>Configuration: Adjust all settings</li> <li>API Key Management: Secure key input</li> </ul>"},{"location":"interfaces/web-ui.html#configuration-options","title":"Configuration Options","text":"<p>Each tab provides settings for:</p> <ul> <li>DPI resolution</li> <li>Language selection</li> <li>VLM provider and API key</li> <li>Restoration tasks</li> <li>Output preferences</li> </ul>"},{"location":"interfaces/web-ui.html#sharing-the-ui","title":"Sharing the UI","text":"<p>Launch with public URL:</p> <pre><code>from doctra import build_demo\n\ndemo = build_demo()\ndemo.launch(share=True)\n</code></pre> <p>This generates a temporary public URL for sharing.</p>"},{"location":"interfaces/web-ui.html#use-cases","title":"Use Cases","text":"<ul> <li>Non-technical Users: No coding required</li> <li>Quick Processing: Fast one-off document processing</li> <li>Experimentation: Try different settings</li> <li>Demonstrations: Show Doctra capabilities</li> <li>Prototyping: Test before integrating</li> </ul>"},{"location":"interfaces/web-ui.html#see-also","title":"See Also","text":"<ul> <li>CLI Reference - Command line interface</li> <li>API Reference - Python API</li> <li>Examples - Usage examples</li> </ul>"},{"location":"user-guide/core-concepts.html","title":"Core Concepts","text":"<p>Understanding Doctra's core concepts will help you use the library effectively.</p>"},{"location":"user-guide/core-concepts.html#document-processing-pipeline","title":"Document Processing Pipeline","text":"<p>Doctra follows a multi-stage pipeline for document processing:</p> <pre><code>graph LR\n    A[PDF Input] --&gt; B[Layout Detection]\n    B --&gt; C[Element Classification]\n    C --&gt; D[OCR Processing]\n    D --&gt; E[VLM Processing]\n    E --&gt; F[Output Generation]\n\n    B -.Optional.-&gt; G[Image Restoration]\n    G --&gt; C</code></pre>"},{"location":"user-guide/core-concepts.html#pipeline-stages","title":"Pipeline Stages","text":"<ol> <li>Layout Detection: Analyzes document structure using PaddleOCR</li> <li>Element Classification: Identifies text, tables, charts, and figures</li> <li>OCR Processing: Extracts text from identified regions</li> <li>VLM Processing (Optional): Converts visual elements to structured data</li> <li>Output Generation: Creates Markdown, Excel, HTML, and JSON files</li> </ol>"},{"location":"user-guide/core-concepts.html#architecture-overview","title":"Architecture Overview","text":"<p>Doctra is organized into several key components:</p>"},{"location":"user-guide/core-concepts.html#parsers","title":"Parsers","text":"<p>Parsers are the main entry point for document processing. They orchestrate the entire pipeline.</p> StructuredPDFParser The base parser for general PDF processing. Handles layout detection, OCR, and output generation. EnhancedPDFParser Extends StructuredPDFParser with image restoration capabilities for low-quality documents. ChartTablePDFParser Specialized parser focused on extracting only charts and tables."},{"location":"user-guide/core-concepts.html#engines","title":"Engines","text":"<p>Engines provide specific processing capabilities:</p> Layout Detection PaddleOCR-based layout analysis to identify document structure. OCR Engine Tesseract-based text extraction from images. DocRes Engine Image restoration for document enhancement. VLM Service Vision Language Model integration for structured data extraction."},{"location":"user-guide/core-concepts.html#exporters","title":"Exporters","text":"<p>Exporters handle output generation in various formats:</p> <ul> <li>MarkdownWriter: Creates human-readable Markdown files</li> <li>ExcelWriter: Generates spreadsheets with structured data</li> <li>HTMLWriter: Produces web-ready HTML documents</li> <li>ImageSaver: Saves cropped visual elements</li> </ul>"},{"location":"user-guide/core-concepts.html#element-types","title":"Element Types","text":"<p>Doctra classifies document elements into four main types:</p>"},{"location":"user-guide/core-concepts.html#text-elements","title":"Text Elements","text":"<p>Regular text content including:</p> <ul> <li>Paragraphs</li> <li>Headings</li> <li>Lists</li> <li>Captions</li> </ul> <p>Processing: OCR \u2192 Text extraction \u2192 Markdown formatting</p>"},{"location":"user-guide/core-concepts.html#tables","title":"Tables","text":"<p>Tabular data with rows and columns.</p> <p>Processing Options:</p> <ol> <li>Without VLM: Saved as images only</li> <li>With VLM: Converted to Excel/HTML + saved as images</li> </ol> <p>Output: <code>tables.xlsx</code>, <code>tables.html</code>, cropped images</p>"},{"location":"user-guide/core-concepts.html#charts","title":"Charts","text":"<p>Visual representations of data including:</p> <ul> <li>Bar charts</li> <li>Line graphs</li> <li>Pie charts</li> <li>Scatter plots</li> </ul> <p>Processing Options:</p> <ol> <li>Without VLM: Saved as images with captions</li> <li>With VLM: Data extracted + description generated</li> </ol> <p>Output: Cropped images, optional structured data</p>"},{"location":"user-guide/core-concepts.html#figures","title":"Figures","text":"<p>General images and diagrams including:</p> <ul> <li>Photographs</li> <li>Illustrations</li> <li>Diagrams</li> <li>Logos</li> </ul> <p>Processing: Cropped and saved as images with context</p>"},{"location":"user-guide/core-concepts.html#layout-detection","title":"Layout Detection","text":"<p>Layout detection is the foundation of Doctra's processing.</p>"},{"location":"user-guide/core-concepts.html#how-it-works","title":"How It Works","text":"<ol> <li>Page Rendering: PDF pages rendered to images at specified DPI</li> <li>Model Inference: PaddleOCR layout model identifies regions</li> <li>Bounding Boxes: Each element gets coordinates and confidence score</li> <li>Classification: Elements labeled as text/table/chart/figure</li> </ol>"},{"location":"user-guide/core-concepts.html#detection-parameters","title":"Detection Parameters","text":"<pre><code>parser = StructuredPDFParser(\n    layout_model_name=\"PP-DocLayout_plus-L\",  # Model choice\n    dpi=200,  # Image resolution\n    min_score=0.5  # Confidence threshold\n)\n</code></pre> layout_model_name PaddleOCR model to use. Options: <code>PP-DocLayout_plus-L</code> (best), <code>PP-DocLayout_plus-M</code> (faster) dpi Image resolution. Higher = better quality but slower. Range: 100-300 min_score Minimum confidence score (0-1). Higher = fewer false positives"},{"location":"user-guide/core-concepts.html#visualization","title":"Visualization","text":"<p>Verify layout detection quality:</p> <pre><code>parser.display_pages_with_boxes(\n    pdf_path=\"document.pdf\",\n    num_pages=3\n)\n</code></pre> <p>This shows bounding boxes with colors:</p> <ul> <li>\ud83d\udd35 Blue: Text</li> <li>\ud83d\udd34 Red: Tables</li> <li>\ud83d\udfe2 Green: Charts</li> <li>\ud83d\udfe0 Orange: Figures</li> </ul>"},{"location":"user-guide/core-concepts.html#ocr-processing","title":"OCR Processing","text":"<p>OCR (Optical Character Recognition) extracts text from images.</p>"},{"location":"user-guide/core-concepts.html#configuration","title":"Configuration","text":"<pre><code>parser = StructuredPDFParser(\n    ocr_lang=\"eng\",  # Language\n    ocr_psm=6,  # Page segmentation mode\n    ocr_oem=3  # OCR Engine mode\n)\n</code></pre> ocr_lang Tesseract language code. Examples: <code>eng</code>, <code>fra</code>, <code>spa</code>, <code>deu</code> ocr_psm <p>Page segmentation mode. Common values:</p> <ul> <li><code>3</code>: Automatic page segmentation</li> <li><code>6</code>: Uniform block of text (default)</li> <li><code>11</code>: Sparse text</li> <li><code>12</code>: Sparse text with OSD</li> </ul> ocr_oem <p>OCR Engine mode:</p> <ul> <li><code>0</code>: Legacy engine</li> <li><code>1</code>: Neural nets LSTM</li> <li><code>3</code>: Default (both)</li> </ul>"},{"location":"user-guide/core-concepts.html#improving-ocr-accuracy","title":"Improving OCR Accuracy","text":"<ol> <li> <p>Increase DPI: Higher resolution = better text recognition    <pre><code>parser = StructuredPDFParser(dpi=300)\n</code></pre></p> </li> <li> <p>Use Image Restoration: Enhance document quality first    <pre><code>from doctra import EnhancedPDFParser\nparser = EnhancedPDFParser(use_image_restoration=True)\n</code></pre></p> </li> <li> <p>Correct Language: Specify document language    <pre><code>parser = StructuredPDFParser(ocr_lang=\"fra\")  # French\n</code></pre></p> </li> </ol>"},{"location":"user-guide/core-concepts.html#image-restoration","title":"Image Restoration","text":"<p>Image restoration improves document quality before processing.</p>"},{"location":"user-guide/core-concepts.html#restoration-tasks","title":"Restoration Tasks","text":"Task Purpose When to Use <code>appearance</code> General enhancement Most documents (default) <code>dewarping</code> Fix perspective Scanned with distortion <code>deshadowing</code> Remove shadows Poor lighting <code>deblurring</code> Reduce blur Motion blur, focus issues <code>binarization</code> B&amp;W conversion Clean text extraction <code>end2end</code> Full pipeline Severely degraded"},{"location":"user-guide/core-concepts.html#usage","title":"Usage","text":"<pre><code>from doctra import EnhancedPDFParser\n\nparser = EnhancedPDFParser(\n    use_image_restoration=True,\n    restoration_task=\"appearance\",\n    restoration_device=\"cuda\"  # or \"cpu\"\n)\n</code></pre>"},{"location":"user-guide/core-concepts.html#performance-impact","title":"Performance Impact","text":"Restoration Processing Time Quality Improvement None Baseline Baseline CPU +200% +30-50% GPU +50% +30-50%"},{"location":"user-guide/core-concepts.html#vlm-integration","title":"VLM Integration","text":"<p>Vision Language Models convert visual elements to structured data.</p>"},{"location":"user-guide/core-concepts.html#supported-providers","title":"Supported Providers","text":"<ul> <li>OpenAI: GPT-4 Vision, GPT-4o</li> <li>Gemini: Google's vision models</li> <li>Anthropic: Claude with vision</li> <li>OpenRouter: Access multiple models</li> <li>Qianfan: Baidu AI Cloud ERNIE models</li> <li>Ollama: Local models (no API key required)</li> </ul>"},{"location":"user-guide/core-concepts.html#configuration_1","title":"Configuration","text":"<pre><code>parser = StructuredPDFParser(\n    use_vlm=True,\n    vlm_provider=\"openai\",\n    vlm_api_key=\"your-api-key\",\n    vlm_model=\"gpt-4o\"  # Optional, uses default if not specified\n)\n</code></pre>"},{"location":"user-guide/core-concepts.html#what-gets-processed","title":"What Gets Processed","text":"<p>With VLM enabled:</p> Tables Converted to Excel/HTML with cell-by-cell data Charts Data extracted + description generated Figures Descriptions and context generated"},{"location":"user-guide/core-concepts.html#cost-considerations","title":"Cost Considerations","text":"<p>VLM processing requires API calls:</p> <ul> <li>Per Document: 1-10 API calls depending on visual elements</li> <li>Per Element: ~1 API call per table/chart</li> <li>Cost: Varies by provider (typically \\(0.01-\\)0.10 per document)</li> </ul>"},{"location":"user-guide/core-concepts.html#output-formats","title":"Output Formats","text":"<p>Doctra generates multiple output formats simultaneously.</p>"},{"location":"user-guide/core-concepts.html#markdown-md","title":"Markdown (.md)","text":"<p>Human-readable document with:</p> <ul> <li>All text content</li> <li>Embedded images</li> <li>Table references</li> <li>Section structure</li> </ul> <p>Best for: Documentation, reading, version control</p>"},{"location":"user-guide/core-concepts.html#html-html","title":"HTML (.html)","text":"<p>Web-ready document with:</p> <ul> <li>Styled content</li> <li>Interactive tables</li> <li>Image galleries</li> <li>Responsive layout</li> </ul> <p>Best for: Web publishing, presentations</p>"},{"location":"user-guide/core-concepts.html#excel-xlsx","title":"Excel (.xlsx)","text":"<p>Spreadsheet with:</p> <ul> <li>One sheet per table</li> <li>Formatted cells</li> <li>Headers and data</li> </ul> <p>Best for: Data analysis, further processing</p>"},{"location":"user-guide/core-concepts.html#json-json","title":"JSON (.json)","text":"<p>Structured data with:</p> <ul> <li>Element metadata</li> <li>Coordinates</li> <li>Content</li> <li>Relationships</li> </ul> <p>Best for: Programmatic access, integration</p>"},{"location":"user-guide/core-concepts.html#best-practices","title":"Best Practices","text":""},{"location":"user-guide/core-concepts.html#choosing-the-right-parser","title":"Choosing the Right Parser","text":"<pre><code># General documents\nfrom doctra import StructuredPDFParser\nparser = StructuredPDFParser()\n\n# Scanned or low-quality documents\nfrom doctra import EnhancedPDFParser\nparser = EnhancedPDFParser(use_image_restoration=True)\n\n# Only need charts/tables\nfrom doctra import ChartTablePDFParser\nparser = ChartTablePDFParser(extract_charts=True, extract_tables=True)\n</code></pre>"},{"location":"user-guide/core-concepts.html#optimizing-performance","title":"Optimizing Performance","text":"<ol> <li> <p>Use appropriate DPI: Higher isn't always better    <pre><code># Good quality documents\nparser = StructuredPDFParser(dpi=150)\n\n# Low quality documents\nparser = StructuredPDFParser(dpi=250)\n</code></pre></p> </li> <li> <p>Enable GPU when available:    <pre><code>parser = EnhancedPDFParser(restoration_device=\"cuda\")\n</code></pre></p> </li> <li> <p>Batch processing: Reuse parser instances    <pre><code>parser = StructuredPDFParser()\nfor pdf in pdf_files:\n    parser.parse(pdf)\n</code></pre></p> </li> </ol>"},{"location":"user-guide/core-concepts.html#managing-costs","title":"Managing Costs","text":"<p>When using VLM:</p> <ol> <li>Test without VLM first: Ensure layout detection works</li> <li>Process selectively: Use ChartTablePDFParser for specific elements</li> <li>Use cheaper models: Consider Gemini for cost savings</li> </ol>"},{"location":"user-guide/core-concepts.html#next-steps","title":"Next Steps","text":"<ul> <li>Structured Parser - Learn about the base parser</li> <li>Enhanced Parser - Document restoration</li> <li>VLM Integration - Structured data extraction</li> <li>Examples - See it in action</li> </ul>"},{"location":"user-guide/engines/docres-engine.html","title":"DocRes Engine","text":"<p>Guide to using the DocRes image restoration engine.</p>"},{"location":"user-guide/engines/docres-engine.html#overview","title":"Overview","text":"<p>The <code>DocResEngine</code> provides direct access to document image restoration capabilities using the DocRes model. Use it for standalone image enhancement or as part of the parsing pipeline.</p>"},{"location":"user-guide/engines/docres-engine.html#key-features","title":"Key Features","text":"<ul> <li>6 Restoration Tasks: Comprehensive document enhancement</li> <li>GPU Acceleration: CUDA support for faster processing</li> <li>Flexible Input: Images or PDFs</li> <li>Detailed Metadata: Processing information returned</li> </ul>"},{"location":"user-guide/engines/docres-engine.html#basic-usage","title":"Basic Usage","text":"<pre><code>from doctra import DocResEngine\n\n# Initialize engine\nengine = DocResEngine(device=\"cuda\")\n\n# Restore image\nrestored_img, metadata = engine.restore_image(\n    image=\"document.jpg\",\n    task=\"appearance\"\n)\n\n# Save result\nrestored_img.save(\"restored.jpg\")\n</code></pre>"},{"location":"user-guide/engines/docres-engine.html#restoration-tasks","title":"Restoration Tasks","text":"Task Description Use Case <code>appearance</code> General enhancement Most documents <code>dewarping</code> Fix perspective Scanned at angle <code>deshadowing</code> Remove shadows Poor lighting <code>deblurring</code> Reduce blur Motion/focus issues <code>binarization</code> B&amp;W conversion Clean text <code>end2end</code> Full pipeline Severe degradation"},{"location":"user-guide/engines/docres-engine.html#pdf-restoration","title":"PDF Restoration","text":"<pre><code>engine = DocResEngine(device=\"cuda\")\n\nrestored_pdf = engine.restore_pdf(\n    pdf_path=\"low_quality.pdf\",\n    output_path=\"enhanced.pdf\",\n    task=\"appearance\",\n    dpi=300\n)\n</code></pre>"},{"location":"user-guide/engines/docres-engine.html#see-also","title":"See Also","text":"<ul> <li>Enhanced Parser - Integrated restoration</li> <li>API Reference - Complete API documentation</li> <li>Core Concepts - Understanding restoration</li> </ul>"},{"location":"user-guide/engines/layout-detection.html","title":"Layout Detection","text":"<p>Guide to document layout detection in Doctra.</p>"},{"location":"user-guide/engines/layout-detection.html#overview","title":"Overview","text":"<p>Layout detection is the foundation of Doctra's processing pipeline. It analyzes PDF pages to identify and classify different document elements (text, tables, charts, figures).</p>"},{"location":"user-guide/engines/layout-detection.html#how-it-works","title":"How It Works","text":"<ol> <li>Render: PDF pages converted to images at specified DPI</li> <li>Detection: PaddleOCR model identifies element regions</li> <li>Classification: Elements labeled by type</li> <li>Filtering: Low-confidence detections removed</li> </ol>"},{"location":"user-guide/engines/layout-detection.html#configuration","title":"Configuration","text":"<pre><code>from doctra import StructuredPDFParser\n\nparser = StructuredPDFParser(\n    layout_model_name=\"PP-DocLayout_plus-L\",\n    dpi=200,\n    min_score=0.5\n)\n</code></pre>"},{"location":"user-guide/engines/layout-detection.html#parameters","title":"Parameters","text":"layout_model_name PaddleOCR model to use - <code>PP-DocLayout_plus-L</code>: Best accuracy (slower) - <code>PP-DocLayout_plus-M</code>: Faster, good accuracy dpi Image resolution - 100-150: Fast, lower quality - 200: Balanced (default) - 250-300: High quality, slower min_score Confidence threshold (0-1) - 0.0: Include all detections - 0.5: Moderate filtering - 0.7+: Conservative, high confidence only"},{"location":"user-guide/engines/layout-detection.html#visualization","title":"Visualization","text":"<p>Verify detection quality:</p> <pre><code>parser.display_pages_with_boxes(\n    pdf_path=\"document.pdf\",\n    num_pages=3\n)\n</code></pre>"},{"location":"user-guide/engines/layout-detection.html#element-types","title":"Element Types","text":"<ul> <li>Text: Regular content (blue boxes)</li> <li>Tables: Tabular data (red boxes)</li> <li>Charts: Graphs and plots (green boxes)</li> <li>Figures: Images and diagrams (orange boxes)</li> </ul>"},{"location":"user-guide/engines/layout-detection.html#see-also","title":"See Also","text":"<ul> <li>Core Concepts - Understanding the pipeline</li> <li>Visualization - Layout visualization</li> <li>API Reference - Configuration options</li> </ul>"},{"location":"user-guide/engines/ocr-engine.html","title":"OCR Engine","text":"<p>Guide to text extraction using OCR in Doctra.</p>"},{"location":"user-guide/engines/ocr-engine.html#overview","title":"Overview","text":"<p>Doctra uses Tesseract OCR to extract text from document images. The OCR engine is highly configurable for different document types and languages.</p>"},{"location":"user-guide/engines/ocr-engine.html#configuration","title":"Configuration","text":"<pre><code>from doctra import StructuredPDFParser\n\nparser = StructuredPDFParser(\n    ocr_lang=\"eng\",\n    ocr_psm=6,\n    ocr_oem=3\n)\n</code></pre>"},{"location":"user-guide/engines/ocr-engine.html#parameters","title":"Parameters","text":"ocr_lang Tesseract language code - <code>eng</code>: English - <code>fra</code>: French - <code>spa</code>: Spanish - <code>deu</code>: German - Multiple: <code>eng+fra</code> ocr_psm Page segmentation mode - <code>3</code>: Automatic - <code>6</code>: Uniform block (default) - <code>11</code>: Sparse text - <code>12</code>: Sparse with OSD ocr_oem OCR engine mode - <code>0</code>: Legacy - <code>1</code>: Neural nets LSTM - <code>3</code>: Default (both)"},{"location":"user-guide/engines/ocr-engine.html#improving-accuracy","title":"Improving Accuracy","text":""},{"location":"user-guide/engines/ocr-engine.html#1-increase-dpi","title":"1. Increase DPI","text":"<pre><code>parser = StructuredPDFParser(dpi=300)\n</code></pre>"},{"location":"user-guide/engines/ocr-engine.html#2-use-image-restoration","title":"2. Use Image Restoration","text":"<pre><code>from doctra import EnhancedPDFParser\n\nparser = EnhancedPDFParser(\n    use_image_restoration=True\n)\n</code></pre>"},{"location":"user-guide/engines/ocr-engine.html#3-correct-language","title":"3. Correct Language","text":"<pre><code>parser = StructuredPDFParser(\n    ocr_lang=\"fra\"  # For French documents\n)\n</code></pre>"},{"location":"user-guide/engines/ocr-engine.html#multi-language-documents","title":"Multi-language Documents","text":"<pre><code>parser = StructuredPDFParser(\n    ocr_lang=\"eng+fra+deu\"  # Multiple languages\n)\n</code></pre>"},{"location":"user-guide/engines/ocr-engine.html#see-also","title":"See Also","text":"<ul> <li>Enhanced Parser - Improve OCR with restoration</li> <li>Core Concepts - Understanding OCR in the pipeline</li> <li>API Reference - OCR configuration options</li> </ul>"},{"location":"user-guide/engines/vlm-integration.html","title":"VLM Integration","text":"<p>Guide to using Vision Language Models with Doctra.</p>"},{"location":"user-guide/engines/vlm-integration.html#overview","title":"Overview","text":"<p>Doctra integrates with Vision Language Models (VLMs) to convert visual elements (charts, tables, figures) into structured data. This enables automatic data extraction and conversion to Excel, HTML, and JSON formats.</p>"},{"location":"user-guide/engines/vlm-integration.html#supported-providers","title":"Supported Providers","text":"<ul> <li>OpenAI: GPT-4 Vision, GPT-4o</li> <li>Gemini: Google's vision models</li> <li>Anthropic: Claude with vision</li> <li>OpenRouter: Access multiple models</li> <li>Qianfan: Baidu AI Cloud ERNIE models</li> <li>Ollama: Local models (no API key required)</li> </ul>"},{"location":"user-guide/engines/vlm-integration.html#basic-configuration","title":"Basic Configuration","text":"<pre><code>from doctra import StructuredPDFParser\n\nparser = StructuredPDFParser(\n    use_vlm=True,\n    vlm_provider=\"openai\",\n    vlm_api_key=\"your-api-key\"\n)\n\nparser.parse(\"document.pdf\")\n</code></pre>"},{"location":"user-guide/engines/vlm-integration.html#provider-setup","title":"Provider Setup","text":""},{"location":"user-guide/engines/vlm-integration.html#openai","title":"OpenAI","text":"<pre><code>parser = StructuredPDFParser(\n    use_vlm=True,\n    vlm_provider=\"openai\",\n    vlm_api_key=\"sk-xxx\",\n    vlm_model=\"gpt-4o\"  # Optional\n)\n</code></pre>"},{"location":"user-guide/engines/vlm-integration.html#gemini","title":"Gemini","text":"<pre><code>parser = StructuredPDFParser(\n    use_vlm=True,\n    vlm_provider=\"gemini\",\n    vlm_api_key=\"your-gemini-key\"\n)\n</code></pre>"},{"location":"user-guide/engines/vlm-integration.html#anthropic","title":"Anthropic","text":"<pre><code>parser = StructuredPDFParser(\n    use_vlm=True,\n    vlm_provider=\"anthropic\",\n    vlm_api_key=\"your-anthropic-key\"\n)\n</code></pre>"},{"location":"user-guide/engines/vlm-integration.html#openrouter","title":"OpenRouter","text":"<pre><code>parser = StructuredPDFParser(\n    use_vlm=True,\n    vlm_provider=\"openrouter\",\n    vlm_api_key=\"your-openrouter-key\",\n    vlm_model=\"x-ai/grok-4\"  # Optional, defaults to x-ai/grok-4\n)\n</code></pre> <p>Available Models: - <code>x-ai/grok-4</code> (default) - Grok-4 model - <code>anthropic/claude-3.5-sonnet</code> - Claude 3.5 Sonnet - <code>openai/gpt-4o</code> - GPT-4o via OpenRouter - <code>google/gemini-pro-vision</code> - Gemini Pro Vision</p>"},{"location":"user-guide/engines/vlm-integration.html#qianfan-baidu-ai-cloud","title":"Qianfan (Baidu AI Cloud)","text":"<pre><code>parser = StructuredPDFParser(\n    use_vlm=True,\n    vlm_provider=\"qianfan\",\n    vlm_api_key=\"your-qianfan-key\",\n    vlm_model=\"ernie-4.5-turbo-vl-32k\"  # Optional, defaults to ernie-4.5-turbo-vl-32k\n)\n</code></pre> <p>Available ERNIE Models: - <code>ernie-4.5-turbo-vl-32k</code> (default) - vision model with 32k context</p>"},{"location":"user-guide/engines/vlm-integration.html#ollama-local-models","title":"Ollama (Local Models)","text":"<pre><code>parser = StructuredPDFParser(\n    use_vlm=True,\n    vlm_provider=\"ollama\",\n    vlm_model=\"llava:latest\"  # Optional, defaults to llava:latest\n)\n</code></pre> <p>Available Models: - <code>llava:latest</code> (default) - LLaVA vision model - <code>llava:7b</code> - LLaVA 7B model - <code>llava:13b</code> - LLaVA 13B model - <code>gemma2:latest</code> - Gemma 2 model - <code>qwen2-vl:latest</code> - Qwen2-VL model</p> <p>Prerequisites: - Ollama must be installed and running locally - No API key required - Models are downloaded automatically on first use</p>"},{"location":"user-guide/engines/vlm-integration.html#what-gets-processed","title":"What Gets Processed","text":"<p>With VLM enabled:</p> <ul> <li>Tables: Converted to Excel/HTML with cell data</li> <li>Charts: Data points extracted + descriptions</li> <li>Figures: Descriptions and context generated</li> </ul>"},{"location":"user-guide/engines/vlm-integration.html#output-files","title":"Output Files","text":"<pre><code>outputs/\n\u2514\u2500\u2500 document/\n    \u2514\u2500\u2500 full_parse/\n        \u251c\u2500\u2500 tables.xlsx      # Extracted table data\n        \u251c\u2500\u2500 tables.html      # HTML tables\n        \u251c\u2500\u2500 vlm_items.json   # Structured data\n        \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"user-guide/engines/vlm-integration.html#cost-considerations","title":"Cost Considerations","text":"<p>VLM processing requires API calls:</p> <ul> <li>~1-10 calls per document</li> <li>~\\(0.01-\\)0.10 per document</li> <li>Costs vary by provider</li> </ul>"},{"location":"user-guide/engines/vlm-integration.html#see-also","title":"See Also","text":"<ul> <li>Parsers - Using VLM with parsers</li> <li>API Reference - VLM configuration options</li> <li>Examples - VLM usage examples</li> </ul>"},{"location":"user-guide/features/split-table-merging.html","title":"Split Table Merging","text":"<p>A comprehensive guide to Doctra's automatic detection and merging of tables split across page boundaries.</p>"},{"location":"user-guide/features/split-table-merging.html#overview","title":"Overview","text":"<p>Many documents contain large tables that span multiple pages. When processing such documents, each page may contain only a portion of a table, making it difficult to extract complete data. Doctra's split table merging feature automatically detects these split tables and combines them into single, complete table images.</p>"},{"location":"user-guide/features/split-table-merging.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>How It Works</li> <li>Process Overview: A Step-by-Step Narrative</li> <li>Detection Algorithm</li> <li>Visual Schema</li> <li>Phase-by-Phase Breakdown</li> <li>Configuration Parameters</li> <li>Examples</li> <li>Troubleshooting</li> </ul>"},{"location":"user-guide/features/split-table-merging.html#how-it-works","title":"How It Works","text":"<p>The split table detection uses a sophisticated two-phase approach:</p> <ol> <li>Phase 1: Proximity Detection - Fast spatial heuristics to identify candidate pairs</li> <li>Phase 2: Structural Validation - Deep structural analysis using computer vision</li> </ol> <p>This design balances speed (avoiding expensive operations on all pairs) with accuracy (validating only promising candidates).</p>"},{"location":"user-guide/features/split-table-merging.html#process-overview-a-step-by-step-narrative","title":"Process Overview: A Step-by-Step Narrative","text":"<p>To help you understand how split table merging works, here's a walkthrough of what happens when Doctra processes a document with split tables:</p>"},{"location":"user-guide/features/split-table-merging.html#the-initial-situation","title":"The Initial Situation","text":"<p>Imagine you have a financial report with a large table that spans two pages. On page 1, the table starts at row 1 and continues until row 15, but runs out of space near the bottom. On page 2, the table resumes at row 16 and continues with the remaining data. To a human reader, this is clearly one continuous table, but to a document parser, these appear as two separate table images.</p>"},{"location":"user-guide/features/split-table-merging.html#phase-1-finding-potential-matches-proximity-detection","title":"Phase 1: Finding Potential Matches (Proximity Detection)","text":"<p>When Doctra begins processing your document, it first identifies all tables on every page. Let's say it finds 8 tables total across 5 pages. Now, instead of performing expensive computer vision analysis on all 64 possible pairs (8\u00d78), Doctra uses smart heuristics to quickly filter potential matches.</p> <p>The Position Check: Doctra looks for tables that are positioned at page boundaries. A table on page 1 that's very close to the bottom (say, within 20% of the page height from the bottom) is a strong candidate for being the first part of a split table. Similarly, a table on page 2 that's very close to the top (within 15% from the top) could be the continuation. If both conditions are met, this pair moves to the next check.</p> <p>The Overlap Check: Even if two tables are at page boundaries, they might be completely different tables. Doctra checks if they overlap horizontally. If two tables have at least 50% horizontal overlap, it suggests they're aligned and could be the same table. Think of it like two puzzle pieces - if they don't overlap at all, they probably don't belong together.</p> <p>The Gap Analysis: Pages have headers, footers, and margins that create gaps between content. Doctra calculates the total gap between the end of the first table and the start of the second table, accounting for the page break. If this gap is reasonable (typically within 25% of the page height), it indicates the tables were separated by a normal page break rather than being intentionally placed far apart.</p> <p>The Width Check: Finally, Doctra verifies that both table segments have similar widths. A table that's 800 pixels wide on page 1 shouldn't suddenly become 500 pixels wide on page 2 - that would indicate a different table structure. Similar widths suggest structural continuity.</p> <p>If all four checks pass, Doctra has found a promising candidate pair and proceeds to deeper analysis. This filtering is crucial - it means Doctra only spends computational resources on pairs that are likely matches, keeping the process fast.</p>"},{"location":"user-guide/features/split-table-merging.html#phase-2-verifying-the-match-structural-validation","title":"Phase 2: Verifying the Match (Structural Validation)","text":"<p>For pairs that pass Phase 1, Doctra performs a more detailed analysis using computer vision to confirm they're truly parts of the same table.</p> <p>Image Preprocessing: Before analyzing structure, Doctra enhances the table images. It converts them to grayscale, improves contrast using CLAHE (Contrast Limited Adaptive Histogram Equalization), converts to binary (black and white), and applies morphological operations to connect broken or dashed lines. This prepares the images for accurate line detection.</p> <p>Column Detection: Using OpenCV's Line Segment Detector (LSD), Doctra identifies all vertical lines in each table segment. These vertical lines represent column boundaries. The algorithm detects line segments, filters for near-vertical lines (75\u00b0 to 105\u00b0), extracts their x-coordinates, clusters nearby coordinates together (since multiple small line segments might represent one column), and finally removes columns too close to the edges (which are often just page borders).</p> <p>Column Comparison: Now Doctra compares the detected columns between the two segments. First, it checks if the column counts are similar - if one segment has 5 columns and the other has 8, they're probably different tables. However, Doctra uses adaptive tolerance - for larger tables (more columns), it allows slightly more variation since column detection can be less precise on complex tables.</p> <p>Alignment Validation: Even if the column counts match, the columns need to align between segments. Doctra pairs each column from segment 1 with the closest column in segment 2 and measures the distance. If at least 60% of columns align within a small tolerance (default 10 pixels), it indicates the same table structure continues across pages.</p> <p>Confidence Scoring: Based on all these factors, Doctra calculates a confidence score between 0 and 1: - Column count match contributes up to 0.3 points - Column alignment contributes up to 0.4 points - Width similarity contributes up to 0.1 points - Horizontal overlap contributes up to 0.2 points</p> <p>If the final confidence score meets the minimum threshold (default 0.65), Doctra confirms this is a split table and proceeds to merge.</p>"},{"location":"user-guide/features/split-table-merging.html#the-merging-process","title":"The Merging Process","text":"<p>Once a match is confirmed, Doctra creates a composite image by stacking the two table segments vertically. The merged image is then treated as a single table for the rest of the processing pipeline. If you have VLM (Vision Language Model) enabled, Doctra extracts structured data from the complete merged table rather than from fragmented pieces.</p>"},{"location":"user-guide/features/split-table-merging.html#fallback-mechanisms","title":"Fallback Mechanisms","text":"<p>Sometimes, tables don't have clear column boundaries (borderless tables) or column detection fails due to image quality. Doctra has fallback mechanisms: - If too many columns are detected (likely noise), it falls back to proximity-based matching with lower confidence - If no columns are detected in either segment, it relies solely on Phase 1 checks with moderate confidence</p> <p>This ensures that even tables without clear borders can be merged if they meet the proximity criteria.</p>"},{"location":"user-guide/features/split-table-merging.html#the-final-result","title":"The Final Result","text":"<p>In your financial report example, Doctra would detect that the table on page 1 (rows 1-15) and page 2 (rows 16+) are parts of the same table. It would merge them into a single image containing all rows, and this complete table would appear in your output markdown, HTML, or Excel files as one continuous table rather than two fragments. The merged table is labeled with the page range (e.g., \"pages 1-2\") and includes the confidence score so you can verify the quality of the merge.</p> <p>This two-phase approach ensures that Doctra can accurately merge split tables while remaining efficient enough for production use, processing large documents with many tables in reasonable time.</p>"},{"location":"user-guide/features/split-table-merging.html#detection-algorithm","title":"Detection Algorithm","text":""},{"location":"user-guide/features/split-table-merging.html#high-level-flow","title":"High-Level Flow","text":"<pre><code>flowchart TD\n    A[Start: Parse PDF Document] --&gt; B[Extract Tables from All Pages]\n    B --&gt; C{Phase 1: Proximity Detection}\n\n    C --&gt; D[1. Check Position&lt;br/&gt;bottom/top thresholds]\n    D --&gt; E[2. Check Horizontal Overlap&lt;br/&gt;\u226550% required]\n    E --&gt; F[3. Check Gap Between Tables&lt;br/&gt;\u226425% of page height]\n    F --&gt; G[4. Check Width Similarity&lt;br/&gt;\u226420% difference]\n\n    G --&gt; H{All Checks Pass?}\n    H --&gt;|Yes| I[Phase 2: Structural Validation]\n    H --&gt;|No| J[Skip Pair&lt;br/&gt;Try Next]\n\n    I --&gt; K[1. Detect Columns using LSD]\n    K --&gt; L[2. Compare Column Counts&lt;br/&gt;Adaptive tolerance]\n    L --&gt; M[3. Check Column Alignment&lt;br/&gt;\u226560% must align]\n    M --&gt; N[4. Calculate Confidence Score]\n\n    N --&gt; O{Confidence \u2265 0.65?}\n    O --&gt;|Yes| P[\u2705 Merge Tables]\n    O --&gt;|No| Q[\u274c Reject Match]\n\n    J --&gt; R[Continue with Next Pair]\n    Q --&gt; R\n    P --&gt; S[Create Merged Image]\n\n    style P fill:#90EE90\n    style Q fill:#FFB6C1\n    style I fill:#87CEEB\n    style C fill:#DDA0DD</code></pre>"},{"location":"user-guide/features/split-table-merging.html#visual-schema","title":"Visual Schema","text":""},{"location":"user-guide/features/split-table-merging.html#document-layout-representation","title":"Document Layout Representation","text":"<pre><code>graph LR\n    subgraph Page1[\"\ud83d\udcc4 Page 1\"]\n        direction TB\n        P1_Content[Document Content]\n        P1_Table[\"\ud83d\udcca Table Segment 1&lt;br/&gt;Row 1&lt;br/&gt;Row 2&lt;br/&gt;Row 3&lt;br/&gt;Row 4&lt;br/&gt;Row 5&lt;br/&gt;&lt;br/&gt;\ud83d\udccd Close to bottom&lt;br/&gt;(\u226580% of page)\"]\n        P1_Content --&gt; P1_Table\n    end\n\n    subgraph Gap[\"\ud83d\udd04 Gap: 18.8%&lt;br/&gt;(Page break +&lt;br/&gt;headers/footers)\"]\n        G[Gap]\n    end\n\n    subgraph Page2[\"\ud83d\udcc4 Page 2\"]\n        direction TB\n        P2_Table[\"\ud83d\udcca Table Segment 2&lt;br/&gt;\ud83d\udccd Close to top&lt;br/&gt;(\u226415% from top)&lt;br/&gt;&lt;br/&gt;Row 6&lt;br/&gt;Row 7&lt;br/&gt;Row 8&lt;br/&gt;...\"]\n        P2_Content[Document Content]\n        P2_Table --&gt; P2_Content\n    end\n\n    P1_Table --&gt; Gap\n    Gap --&gt; P2_Table\n\n    style P1_Table fill:#FFE4B5\n    style P2_Table fill:#FFE4B5\n    style Gap fill:#F0F0F0</code></pre>"},{"location":"user-guide/features/split-table-merging.html#phase-1-proximity-detection-schema","title":"Phase 1: Proximity Detection Schema","text":"<pre><code>flowchart TD\n    Start[Proximity Detection] --&gt; Check1[Check 1: Position]\n\n    Check1 --&gt; C1A[\"\ud83d\udccd Segment 1&lt;br/&gt;y2 \u2265 80% of page&lt;br/&gt;(Close to bottom)\"]\n    Check1 --&gt; C1B[\"\ud83d\udccd Segment 2&lt;br/&gt;y1 \u2264 15% of page&lt;br/&gt;(Close to top)\"]\n    C1A --&gt; C1Result{Position OK?}\n    C1B --&gt; C1Result\n\n    C1Result --&gt;|Yes| Check2[Check 2: Horizontal Overlap]\n    C1Result --&gt;|No| Fail1[\u274c Reject]\n\n    Check2 --&gt; C2Calc[\"Calculate Overlap&lt;br/&gt;Overlap = min(x2_1, x2_2) - max(x1_1, x1_2)&lt;br/&gt;Ratio = Overlap / max(width1, width2)\"]\n    C2Calc --&gt; C2Result{Overlap \u2265 50%?}\n\n    C2Result --&gt;|Yes| Check3[Check 3: Gap Analysis]\n    C2Result --&gt;|No| Fail2[\u274c Reject]\n\n    Check3 --&gt; C3Calc[\"Gap = (Page1_height - Seg1_y2) + Seg2_y1&lt;br/&gt;Gap Ratio = Gap / Page Height\"]\n    C3Calc --&gt; C3Result{Gap \u2264 25%?}\n\n    C3Result --&gt;|Yes| Check4[Check 4: Width Similarity]\n    C3Result --&gt;|No| Fail3[\u274c Reject]\n\n    Check4 --&gt; C4Calc[\"Width Diff = |width1 - width2|&lt;br/&gt;Ratio = Diff / max(width1, width2)\"]\n    C4Calc --&gt; C4Result{Width Diff \u2264 20%?}\n\n    C4Result --&gt;|Yes| Pass[\u2705 All Checks Pass&lt;br/&gt;Proceed to Phase 2]\n    C4Result --&gt;|No| Fail4[\u274c Reject]\n\n    style Pass fill:#90EE90\n    style Fail1 fill:#FFB6C1\n    style Fail2 fill:#FFB6C1\n    style Fail3 fill:#FFB6C1\n    style Fail4 fill:#FFB6C1\n    style Check1 fill:#FFE4B5\n    style Check2 fill:#FFE4B5\n    style Check3 fill:#FFE4B5\n    style Check4 fill:#FFE4B5</code></pre>"},{"location":"user-guide/features/split-table-merging.html#phase-2-structural-validation-schema","title":"Phase 2: Structural Validation Schema","text":"<pre><code>flowchart TD\n    Start[Structural Validation] --&gt; Preprocess[Step 1: Image Preprocessing]\n\n    Preprocess --&gt; P1[RGB Image]\n    P1 --&gt; P2[Grayscale Conversion]\n    P2 --&gt; P3[CLAHE Enhancement]\n    P3 --&gt; P4[OTSU Thresholding]\n    P4 --&gt; P5[Morphological Operations]\n    P5 --&gt; P6[\"Enhanced Binary Image&lt;br/&gt;Ready for LSD\"]\n\n    P6 --&gt; LSD[Step 2: LSD Column Detection]\n\n    LSD --&gt; LSD1[\"Segment 1&lt;br/&gt;Detect Vertical Lines\"]\n    LSD --&gt; LSD2[\"Segment 2&lt;br/&gt;Detect Vertical Lines\"]\n\n    LSD1 --&gt; Col1[\"Columns Detected&lt;br/&gt;Positions: 100, 250, 400, 550, 700&lt;br/&gt;Count: 5\"]\n    LSD2 --&gt; Col2[\"Columns Detected&lt;br/&gt;Positions: 102, 248, 402, 552, 698&lt;br/&gt;Count: 5\"]\n\n    Col1 --&gt; Compare[Step 3: Column Comparison]\n    Col2 --&gt; Compare\n\n    Compare --&gt; CountCheck{\"Column Count&lt;br/&gt;Difference within Threshold?\"}\n    CountCheck --&gt;|Yes| AlignCheck[\"Calculate Alignment&lt;br/&gt;Check position difference\"]\n    CountCheck --&gt;|No| Reject1[\"Reject: Count Mismatch\"]\n\n    AlignCheck --&gt; AlignResult[\"Alignment Score&lt;br/&gt;Aligned columns / Total columns&lt;br/&gt;Example: 5/5 = 100%\"]\n    AlignResult --&gt; AlignCheck2{\"Alignment &gt;= 60%?\"}\n\n    AlignCheck2 --&gt;|Yes| ConfCalc[Step 4: Confidence Calculation]\n    AlignCheck2 --&gt;|No| Reject2[\"Reject: Poor Alignment\"]\n\n    ConfCalc --&gt; ConfFactors[\"Score Factors&lt;br/&gt;Column match: +0.3&lt;br/&gt;Alignment: +0.4&lt;br/&gt;Width similarity: +0.1&lt;br/&gt;Overlap: +0.2\"]\n    ConfFactors --&gt; ConfScore{\"Confidence &gt;= 0.65?\"}\n\n    ConfScore --&gt;|Yes| Merge[\"Merge Tables&lt;br/&gt;Confidence: 1.0\"]\n    ConfScore --&gt;|No| Reject3[\"Reject: Low Confidence\"]\n\n    style Merge fill:#90EE90\n    style Reject1 fill:#FFB6C1\n    style Reject2 fill:#FFB6C1\n    style Reject3 fill:#FFB6C1\n    style Preprocess fill:#87CEEB\n    style LSD fill:#DDA0DD</code></pre>"},{"location":"user-guide/features/split-table-merging.html#phase-by-phase-breakdown","title":"Phase-by-Phase Breakdown","text":""},{"location":"user-guide/features/split-table-merging.html#phase-1-proximity-detection","title":"Phase 1: Proximity Detection","text":""},{"location":"user-guide/features/split-table-merging.html#11-position-check","title":"1.1 Position Check","text":"<p>Purpose: Identify tables that are positioned at page boundaries, which is a strong indicator of page breaks.</p> <p>Algorithm: <pre><code>For table segment 1 on page N:\n  bottom_ratio = segment_y2 / page_height\n  If bottom_ratio &gt;= (1.0 - bottom_threshold_ratio):\n    \u2705 Candidate for first segment\n\nFor table segment 2 on page N+1:\n  top_ratio = segment_y1 / page_height\n  If top_ratio &lt;= top_threshold_ratio:\n    \u2705 Candidate for second segment\n</code></pre></p> <p>Example: <pre><code>Page Height: 1000px\nSegment 1 y2: 850px\n  \u2192 bottom_ratio = 850/1000 = 0.85\n  \u2192 Threshold: 1.0 - 0.20 = 0.80\n  \u2192 0.85 &gt;= 0.80 \u2705 PASS\n\nSegment 2 y1: 150px\n  \u2192 top_ratio = 150/1000 = 0.15\n  \u2192 Threshold: 0.15\n  \u2192 0.15 &lt;= 0.15 \u2705 PASS\n</code></pre></p>"},{"location":"user-guide/features/split-table-merging.html#12-horizontal-overlap-check","title":"1.2 Horizontal Overlap Check","text":"<p>Purpose: Ensure tables are aligned horizontally, indicating they're the same table.</p> <p>Algorithm: <pre><code>overlap = calculate_overlap(seg1_x1, seg1_x2, seg2_x1, seg2_x2)\n  = max(0, min(seg1_x2, seg2_x2) - max(seg1_x1, seg2_x1))\n\noverlap_ratio = overlap / max(seg1_width, seg2_width)\n\nIf overlap_ratio &gt;= 0.5:\n  \u2705 PASS (at least 50% overlap)\n</code></pre></p> <p>Visual Representation:</p> <pre><code>graph LR\n    subgraph Seg1[\"Segment 1\"]\n        S1[Table Width&lt;br/&gt;11 units]\n    end\n\n    subgraph Seg2[\"Segment 2\"]\n        S2[Table Width&lt;br/&gt;11 units]\n    end\n\n    subgraph Overlap[\"Overlap Zone\"]\n        OV[\"Overlap: 7 units&lt;br/&gt;Ratio: 63.6% \u2705\"]\n    end\n\n    S1 -.-&gt;|Overlap| OV\n    S2 -.-&gt;|Overlap| OV\n\n    style Overlap fill:#90EE90\n    style Seg1 fill:#FFE4B5\n    style Seg2 fill:#FFE4B5</code></pre>"},{"location":"user-guide/features/split-table-merging.html#13-gap-analysis","title":"1.3 Gap Analysis","text":"<p>Purpose: Measure the space between tables accounting for page breaks, headers, and footers.</p> <p>Algorithm: <pre><code>gap_pixels = (page1_height - seg1_y2) + seg2_y1\ngap_ratio = gap_pixels / page1_height\n\nIf gap_ratio &lt;= max_gap_ratio:\n  \u2705 PASS (gap is reasonable)\n</code></pre></p> <p>Considerations: - Headers/footers take up space - Page margins create natural gaps - Default 25% accommodates typical document layouts</p>"},{"location":"user-guide/features/split-table-merging.html#14-width-similarity-check","title":"1.4 Width Similarity Check","text":"<p>Purpose: Verify both segments have similar widths, confirming they share the same structure.</p> <p>Algorithm: <pre><code>width1 = seg1_x2 - seg1_x1\nwidth2 = seg2_x2 - seg2_x1\nwidth_diff = abs(width1 - width2)\nwidth_ratio = width_diff / max(width1, width2)\n\nIf width_ratio &lt;= width_similarity_threshold (0.20):\n  \u2705 PASS (widths are similar)\n</code></pre></p>"},{"location":"user-guide/features/split-table-merging.html#phase-2-structural-validation","title":"Phase 2: Structural Validation","text":""},{"location":"user-guide/features/split-table-merging.html#21-image-preprocessing","title":"2.1 Image Preprocessing","text":"<p>Purpose: Enhance images for optimal line detection.</p> <p>Steps:</p> <ol> <li> <p>Grayscale Conversion <pre><code>Original RGB \u2192 Grayscale\n</code></pre></p> </li> <li> <p>Contrast Enhancement (CLAHE) <pre><code>Apply Contrast Limited Adaptive Histogram Equalization\n\u2192 Improves line visibility in low-contrast areas\n</code></pre></p> </li> <li> <p>Binary Thresholding (OTSU) <pre><code>Grayscale \u2192 Binary (black/white)\n\u2192 OTSU automatically determines optimal threshold\n</code></pre></p> </li> <li> <p>Morphological Operations <pre><code>Apply MORPH_CLOSE with vertical kernel (1x5)\n\u2192 Connects broken or dashed lines\n\u2192 Enhances vertical line detection\n</code></pre></p> </li> </ol> <p>Visual Flow:</p> <pre><code>graph LR\n    A[RGB Image] --&gt; B[Grayscale&lt;br/&gt;Conversion]\n    B --&gt; C[CLAHE&lt;br/&gt;Enhancement]\n    C --&gt; D[OTSU&lt;br/&gt;Thresholding]\n    D --&gt; E[Morphological&lt;br/&gt;Operations]\n    E --&gt; F[Ready for&lt;br/&gt;LSD Detection]\n\n    style A fill:#FFE4B5\n    style B fill:#DDA0DD\n    style C fill:#87CEEB\n    style D fill:#98FB98\n    style E fill:#F0E68C\n    style F fill:#90EE90</code></pre>"},{"location":"user-guide/features/split-table-merging.html#22-lsd-column-detection","title":"2.2 LSD Column Detection","text":"<p>Purpose: Detect vertical lines representing column boundaries using OpenCV's Line Segment Detector.</p> <p>LSD Algorithm Overview: <pre><code>1. Gradient Computation\n   \u2192 Calculate image gradients\n   \u2192 Identify edge regions\n\n2. Line Region Growing\n   \u2192 Grow line segments from seed points\n   \u2192 Connect adjacent pixels with similar orientation\n\n3. Region Validation\n   \u2192 Verify regions meet line criteria\n   \u2192 Filter by length and support\n\n4. Refinement\n   \u2192 Refine line endpoints\n   \u2192 Adjust for sub-pixel accuracy\n</code></pre></p> <p>Column Extraction Process:</p> <pre><code>flowchart TD\n    Step1[\"Step 1: Detect Line Segments&lt;br/&gt;LSD Algorithm&lt;br/&gt;Returns: List of (x1,y1,x2,y2)\"] --&gt; Step2\n\n    Step2[\"Step 2: Filter by Angle&lt;br/&gt;Keep: 75\u00b0 to 105\u00b0&lt;br/&gt;Remove: horizontal/diagonal lines\"] --&gt; Step3\n\n    Step3[\"Step 3: Extract X-Coordinates&lt;br/&gt;Collect: x1, x2 for each line&lt;br/&gt;Result: [x1, x2, x3, ..., xn]\"] --&gt; Step4\n\n    Step4[\"Step 4: Cluster Nearby Coordinates&lt;br/&gt;Threshold: 1% of image width&lt;br/&gt;Merge: |x_i - x_j| &lt; threshold&lt;br/&gt;Result: [col1, col2, col3, ...]\"] --&gt; Step5\n\n    Step5[\"Step 5: Filter Edge Columns&lt;br/&gt;Remove: within 2% of edges&lt;br/&gt;Result: Valid column positions\"]\n\n    style Step1 fill:#FFE4B5\n    style Step2 fill:#DDA0DD\n    style Step3 fill:#87CEEB\n    style Step4 fill:#98FB98\n    style Step5 fill:#90EE90</code></pre> <p>Clustering Example: <pre><code>Detected x-coordinates:\n[98, 100, 102, 248, 250, 252, 398, 400, 402]\n\nAfter clustering (threshold=5px):\n[100, 250, 400]  \u2190 3 columns detected\n</code></pre></p>"},{"location":"user-guide/features/split-table-merging.html#23-column-count-matching","title":"2.3 Column Count Matching","text":"<p>Purpose: Compare the number of columns in both segments with adaptive tolerance.</p> <p>Algorithm: <pre><code>col_count1 = len(columns_detected_in_seg1)\ncol_count2 = len(columns_detected_in_seg2)\ndiff = abs(col_count1 - col_count2)\n\n# Adaptive threshold based on table size\nmax_cols = max(col_count1, col_count2)\n\nIf max_cols &lt;= 5:\n    max_allowed_diff = 1\nElse if max_cols &lt;= 10:\n    max_allowed_diff = 2\nElse if max_cols &lt;= 20:\n    max_allowed_diff = max(3, int(max_cols * 0.15))\nElse:\n    max_allowed_diff = max(5, int(max_cols * 0.20))\n\nIf diff &lt;= max_allowed_diff:\n    \u2705 PASS\n</code></pre></p> <p>Examples: <pre><code>Small table: 4 vs 5 columns \u2192 diff=1 \u2192 \u2705 (threshold=1)\nMedium: 8 vs 10 \u2192 diff=2 \u2192 \u2705 (threshold=2)\nLarge: 15 vs 18 \u2192 diff=3 \u2192 \u2705 (threshold=3, 15*0.15=2.25\u21923)\n</code></pre></p>"},{"location":"user-guide/features/split-table-merging.html#24-column-alignment-validation","title":"2.4 Column Alignment Validation","text":"<p>Purpose: Verify columns align between segments, ensuring structural continuity.</p> <p>Algorithm: <pre><code>For each column in segment1:\n    Find closest column in segment2\n    Calculate distance = |col1_pos - col2_pos|\n    If distance &lt;= tolerance:\n        \u2705 Aligned column\n    Else:\n        \u274c Misaligned column\n\nalignment_score = aligned_columns / total_columns\n\nIf alignment_score &gt;= 0.6:\n    \u2705 PASS (at least 60% alignment)\n</code></pre></p> <p>Visual Example:</p> <pre><code>graph TB\n    subgraph Seg1[\"Segment 1 Columns\"]\n        C1A[Column 1: 100px]\n        C2A[Column 2: 250px]\n        C3A[Column 3: 400px]\n        C4A[Column 4: 550px]\n        C5A[Column 5: 700px]\n    end\n\n    subgraph Seg2[\"Segment 2 Columns\"]\n        C1B[Column 1: 102px]\n        C2B[Column 2: 248px]\n        C3B[Column 3: 402px]\n        C4B[Column 4: 552px]\n        C5B[Column 5: 698px]\n    end\n\n    subgraph Alignment[\"Alignment Check\"]\n        A1[\"Diff: 2px \u2705\"]\n        A2[\"Diff: 2px \u2705\"]\n        A3[\"Diff: 2px \u2705\"]\n        A4[\"Diff: 2px \u2705\"]\n        A5[\"Diff: 2px \u2705\"]\n        AScore[\"Score: 5/5 = 100% \u2705\"]\n    end\n\n    C1A --&gt; A1\n    C1B --&gt; A1\n    C2A --&gt; A2\n    C2B --&gt; A2\n    C3A --&gt; A3\n    C3B --&gt; A3\n    C4A --&gt; A4\n    C4B --&gt; A4\n    C5A --&gt; A5\n    C5B --&gt; A5\n\n    A1 --&gt; AScore\n    A2 --&gt; AScore\n    A3 --&gt; AScore\n    A4 --&gt; AScore\n    A5 --&gt; AScore\n\n    style Alignment fill:#90EE90\n    style Seg1 fill:#FFE4B5\n    style Seg2 fill:#FFE4B5\n    style AScore fill:#90EE90</code></pre>"},{"location":"user-guide/features/split-table-merging.html#25-confidence-calculation","title":"2.5 Confidence Calculation","text":"<p>Purpose: Compute overall confidence score for the merge decision.</p> <p>Scoring Formula: <pre><code>confidence = 0.0\n\n# Column count match (max 0.3)\nif column_count_match:\n    confidence += 0.3\nelif column_diff &lt;= 1:\n    confidence += 0.2\nelif column_diff &lt;= 2:\n    confidence += 0.1\n\n# Column alignment (max 0.4)\nalignment_weight = alignment_score * 0.4\nconfidence += alignment_weight\n\n# Width similarity (max 0.1)\nwidth_score = 1.0 - min(1.0, width_ratio / 0.2)\nconfidence += width_score * 0.1\n\n# Overlap ratio (max 0.2)\noverlap_score = min(1.0, (overlap_ratio - 0.5) / 0.5)  # 0.5-1.0 \u2192 0.0-1.0\nconfidence += overlap_score * 0.2\n\nFinal: confidence (0.0 - 1.0)\n</code></pre></p> <p>Example Calculation: <pre><code>Perfect match:\n  - Column count: 5 vs 5 \u2192 +0.3\n  - Alignment: 100% \u2192 +0.4\n  - Width: 800px vs 802px (0.25%) \u2192 +0.1\n  - Overlap: 98% \u2192 +0.2\n  Total: 1.0 (100% confidence) \u2705\n</code></pre></p>"},{"location":"user-guide/features/split-table-merging.html#fallback-mechanisms_1","title":"Fallback Mechanisms","text":""},{"location":"user-guide/features/split-table-merging.html#too-many-columns-detected","title":"Too Many Columns Detected","text":"<p>Problem: LSD may detect noise (horizontal lines, text boundaries) as columns.</p> <p>Solution: <pre><code>If detected_columns &gt; 20:\n    \u2192 Likely noise, not real columns\n    \u2192 Skip structural validation\n    \u2192 Use proximity-based fallback\n    \u2192 Confidence: 0.70 (lower than validated)\n</code></pre></p>"},{"location":"user-guide/features/split-table-merging.html#no-columns-detected","title":"No Columns Detected","text":"<p>Problem: Borderless tables or poor image quality prevent column detection.</p> <p>Solution: <pre><code>If columns_detected == 0 in both segments:\n    \u2192 Tables lack visible borders\n    \u2192 Fall back to proximity matching\n    \u2192 Confidence: 0.65\n    \u2192 Reason: \"Proximity match (no columns detected by LSD)\"\n</code></pre></p>"},{"location":"user-guide/features/split-table-merging.html#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"user-guide/features/split-table-merging.html#detailed-parameter-reference","title":"Detailed Parameter Reference","text":"Parameter Type Default Range Impact <code>merge_split_tables</code> bool <code>False</code> True/False Master switch for feature <code>bottom_threshold_ratio</code> float <code>0.20</code> 0.0-1.0 How close to bottom triggers detection <code>top_threshold_ratio</code> float <code>0.15</code> 0.0-1.0 How close to top triggers detection <code>max_gap_ratio</code> float <code>0.25</code> 0.0-1.0 Maximum gap between segments <code>column_alignment_tolerance</code> float <code>10.0</code> 1.0-50.0 Pixel tolerance for alignment <code>min_merge_confidence</code> float <code>0.65</code> 0.0-1.0 Minimum confidence to merge"},{"location":"user-guide/features/split-table-merging.html#tuning-guidelines","title":"Tuning Guidelines","text":""},{"location":"user-guide/features/split-table-merging.html#for-documents-with-large-headersfooters","title":"For Documents with Large Headers/Footers","text":"<pre><code>parser = StructuredPDFParser(\n    merge_split_tables=True,\n    max_gap_ratio=0.30,  # Increase to 30% for larger headers\n)\n</code></pre>"},{"location":"user-guide/features/split-table-merging.html#for-stricter-merging-fewer-false-positives","title":"For Stricter Merging (Fewer False Positives)","text":"<pre><code>parser = StructuredPDFParser(\n    merge_split_tables=True,\n    min_merge_confidence=0.80,  # Higher threshold\n    column_alignment_tolerance=5.0,  # Tighter alignment\n)\n</code></pre>"},{"location":"user-guide/features/split-table-merging.html#for-more-aggressive-merging-catch-more-cases","title":"For More Aggressive Merging (Catch More Cases)","text":"<pre><code>parser = StructuredPDFParser(\n    merge_split_tables=True,\n    min_merge_confidence=0.55,  # Lower threshold\n    max_gap_ratio=0.35,  # Allow larger gaps\n    bottom_threshold_ratio=0.25,  # More lenient position check\n    top_threshold_ratio=0.20,\n)\n</code></pre>"},{"location":"user-guide/features/split-table-merging.html#examples","title":"Examples","text":""},{"location":"user-guide/features/split-table-merging.html#example-1-financial-report-table","title":"Example 1: Financial Report Table","text":"<pre><code>Document: Quarterly Financial Report\nPages: 2 pages, table spans both\n\nDetection Result:\n\u2705 Match found: Page 1\u21922\n   Confidence: 0.92\n   Reason: LSD validation: 6 vs 6 columns, alignment=0.95\n   Gap: 18.8% of page height\n\nOutput:\n- Merged image: merged_table_1_2.png\n- Markdown: Single table entry with note \"pages 1-2\"\n</code></pre>"},{"location":"user-guide/features/split-table-merging.html#example-2-borderless-table","title":"Example 2: Borderless Table","text":"<pre><code>Document: Research Data Table\nPages: 2 pages, no visible borders\n\nDetection Result:\n\u2705 Match found: Page 3\u21924\n   Confidence: 0.70\n   Reason: Proximity match (too many columns detected, likely noise)\n   Note: Using fallback validation (no clear column boundaries)\n\nOutput:\n- Merged image created\n- Lower confidence due to lack of structural validation\n</code></pre>"},{"location":"user-guide/features/split-table-merging.html#example-3-rejected-match","title":"Example 3: Rejected Match","text":"<pre><code>Document: Separate Tables\nPages: 2 pages with different tables\n\nDetection Result:\n\u274c No match\n   Reason: Column count mismatch (4 vs 7 columns)\n   Confidence: 0.45 (below threshold of 0.65)\n\nOutput:\n- Tables processed separately\n- No merge attempted\n</code></pre>"},{"location":"user-guide/features/split-table-merging.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/features/split-table-merging.html#tables-not-being-merged","title":"Tables Not Being Merged","text":"<p>Problem: Split tables are not being detected.</p> <p>Solutions:</p> <ol> <li> <p>Check Position Thresholds <pre><code># Verify tables are actually near page boundaries\nbottom_threshold_ratio=0.25  # Try increasing\ntop_threshold_ratio=0.20\n</code></pre></p> </li> <li> <p>Check Gap Tolerance <pre><code># Large headers/footers may require:\nmax_gap_ratio=0.30  # Increase from 0.25\n</code></pre></p> </li> <li> <p>Lower Confidence Threshold <pre><code>min_merge_confidence=0.60  # Try lowering from 0.65\n</code></pre></p> </li> <li> <p>Enable Debug Mode <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre></p> </li> </ol>"},{"location":"user-guide/features/split-table-merging.html#false-positives-incorrect-merges","title":"False Positives (Incorrect Merges)","text":"<p>Problem: Separate tables are being incorrectly merged.</p> <p>Solutions:</p> <ol> <li> <p>Increase Confidence Threshold <pre><code>min_merge_confidence=0.75  # More conservative\n</code></pre></p> </li> <li> <p>Tighten Alignment Tolerance <pre><code>column_alignment_tolerance=5.0  # Stricter alignment\n</code></pre></p> </li> <li> <p>Adjust Position Thresholds <pre><code>bottom_threshold_ratio=0.15  # More restrictive\ntop_threshold_ratio=0.10\n</code></pre></p> </li> </ol>"},{"location":"user-guide/features/split-table-merging.html#performance-issues","title":"Performance Issues","text":"<p>Problem: Processing is too slow with split table detection.</p> <p>Solutions:</p> <ol> <li> <p>Disable if not needed <pre><code>merge_split_tables=False  # Skip detection entirely\n</code></pre></p> </li> <li> <p>The feature is already optimized - Phase 1 filters out most pairs before expensive Phase 2 operations</p> </li> </ol>"},{"location":"user-guide/features/split-table-merging.html#column-detection-failures","title":"Column Detection Failures","text":"<p>Problem: LSD not detecting columns correctly.</p> <p>Causes &amp; Solutions:</p> <ol> <li>Low image quality</li> <li> <p>Solution: Increase DPI    <pre><code>dpi=300  # Instead of 200\n</code></pre></p> </li> <li> <p>Dashed/broken lines</p> </li> <li>Solution: Already handled by morphological operations</li> <li> <p>May need to check preprocessing parameters</p> </li> <li> <p>Borderless tables</p> </li> <li>Solution: System automatically falls back to proximity matching</li> </ol>"},{"location":"user-guide/features/split-table-merging.html#technical-implementation-details","title":"Technical Implementation Details","text":""},{"location":"user-guide/features/split-table-merging.html#data-structures","title":"Data Structures","text":"<pre><code>@dataclass\nclass TableSegment:\n    \"\"\"Represents a table segment with bounding box and page info.\"\"\"\n    page_index: int\n    box_index: int\n    x1: float\n    y1: float\n    x2: float\n    y2: float\n    page_width: int\n    page_height: int\n    image: Image.Image  # Cropped table image\n    confidence: float\n\n@dataclass\nclass SplitTableMatch:\n    \"\"\"Represents a validated split table match.\"\"\"\n    segment1: TableSegment\n    segment2: TableSegment\n    confidence: float\n    merge_reason: str\n    column_count1: int\n    column_count2: int\n</code></pre>"},{"location":"user-guide/features/split-table-merging.html#performance-characteristics","title":"Performance Characteristics","text":"<ul> <li>Time Complexity: O(n\u00b2) for table pairs, but Phase 1 filters dramatically reduce n</li> <li>Space Complexity: O(n) for storing segments and matches</li> <li>Typical Performance: </li> <li>10 pages with 20 tables \u2192 ~10ms for Phase 1, ~50ms for Phase 2</li> <li>Most time spent in image processing (LSD detection)</li> </ul>"},{"location":"user-guide/features/split-table-merging.html#dependencies","title":"Dependencies","text":"<ul> <li>OpenCV: For LSD (Line Segment Detector) and image processing</li> <li>NumPy: For numerical operations</li> <li>PIL: For image manipulation</li> </ul>"},{"location":"user-guide/features/split-table-merging.html#best-practices","title":"Best Practices","text":"<ol> <li>Enable for financial/structured documents: Most likely to have split tables</li> <li>Disable for narrative documents: Tables are usually separate</li> <li>Adjust thresholds based on document type: Financial reports may need different settings than academic papers</li> <li>Review merged results: Especially when using lower confidence thresholds</li> <li>Use appropriate DPI: Higher DPI improves column detection accuracy</li> </ol>"},{"location":"user-guide/features/split-table-merging.html#related-documentation","title":"Related Documentation","text":"<ul> <li>Structured Parser Guide - Main parser documentation</li> <li>API Reference - Complete parameter reference</li> <li>Examples - Code examples and use cases</li> </ul>"},{"location":"user-guide/outputs/export-formats.html","title":"Export Formats","text":"<p>Guide to Doctra's output formats.</p>"},{"location":"user-guide/outputs/export-formats.html#overview","title":"Overview","text":"<p>Doctra generates multiple output formats simultaneously, each optimized for different use cases.</p>"},{"location":"user-guide/outputs/export-formats.html#available-formats","title":"Available Formats","text":""},{"location":"user-guide/outputs/export-formats.html#markdown-md","title":"Markdown (.md)","text":"<p>Human-readable document with:</p> <ul> <li>All text content</li> <li>Embedded image references</li> <li>Table links</li> <li>Section structure</li> </ul> <p>Best for: Documentation, version control, reading</p> <p>Example: <pre><code># Document Title\n\n## Section 1\n\nText content...\n\n![Figure 1](images/figures/figure_001.jpg)\n\nSee tables in [tables.xlsx](tables.xlsx)\n</code></pre></p>"},{"location":"user-guide/outputs/export-formats.html#html-html","title":"HTML (.html)","text":"<p>Web-ready document with:</p> <ul> <li>Styled content</li> <li>Embedded images</li> <li>Interactive tables</li> <li>Responsive layout</li> </ul> <p>Best for: Web publishing, presentations</p>"},{"location":"user-guide/outputs/export-formats.html#excel-xlsx","title":"Excel (.xlsx)","text":"<p>Spreadsheet with extracted data:</p> <ul> <li>One sheet per table</li> <li>Formatted cells</li> <li>Headers preserved</li> <li>Data structured</li> </ul> <p>Best for: Data analysis, further processing</p> <p>Only generated when VLM is enabled</p>"},{"location":"user-guide/outputs/export-formats.html#json-json","title":"JSON (.json)","text":"<p>Structured data with:</p> <ul> <li>Element metadata</li> <li>Coordinates</li> <li>Content</li> <li>Relationships</li> </ul> <p>Best for: Programmatic access, integration</p> <p>Only generated when VLM is enabled</p>"},{"location":"user-guide/outputs/export-formats.html#images","title":"Images","text":"<p>Cropped visual elements:</p> <ul> <li><code>figures/</code>: Document images</li> <li><code>charts/</code>: Graphs and plots</li> <li><code>tables/</code>: Table images</li> </ul> <p>Format: JPEG or PNG Best for: Direct use, presentations</p>"},{"location":"user-guide/outputs/export-formats.html#output-structure","title":"Output Structure","text":"<pre><code>outputs/\n\u2514\u2500\u2500 document/\n    \u2514\u2500\u2500 full_parse/\n        \u251c\u2500\u2500 result.md          # Markdown\n        \u251c\u2500\u2500 result.html        # HTML\n        \u251c\u2500\u2500 tables.xlsx        # Excel (VLM)\n        \u251c\u2500\u2500 tables.html        # HTML tables (VLM)\n        \u251c\u2500\u2500 vlm_items.json     # JSON data (VLM)\n        \u2514\u2500\u2500 images/\n            \u251c\u2500\u2500 figures/\n            \u251c\u2500\u2500 charts/\n            \u2514\u2500\u2500 tables/\n</code></pre>"},{"location":"user-guide/outputs/export-formats.html#choosing-formats","title":"Choosing Formats","text":"Use Case Recommended Format Reading Markdown or HTML Data analysis Excel Web publishing HTML Integration JSON Presentations Images + HTML Version control Markdown"},{"location":"user-guide/outputs/export-formats.html#see-also","title":"See Also","text":"<ul> <li>Visualization - Visual outputs</li> <li>Examples - Usage examples</li> <li>API Reference - Exporter documentation</li> </ul>"},{"location":"user-guide/outputs/visualization.html","title":"Visualization","text":"<p>Guide to visualizing Doctra's processing results.</p>"},{"location":"user-guide/outputs/visualization.html#overview","title":"Overview","text":"<p>Doctra provides visualization tools to help you understand and verify document processing results.</p>"},{"location":"user-guide/outputs/visualization.html#layout-visualization","title":"Layout Visualization","text":"<p>Display detected document elements with bounding boxes:</p> <pre><code>from doctra import StructuredPDFParser\n\nparser = StructuredPDFParser()\n\nparser.display_pages_with_boxes(\n    pdf_path=\"document.pdf\",\n    num_pages=3\n)\n</code></pre>"},{"location":"user-guide/outputs/visualization.html#features","title":"Features","text":"<ul> <li>Color-coded Elements: Each type has a distinct color</li> <li>Confidence Scores: Shows detection confidence</li> <li>Grid Layout: Multiple pages in organized grid</li> <li>Element Counts: Summary statistics per page</li> </ul>"},{"location":"user-guide/outputs/visualization.html#color-scheme","title":"Color Scheme","text":"<ul> <li>\ud83d\udd35 Blue: Text regions</li> <li>\ud83d\udd34 Red: Tables</li> <li>\ud83d\udfe2 Green: Charts</li> <li>\ud83d\udfe0 Orange: Figures</li> </ul>"},{"location":"user-guide/outputs/visualization.html#configuration","title":"Configuration","text":"<pre><code>parser.display_pages_with_boxes(\n    pdf_path=\"document.pdf\",\n    num_pages=5,        # Pages to visualize\n    cols=3,             # Grid columns\n    page_width=700,     # Page width in pixels\n    spacing=40,         # Spacing between pages\n    save_path=\"viz.png\" # Save instead of display\n)\n</code></pre>"},{"location":"user-guide/outputs/visualization.html#use-cases","title":"Use Cases","text":"<ol> <li>Quality Assurance: Verify detection accuracy</li> <li>Debugging: Identify layout issues</li> <li>Documentation: Create visual reports</li> <li>Analysis: Understand document structure</li> </ol>"},{"location":"user-guide/outputs/visualization.html#cli-visualization","title":"CLI Visualization","text":"<pre><code>doctra visualize document.pdf --num-pages 5 --output layout.png\n</code></pre>"},{"location":"user-guide/outputs/visualization.html#see-also","title":"See Also","text":"<ul> <li>Layout Detection - Understanding detection</li> <li>Core Concepts - Processing pipeline</li> <li>CLI Reference - Command line tools</li> </ul>"},{"location":"user-guide/parsers/chart-table-extractor.html","title":"Chart &amp; Table Extractor","text":"<p>Guide to using the <code>ChartTablePDFParser</code> for targeted extraction.</p>"},{"location":"user-guide/parsers/chart-table-extractor.html#overview","title":"Overview","text":"<p>The <code>ChartTablePDFParser</code> is a specialized parser focused exclusively on extracting charts and tables from PDF documents. It's optimized for scenarios where you only need these specific elements.</p>"},{"location":"user-guide/parsers/chart-table-extractor.html#key-features","title":"Key Features","text":"<ul> <li>Focused Extraction: Extract only charts and/or tables</li> <li>Selective Processing: Choose what to extract</li> <li>VLM Integration: Convert visuals to structured data</li> <li>Faster Processing: Skips unnecessary elements</li> </ul>"},{"location":"user-guide/parsers/chart-table-extractor.html#basic-usage","title":"Basic Usage","text":"<pre><code>from doctra import ChartTablePDFParser\n\nparser = ChartTablePDFParser(\n    extract_charts=True,\n    extract_tables=True\n)\n\nparser.parse(\"data_report.pdf\")\n</code></pre>"},{"location":"user-guide/parsers/chart-table-extractor.html#selective-extraction","title":"Selective Extraction","text":"<pre><code># Extract only tables\nparser = ChartTablePDFParser(\n    extract_charts=False,\n    extract_tables=True\n)\n\n# Extract only charts\nparser = ChartTablePDFParser(\n    extract_charts=True,\n    extract_tables=False\n)\n</code></pre>"},{"location":"user-guide/parsers/chart-table-extractor.html#with-vlm-for-structured-data","title":"With VLM for Structured Data","text":"<pre><code>parser = ChartTablePDFParser(\n    extract_charts=True,\n    extract_tables=True,\n    use_vlm=True,\n    vlm_provider=\"openai\",\n    vlm_api_key=\"your-key\"\n)\n\nparser.parse(\"report.pdf\")\n# Outputs: tables.xlsx, tables.html, vlm_items.json\n</code></pre>"},{"location":"user-guide/parsers/chart-table-extractor.html#when-to-use","title":"When to Use","text":"<p>Use <code>ChartTablePDFParser</code> when:</p> <ul> <li>You only need charts and/or tables</li> <li>Faster processing is important</li> <li>Working with data-heavy documents</li> <li>Extracting data for analysis</li> </ul>"},{"location":"user-guide/parsers/chart-table-extractor.html#see-also","title":"See Also","text":"<ul> <li>VLM Integration - Structured data extraction</li> <li>Structured Parser - Full document parsing</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"user-guide/parsers/docx-parser.html","title":"DOCX Parser","text":"<p>The <code>StructuredDOCXParser</code> is a comprehensive parser for Microsoft Word documents (.docx files) that extracts text, tables, images, and structured content while preserving document formatting and order.</p>"},{"location":"user-guide/parsers/docx-parser.html#overview","title":"Overview","text":"<p>The DOCX parser provides:</p> <ul> <li>Complete DOCX Support: Extracts text, tables, images, and formatting from Word documents</li> <li>Document Order Preservation: Maintains the original sequence of elements (paragraphs, tables, images)</li> <li>VLM Integration: Optional Vision Language Model support for image analysis and table extraction</li> <li>Multiple Output Formats: Generates Markdown, HTML, and Excel files</li> <li>Excel Export: Creates structured Excel files with Table of Contents and clickable hyperlinks</li> <li>Formatting Preservation: Maintains text formatting (bold, italic, etc.) in output</li> <li>Progress Tracking: Real-time progress bars for VLM processing</li> </ul>"},{"location":"user-guide/parsers/docx-parser.html#basic-usage","title":"Basic Usage","text":"<pre><code>from doctra.parsers.structured_docx_parser import StructuredDOCXParser\n\n# Basic DOCX parsing\nparser = StructuredDOCXParser(\n    extract_images=True,\n    preserve_formatting=True,\n    table_detection=True,\n    export_excel=True\n)\n\n# Parse DOCX document\nparser.parse(\"document.docx\")\n</code></pre>"},{"location":"user-guide/parsers/docx-parser.html#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"user-guide/parsers/docx-parser.html#with-vlm-enhancement","title":"With VLM Enhancement","text":"<pre><code>parser = StructuredDOCXParser(\n    # VLM Settings\n    use_vlm=True,\n    vlm_provider=\"openai\",  # or \"gemini\", \"anthropic\", \"openrouter\"\n    vlm_model=\"gpt-4-vision\",\n    vlm_api_key=\"your_api_key\",\n\n    # Processing Options\n    extract_images=True,\n    preserve_formatting=True,\n    table_detection=True,\n    export_excel=True\n)\n\n# Parse with VLM enhancement\nparser.parse(\"document.docx\")\n</code></pre>"},{"location":"user-guide/parsers/docx-parser.html#custom-processing-options","title":"Custom Processing Options","text":"<pre><code>parser = StructuredDOCXParser(\n    # Disable image extraction for faster processing\n    extract_images=False,\n\n    # Disable formatting preservation for plain text\n    preserve_formatting=False,\n\n    # Disable table detection if not needed\n    table_detection=False,\n\n    # Disable Excel export\n    export_excel=False\n)\n</code></pre>"},{"location":"user-guide/parsers/docx-parser.html#output-structure","title":"Output Structure","text":"<p>When parsing a DOCX document, the parser creates:</p> <pre><code>outputs/document_name/\n\u251c\u2500\u2500 document.md          # Markdown version with all content\n\u251c\u2500\u2500 document.html        # HTML version with styling\n\u251c\u2500\u2500 tables.xlsx         # Excel file with extracted tables\n\u2502   \u251c\u2500\u2500 Table of Contents  # Summary sheet with hyperlinks\n\u2502   \u251c\u2500\u2500 Table 1         # Individual table sheets\n\u2502   \u251c\u2500\u2500 Table 2\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 images/             # Extracted images\n    \u251c\u2500\u2500 image1.png\n    \u251c\u2500\u2500 image2.jpg\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"user-guide/parsers/docx-parser.html#vlm-integration-features","title":"VLM Integration Features","text":"<p>When VLM is enabled, the parser:</p> <ul> <li>Analyzes Images: Uses AI to extract structured data from images</li> <li>Creates Tables: Converts chart images to structured table data</li> <li>Enhanced Excel Output: Includes VLM-extracted tables in Excel file</li> <li>Smart Content Display: Shows extracted tables instead of images in Markdown/HTML</li> <li>Progress Tracking: Shows progress based on number of images processed</li> </ul>"},{"location":"user-guide/parsers/docx-parser.html#vlm-processing-flow","title":"VLM Processing Flow","text":"<ol> <li>Image Detection: Scans document for embedded images</li> <li>VLM Analysis: Processes each image with the selected VLM model</li> <li>Structured Extraction: Converts visual content to structured data</li> <li>Excel Integration: Adds VLM-extracted tables to Excel output</li> <li>Content Replacement: Replaces image references with extracted tables in Markdown/HTML</li> </ol>"},{"location":"user-guide/parsers/docx-parser.html#excel-output-features","title":"Excel Output Features","text":"<p>The generated Excel file includes:</p> <ul> <li>Table of Contents: Summary sheet with all extracted tables</li> <li>Clickable Hyperlinks: Navigate between table sheets</li> <li>Consistent Styling: Professional formatting with colors and fonts</li> <li>VLM Integration: Includes both original and VLM-extracted tables</li> <li>Sheet Naming: Uses actual table titles as sheet names</li> </ul>"},{"location":"user-guide/parsers/docx-parser.html#cli-usage","title":"CLI Usage","text":"<pre><code># Basic DOCX parsing\ndoctra parse-docx document.docx\n\n# With VLM enhancement\ndoctra parse-docx document.docx --use-vlm --vlm-provider openai --vlm-api-key your_key\n\n# Custom options\ndoctra parse-docx document.docx \\\n  --extract-images \\\n  --preserve-formatting \\\n  --table-detection \\\n  --export-excel\n</code></pre>"},{"location":"user-guide/parsers/docx-parser.html#web-ui-usage","title":"Web UI Usage","text":"<p>The DOCX parser is available in the Gradio web interface:</p> <ol> <li>Upload DOCX File: Drag and drop your Word document</li> <li>Configure VLM: Enable VLM and set your API key</li> <li>Processing Options: Choose extraction settings</li> <li>Parse Document: Click \"Parse DOCX\" to process</li> <li>View Results: Preview content and download outputs</li> </ol>"},{"location":"user-guide/parsers/docx-parser.html#parameters-reference","title":"Parameters Reference","text":""},{"location":"user-guide/parsers/docx-parser.html#vlm-settings","title":"VLM Settings","text":"Parameter Type Default Description <code>use_vlm</code> bool False Enable VLM processing <code>vlm_provider</code> str None Provider: \"openai\", \"gemini\", \"anthropic\", \"openrouter\" <code>vlm_api_key</code> str None API key for the VLM provider <code>vlm_model</code> str None Specific model to use (provider-dependent)"},{"location":"user-guide/parsers/docx-parser.html#processing-options","title":"Processing Options","text":"Parameter Type Default Description <code>extract_images</code> bool True Extract embedded images from DOCX <code>preserve_formatting</code> bool True Preserve text formatting in output <code>table_detection</code> bool True Detect and extract tables <code>export_excel</code> bool True Export tables to Excel file"},{"location":"user-guide/parsers/docx-parser.html#error-handling","title":"Error Handling","text":"<p>The parser handles common errors:</p> <ul> <li>File Not Found: Invalid DOCX file path</li> <li>Permission Errors: Read-only files or locked documents</li> <li>VLM API Errors: Invalid API keys or rate limits</li> <li>Processing Errors: Corrupted documents or unsupported formats</li> </ul> <pre><code>try:\n    parser.parse(\"document.docx\")\nexcept FileNotFoundError:\n    print(\"DOCX file not found!\")\nexcept Exception as e:\n    print(f\"Processing error: {e}\")\n</code></pre>"},{"location":"user-guide/parsers/docx-parser.html#best-practices","title":"Best Practices","text":""},{"location":"user-guide/parsers/docx-parser.html#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Disable Unused Features: Turn off image extraction or Excel export if not needed</li> <li>VLM Usage: Use VLM only when structured data extraction is required</li> <li>Large Documents: Consider processing large documents in smaller chunks</li> </ul>"},{"location":"user-guide/parsers/docx-parser.html#output-quality","title":"Output Quality","text":"<ul> <li>Formatting Preservation: Keep enabled for better output quality</li> <li>Table Detection: Essential for structured data extraction</li> <li>VLM Enhancement: Improves table extraction from images</li> </ul>"},{"location":"user-guide/parsers/docx-parser.html#error-prevention","title":"Error Prevention","text":"<ul> <li>File Validation: Ensure DOCX files are not corrupted</li> <li>API Keys: Set up VLM API keys before processing</li> <li>Permissions: Ensure write access to output directory</li> </ul>"},{"location":"user-guide/parsers/docx-parser.html#examples","title":"Examples","text":""},{"location":"user-guide/parsers/docx-parser.html#example-1-basic-document-processing","title":"Example 1: Basic Document Processing","text":"<pre><code>from doctra.parsers.structured_docx_parser import StructuredDOCXParser\n\n# Initialize parser\nparser = StructuredDOCXParser()\n\n# Process document\nparser.parse(\"report.docx\")\n\n# Output: outputs/report/document.md, document.html, tables.xlsx\n</code></pre>"},{"location":"user-guide/parsers/docx-parser.html#example-2-vlm-enhanced-processing","title":"Example 2: VLM-Enhanced Processing","text":"<pre><code>parser = StructuredDOCXParser(\n    use_vlm=True,\n    vlm_provider=\"openai\",\n    vlm_api_key=\"your_api_key\"\n)\n\n# Process with AI enhancement\nparser.parse(\"financial_report.docx\")\n\n# Output: Enhanced Excel with VLM-extracted tables\n</code></pre>"},{"location":"user-guide/parsers/docx-parser.html#example-3-custom-configuration","title":"Example 3: Custom Configuration","text":"<pre><code>parser = StructuredDOCXParser(\n    extract_images=True,\n    preserve_formatting=False,  # Plain text output\n    table_detection=True,\n    export_excel=True\n)\n\n# Process with custom settings\nparser.parse(\"data_document.docx\")\n</code></pre>"},{"location":"user-guide/parsers/docx-parser.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/parsers/docx-parser.html#common-issues","title":"Common Issues","text":"<ol> <li>\"python-docx not installed\"</li> <li> <p>Solution: <code>pip install python-docx</code></p> </li> <li> <p>\"No tables extracted\"</p> </li> <li>Check if <code>table_detection=True</code></li> <li> <p>Verify document contains tables</p> </li> <li> <p>\"VLM API error\"</p> </li> <li>Verify API key is correct</li> <li> <p>Check provider and model compatibility</p> </li> <li> <p>\"Images not extracted\"</p> </li> <li>Check if <code>extract_images=True</code></li> <li>Verify document contains embedded images</li> </ol>"},{"location":"user-guide/parsers/docx-parser.html#performance-tips","title":"Performance Tips","text":"<ul> <li>Use VLM only when needed (adds processing time)</li> <li>Disable unused features for faster processing</li> <li>Process large documents in smaller batches</li> <li>Ensure sufficient disk space for outputs</li> </ul>"},{"location":"user-guide/parsers/docx-parser.html#related-documentation","title":"Related Documentation","text":"<ul> <li>API Reference</li> <li>VLM Integration</li> <li>Export Formats</li> <li>Web UI Guide</li> </ul>"},{"location":"user-guide/parsers/enhanced-parser.html","title":"Enhanced PDF Parser","text":"<p>Guide to using the <code>EnhancedPDFParser</code> with image restoration.</p>"},{"location":"user-guide/parsers/enhanced-parser.html#overview","title":"Overview","text":"<p>The <code>EnhancedPDFParser</code> extends <code>StructuredPDFParser</code> with DocRes image restoration capabilities. It's ideal for processing scanned documents, low-quality PDFs, or documents with visual distortions.</p>"},{"location":"user-guide/parsers/enhanced-parser.html#key-features","title":"Key Features","text":"<ul> <li>Image Restoration: DocRes integration for document enhancement</li> <li>6 Restoration Tasks: Dewarping, deshadowing, deblurring, and more</li> <li>GPU Acceleration: Optional CUDA support for faster processing</li> <li>All Base Features: Inherits all <code>StructuredPDFParser</code> capabilities</li> </ul>"},{"location":"user-guide/parsers/enhanced-parser.html#basic-usage","title":"Basic Usage","text":"<pre><code>from doctra import EnhancedPDFParser\n\nparser = EnhancedPDFParser(\n    use_image_restoration=True,\n    restoration_task=\"appearance\"\n)\n\nparser.parse(\"scanned_document.pdf\")\n</code></pre>"},{"location":"user-guide/parsers/enhanced-parser.html#restoration-tasks","title":"Restoration Tasks","text":"Task Best For <code>appearance</code> General enhancement (default) <code>dewarping</code> Perspective distortion <code>deshadowing</code> Shadow removal <code>deblurring</code> Blur reduction <code>binarization</code> Clean B&amp;W conversion <code>end2end</code> Severe degradation"},{"location":"user-guide/parsers/enhanced-parser.html#when-to-use","title":"When to Use","text":"<p>Use <code>EnhancedPDFParser</code> for:</p> <ul> <li>Scanned documents</li> <li>Low-quality PDFs</li> <li>Documents with visual distortions</li> <li>When OCR accuracy is poor with standard parser</li> </ul>"},{"location":"user-guide/parsers/enhanced-parser.html#see-also","title":"See Also","text":"<ul> <li>DocRes Engine - Image restoration details</li> <li>Structured Parser - Base parser</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"user-guide/parsers/structured-parser.html","title":"Structured PDF Parser","text":"<p>Comprehensive guide to using the <code>StructuredPDFParser</code>.</p>"},{"location":"user-guide/parsers/structured-parser.html#overview","title":"Overview","text":"<p>The <code>StructuredPDFParser</code> is the foundational parser in Doctra, designed for general-purpose PDF document processing. It combines layout detection, OCR, and optional VLM integration to extract all content from PDF documents.</p>"},{"location":"user-guide/parsers/structured-parser.html#key-features","title":"Key Features","text":"<ul> <li>Layout Detection: PaddleOCR-based document structure analysis</li> <li>OCR Processing: Text extraction from all document elements</li> <li>Visual Element Extraction: Automatic cropping of figures, charts, and tables</li> <li>VLM Integration: Optional structured data extraction</li> <li>Split Table Merging: Automatic detection and merging of tables split across pages</li> <li>Multiple Output Formats: Markdown, HTML, Excel, JSON</li> </ul>"},{"location":"user-guide/parsers/structured-parser.html#basic-usage","title":"Basic Usage","text":"<pre><code>from doctra import StructuredPDFParser\n\n# Initialize parser with defaults\nparser = StructuredPDFParser()\n\n# Parse document\nparser.parse(\"document.pdf\")\n</code></pre>"},{"location":"user-guide/parsers/structured-parser.html#configuration","title":"Configuration","text":"<p>See API Reference for detailed parameter documentation.</p>"},{"location":"user-guide/parsers/structured-parser.html#split-table-merging","title":"Split Table Merging","text":"<p>The <code>StructuredPDFParser</code> includes an advanced feature to automatically detect and merge tables that are split across multiple pages. This is especially useful for processing financial reports, data tables, and other documents where large tables span page boundaries.</p>"},{"location":"user-guide/parsers/structured-parser.html#how-it-works","title":"How It Works","text":"<p>The split table detection uses a two-phase approach combining spatial heuristics with structural analysis:</p>"},{"location":"user-guide/parsers/structured-parser.html#phase-1-proximity-detection","title":"Phase 1: Proximity Detection","text":"<p>The system first identifies candidate table pairs using position-based heuristics:</p> <ol> <li>Position Check: </li> <li>The first table segment must be close to the bottom of its page (within 20% by default)</li> <li> <p>The second table segment must be close to the top of the next page (within 15% by default)</p> </li> <li> <p>Horizontal Alignment:</p> </li> <li>The tables must have significant horizontal overlap (at least 50%)</li> <li> <p>This ensures they're aligned vertically on the page</p> </li> <li> <p>Gap Analysis:</p> </li> <li>The gap between the end of the first table and start of the second table is measured</li> <li>The gap accounts for page breaks, headers, and footers</li> <li> <p>Maximum allowed gap is 25% of page height by default</p> </li> <li> <p>Width Similarity:</p> </li> <li>Both table segments must have similar widths (within 20% difference)</li> <li>This ensures they belong to the same table structure</li> </ol>"},{"location":"user-guide/parsers/structured-parser.html#phase-2-structural-validation","title":"Phase 2: Structural Validation","text":"<p>Once proximity checks pass, the system performs deeper structural analysis using LSD (Line Segment Detector) from OpenCV:</p> <ol> <li>Column Detection:</li> <li>LSD detects vertical lines in both table segments</li> <li>These lines represent column boundaries</li> <li> <p>The algorithm is adaptive and works across different table structures without parameter tuning</p> </li> <li> <p>Column Count Matching:</p> </li> <li>The system compares the number of columns detected in both segments</li> <li> <p>Tolerance is adaptive based on table size:</p> <ul> <li>Small tables (\u22645 columns): Allows 1 column difference</li> <li>Medium tables (6-10 columns): Allows 2 column differences</li> <li>Large tables (11-20 columns): Allows 15% difference or minimum 3</li> </ul> </li> <li> <p>Column Alignment:</p> </li> <li>The relative positions of columns are compared between segments</li> <li>At least 60% of columns must align within a tolerance (10 pixels by default)</li> <li> <p>This ensures structural continuity</p> </li> <li> <p>Confidence Scoring:</p> </li> <li>A confidence score (0-1) is calculated based on:<ul> <li>Column count match</li> <li>Column alignment quality</li> <li>Width similarity</li> <li>Overlap ratio</li> </ul> </li> <li>Only matches with confidence \u2265 0.65 (default) are merged</li> </ol>"},{"location":"user-guide/parsers/structured-parser.html#fallback-mechanisms","title":"Fallback Mechanisms","text":"<p>The system includes intelligent fallbacks for edge cases:</p> <ul> <li> <p>Too Many Columns Detected: If LSD detects &gt;20 columns, it likely indicates noise (horizontal lines, text boundaries). The system falls back to proximity-based matching with lower confidence.</p> </li> <li> <p>No Columns Detected: For borderless tables or poor image quality, if no columns are detected in either segment, the system uses proximity-based matching.</p> </li> <li> <p>Noise Filtering: The system filters out edge columns that are too close to image boundaries (likely artifacts).</p> </li> </ul>"},{"location":"user-guide/parsers/structured-parser.html#enabling-split-table-merging","title":"Enabling Split Table Merging","text":"<pre><code>from doctra import StructuredPDFParser\n\n# Enable split table merging with default settings\nparser = StructuredPDFParser(\n    merge_split_tables=True\n)\n\n# Parse document - split tables will be automatically merged\nparser.parse(\"document.pdf\")\n</code></pre>"},{"location":"user-guide/parsers/structured-parser.html#configuration-options","title":"Configuration Options","text":"<pre><code>parser = StructuredPDFParser(\n    merge_split_tables=True,\n\n    # Position thresholds\n    bottom_threshold_ratio=0.20,  # 20% from bottom of page\n    top_threshold_ratio=0.15,     # 15% from top of page\n\n    # Gap tolerance\n    max_gap_ratio=0.25,            # 25% of page height max gap\n\n    # Structural validation\n    column_alignment_tolerance=10.0,  # Pixel tolerance for column alignment\n    min_merge_confidence=0.65,       # Minimum confidence to merge (0-1)\n)\n</code></pre>"},{"location":"user-guide/parsers/structured-parser.html#parameter-details","title":"Parameter Details","text":"Parameter Default Description <code>merge_split_tables</code> <code>False</code> Enable/disable split table detection <code>bottom_threshold_ratio</code> <code>0.20</code> Ratio for detecting tables near bottom of page (0-1) <code>top_threshold_ratio</code> <code>0.15</code> Ratio for detecting tables near top of page (0-1) <code>max_gap_ratio</code> <code>0.25</code> Maximum allowed gap between tables (accounts for headers/footers) <code>column_alignment_tolerance</code> <code>10.0</code> Pixel tolerance for column alignment validation <code>min_merge_confidence</code> <code>0.65</code> Minimum confidence score (0-1) required to merge tables"},{"location":"user-guide/parsers/structured-parser.html#output","title":"Output","text":"<p>When split tables are detected and merged:</p> <ul> <li>Merged Image: A single composite image is created combining both table segments</li> <li>Markdown Output: The merged table appears once in the markdown with a note indicating it spans multiple pages</li> <li>HTML Output: Similar to markdown, the merged table appears as a single element</li> <li>File Location: Merged tables are saved as <code>merged_table_{page1}_{page2}.png</code> in the <code>tables/</code> directory</li> </ul> <p>Example output note: <pre><code>\ud83d\udcca Table (merged from pages 1-2, confidence: 0.75)\n</code></pre></p>"},{"location":"user-guide/parsers/structured-parser.html#when-to-use","title":"When to Use","text":"<p>Enable split table merging when:</p> <ul> <li>Processing documents with large tables spanning multiple pages</li> <li>Working with financial reports, data tables, or structured documents</li> <li>You want complete table context in a single view</li> <li>Tables frequently break across page boundaries</li> </ul> <p>Consider disabling when:</p> <ul> <li>Tables are intentionally separate across pages</li> <li>Processing speed is critical (adds minor overhead)</li> <li>Document structure is inconsistent</li> </ul>"},{"location":"user-guide/parsers/structured-parser.html#technical-details","title":"Technical Details","text":"<p>Algorithm: The detection uses OpenCV's LSD (Line Segment Detector), which is: - Adaptive: Works across different table structures without parameter tuning - Robust: Handles various line styles (solid, dashed, partially broken) - Efficient: Fast processing suitable for batch operations</p> <p>Preprocessing: Before column detection, images undergo: - Grayscale conversion - Contrast enhancement (CLAHE) - Binary thresholding (OTSU) - Morphological operations to connect broken lines</p> <p>Clustering: Detected line segments are clustered using adaptive thresholds based on image width, ensuring that multiple detections of the same column boundary are merged into a single column marker.</p> <p>For a complete, detailed explanation with visual schemas and examples, see the Split Table Merging Guide.</p>"},{"location":"user-guide/parsers/structured-parser.html#output-structure","title":"Output Structure","text":"<pre><code>outputs/\n\u2514\u2500\u2500 document/\n    \u2514\u2500\u2500 full_parse/\n        \u251c\u2500\u2500 result.md\n        \u251c\u2500\u2500 result.html\n        \u2514\u2500\u2500 images/\n</code></pre>"},{"location":"user-guide/parsers/structured-parser.html#when-to-use_1","title":"When to Use","text":"<p>Use <code>StructuredPDFParser</code> for:</p> <ul> <li>General PDF processing</li> <li>Good quality documents</li> <li>When image restoration is not needed</li> <li>Extracting all content types</li> </ul>"},{"location":"user-guide/parsers/structured-parser.html#see-also","title":"See Also","text":"<ul> <li>Split Table Merging - Detailed guide to automatic table merging</li> <li>Enhanced Parser - With image restoration</li> <li>Chart &amp; Table Extractor - Focused extraction</li> <li>API Reference - Complete API documentation</li> </ul>"}]}